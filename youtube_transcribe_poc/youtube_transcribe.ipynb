{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e68789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yt_dlp\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import whisperx\n",
    "import torch\n",
    "from whisperx.diarize import DiarizationPipeline\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0cff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verify if using GPU or CPU\n",
    "print(\"Using device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0948eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIG ----------\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if DEVICE == \"cuda\" else \"float32\"\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b31a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download audio from a youtube video URL\n",
    "def download_audio(video_url, output_name=\"audio\"):\n",
    "    \"\"\"Download audio from youtube video URL using yt_dlp\"\"\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"format\": \"bestaudio/best\",\n",
    "        \"quiet\": True,\n",
    "        \"outtmpl\": output_name + \".%(ext)s\",\n",
    "        \"postprocessors\": [{\n",
    "            \"key\": \"FFmpegExtractAudio\",\n",
    "            \"preferredcodec\": \"mp3\",\n",
    "            \"preferredquality\": \"192\",\n",
    "        }],\n",
    "    }\n",
    "\n",
    "    # Save audio output as a .mp3 file\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(video_url, download=True)\n",
    "        filename = ydl.prepare_filename(info)\n",
    "\n",
    "    # Replace extension with .mp3\n",
    "    audio_path = os.path.splitext(filename)[0] + \".mp3\"\n",
    "\n",
    "    return audio_path\n",
    "\n",
    "def transcribe_with_whisperx(audio_path, model=\"base\"):\n",
    "    \"\"\"Generate transcript + speaker diarization using WhisperX.\"\"\"\n",
    "\n",
    "    if HF_TOKEN is None:\n",
    "        raise RuntimeError(\"Missing HF_TOKEN in environment variables.\")\n",
    "\n",
    "    _original_torch_load = torch.load\n",
    "\n",
    "    def _trusted_load(*args, **kwargs):\n",
    "        kwargs['weights_only'] = False\n",
    "        return _original_torch_load(*args, **kwargs)\n",
    "\n",
    "    torch.load = _trusted_load\n",
    "    \n",
    "    # Transcribe\n",
    "    model = whisperx.load_model(model, DEVICE, compute_type=compute_type)\n",
    "    result = model.transcribe(audio_path)\n",
    "\n",
    "    # Alignment\n",
    "    model_a, metadata = whisperx.load_align_model(\n",
    "        language_code=result[\"language\"], device=DEVICE\n",
    "    )\n",
    "    result_aligned = whisperx.align(\n",
    "        result[\"segments\"], model_a, metadata, audio_path, DEVICE\n",
    "    )\n",
    "\n",
    "    # Diarization\n",
    "    diarize_model = DiarizationPipeline(\n",
    "        use_auth_token=HF_TOKEN, device=DEVICE\n",
    "    )\n",
    "    diarization = diarize_model(audio_path)\n",
    "\n",
    "    # Assign speakers to alignment\n",
    "    result_aligned = whisperx.assign_word_speakers(diarization, result_aligned)\n",
    "\n",
    "    # Extract to DataFrame\n",
    "    segments = result_aligned[\"segments\"]\n",
    "\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            \"speaker\": seg.get(\"speaker\", \"unknown\"),\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"text\": seg[\"text\"],\n",
    "        }\n",
    "        for seg in segments\n",
    "    ])\n",
    "\n",
    "    # Save output in a .txt file\n",
    "    with open(\"transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(f\"{row.speaker}: {row.text}\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607fdcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n",
      "WARNING: [youtube] oya_pgbik7g: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: [youtube] oya_pgbik7g: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "print(\"Downloading audio...\")\n",
    "audio_path = download_audio(\"https://www.youtube.com/watch?v=oya_pgbik7g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c0e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing with WhisperX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  available_backends = torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:18:15 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n",
      "2025-12-02 17:18:15 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n",
      "c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cu128. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\site-packages\\pyannote\\audio\\utils\\reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:18:22 - whisperx.asr - INFO - Detected language: fr (1.00) in first 30s of audio\n",
      "2025-12-02 17:22:49 - whisperx.diarize - INFO - Loading diarization model: pyannote/speaker-diarization-3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n",
      "c:\\Users\\Gery\\anaconda3\\envs\\ai_transcribe\\lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript sucessfully saved to transcript.txt\n",
      "      speaker   start     end  \\\n",
      "0  SPEAKER_00   0.031   3.178   \n",
      "1  SPEAKER_00   3.239  10.315   \n",
      "2  SPEAKER_00  10.335  15.407   \n",
      "3  SPEAKER_00  15.387  22.262   \n",
      "4  SPEAKER_00  22.282  25.850   \n",
      "\n",
      "                                                text  \n",
      "0   Aujourd'hui, on a la chance de recevoir une d...  \n",
      "1  Elle fait des filatures, mène des enquêtes sou...  \n",
      "2  Mais surtout, elle a une spécialité très origi...  \n",
      "3   Elle va nous raconter comment elle a résolu l...  \n",
      "4  Mais surtout, elle va lever le voile sur le ma...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Transcribing with WhisperX...\")\n",
    "df = transcribe_with_whisperx(audio_path,\"large-v3\")\n",
    "\n",
    "print(\"Transcript sucessfully saved to transcript.txt\")\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_transcribe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
