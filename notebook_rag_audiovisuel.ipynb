{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b419c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 0800cbac9c20: 100% ▕██████████████████▏ 621 MB                         \u001b[K\n",
      "pulling 1adbfec9dcf0: 100% ▕██████████████████▏ 8.4 KB                         \u001b[K\n",
      "pulling 45dc10444b87: 100% ▕██████████████████▏   34 B                         \u001b[K\n",
      "pulling 3901c6a1d7c2: 100% ▕██████████████████▏  416 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# créer un nouvel env conda à partir du terminal\n",
    "# conda create --name pathrag python=3.10\n",
    "# installer ollama: https://www.ollama.com/download\n",
    "\n",
    "# installer les dépendences\n",
    "#%pip install -r requirements.txt\n",
    "# intsaller le modème d'OllamaEmbeddings\n",
    "!ollama pull embeddinggemma\n",
    "# créer une clé api sur openrouter pour utiliser des llm gratuitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0299e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from rag.pathrag_retriever import create_graphdb, load_existing_graphdb, load_knowledgeGraph_vis\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573138d",
   "metadata": {},
   "source": [
    "#### Base vectorielle\n",
    "Création de la base vect. avec le PP Mahakam (20 premières pages, ligne 45 `docs[:20]`)\n",
    "\n",
    "Relancer la cellule à chaque ouverture pour utiliser le rag vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "514d8aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from data/audio-text.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de chuncks: 17\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "#========= choix du modèle d'embedding\n",
    "\"\"\"\n",
    "    Le modèle choisi impacte la qualité du retriever, mais aussi le temps de traitement\n",
    "    Si le déploiement est prévu sur une VM limitée, un modèle plus petit est nécessaire\n",
    "    Explorer les comparatifs: https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "\"\"\"\n",
    "# Utiliser OllamaEmbeddings avec le modèle local \"embeddinggemma\"\n",
    "embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "\n",
    "\n",
    "\n",
    "# chargement et fragmentation du doc\n",
    "## Nom du doc à traiter\n",
    "filename=\"data/audio-text.txt\"\n",
    "\n",
    "## Nom pour la base vectorielle\n",
    "doc_name_hybrid=\"L-IA-notre-deuxieme-conscience_sample\" # nom de doc significatif\n",
    "\n",
    "\n",
    "# loader = UnstructuredFileLoader(filename)\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_doc = loader.load()\n",
    "print(f\"Loaded {len(txt_doc)} documents from {filename}\")\n",
    "\n",
    "\n",
    "#======== choix des paramètres de fragmentation\n",
    "\"\"\"\n",
    "    la taille du chunck_size est très important dans l'accès à une info précise\n",
    "    une plus petite taille permet de cibler de courts passages contenant l'info nécessaire à des réponses précises:\n",
    "        * lieu du projet\n",
    "        * dates du projet\n",
    "        * budget ...    \n",
    "    l'envoi de passages plus courts au llm évite une dispertion de son attention\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(txt_doc)\n",
    "\n",
    "# Filter out complex metadata (e.g., lists, dicts)\n",
    "docs = [Document(doc.page_content) for doc in docs]\n",
    "\n",
    "\n",
    "# Conversion des docs en embeddings \n",
    "chroma_db = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=f'storage/vector_stores/{doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "    collection_name=doc_name_hybrid.replace(\" \",\"_\")\n",
    ")\n",
    "\n",
    "retriever=chroma_db.as_retriever()\n",
    "\n",
    "all_docs = chroma_db.get()\n",
    "print(\"Nb de chuncks:\", len(all_docs['documents']))  # This will print the total number of docs stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b6d50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nathan Devert]: France Culture. [Nathan Devert]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Comment expliquer les progrès phénoménaux que semble avoir accompli l'intelligence artificielle au cours de ces dernières années ? Depuis l'apparition de ChatGPT en novembre 2022, rapidement suivi par d'autres agents conversationnels, cette révolution technologique aux multiples aspects, paraît désormais capable d'exécuter de nombreuses tâches intellectuelles sur lesquelles l'esprit humain pensait jusqu'alors exercer un monopole. Écrire des articles, synthétiser des documents, traiter des données dans n'importe quel domaine, diagnostiquer une maladie, rédiger une dissertation, ou pourquoi pas, un scénario de film. Non contente de révolutionner le monde du travail, ses prouesses stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et \n",
      "=============\n",
      "stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et encore moins d'une âme, peut s'implémenter dans n'importe quelle machine, voir, comme s'inquiètent ou exultent certains, que l'IA sera un jour en mesure de remplacer notre conscience. À moins qu'il ne faille poser cette question à l'envers. Comment se fait-il que la machine puisse imiter les œuvres de de notre esprit ? Comment est-il possible en d'autres termes que notre vie mentale se révèle comme mécanisable, au même titre que nos facultés manuelles ? L'intelligence artificielle ne doit-elle pas sa capacité d'imitation au fait qu'elle a été théorisée, inventée, développée d'après une certaine représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre \n",
      "=============\n",
      "représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre propre pensée. L'intelligence artificielle, notre deuxième conscience. À l'occasion de la fête de la Science, j'aurai le plaisir d'en débattre sans préjuger aux côtés du mathématicien et philosophe Daniel Andler, de la professeur en intelligence artificielle et chercheuse Laurence Devillers et du philosophe Valentin Husson. [Narrateur radio]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Bonjour Laurence Devillers. [Laurence Devillers]: Bonjour. [Nathan Devert]: Vous êtes professeur en intelligence artificielle à Sorbonne Université, chercheuse au CNRS, auteur de \"L'IA, ange ou démon\" paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur \n",
      "=============\n",
      "paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur émérite à Sorbonne Université, membre de l'Académie des Sciences morales et politiques et auteur de \"Intelligence artificielle, intelligence humaine, la double énigme\" aux éditions Gallimard et bonjour Valentin Husson. [Valentin Husson]: Vous êtes philosophe et auteur de \"Folle résentimentale, petite philosophie des trolls\" chez Philosophie Magazine éditeur qui vient de paraître. Alors pour commencer cette cette discussion sur l'intelligence artificielle, je voulais vous soumettre une histoire qui a eu lieu récemment en mars 2025. Le Nouvel Obs a décidé d'organiser un concours littéraire entre deux auteurs, entre guillemets. Alors le premier était Hervé Le Tellier, pris Goncourt 2020, auteur de du célèbre roman l'anomalie et le deuxième était l'intelligence artificielle et plus précisément \n",
      "=============\n",
      "deux auteurs, entre guillemets. Alors le premier était Hervé Le Tellier, pris Goncourt 2020, auteur de du célèbre roman l'anomalie et le deuxième était l'intelligence artificielle et plus précisément ChatGPT. Le l'objet de ce concours était de demander à l'un comme à l'autre entre guillemets, de rédiger une nouvelle et puis avec la première phrase qui était indiquée c'était quelqu'un qui découvrait un cadavre et la dernière phrase de la nouvelle aussi. Hervé Le Tellier a rendu sa copie. ChatGPT aussi et de l'aveu même de Hervé Le Tellier, il a dit franchement, je suis absolument bluffé par la créativité littéraire entre guillemets de ChatGPT. Peut-être même que ChatGPT m'a battu, que la nouvelle qui a été rendue par l'intelligence artificielle était plus drôle, plus originale, plus vive que celle que j'ai écrite et peut-être donc que ça inaugure un monde où les écrivains seront moins moins présents que l'intelligence artificielle. Daniel Andler, une telle histoire qu'est-ce qu'elle \n",
      "=============\n",
      "que celle que j'ai écrite et peut-être donc que ça inaugure un monde où les écrivains seront moins moins présents que l'intelligence artificielle. Daniel Andler, une telle histoire qu'est-ce qu'elle révèle ? Est-ce qu'elle signifie que l'IA peut battre l'esprit humain dans ce qu'il a de plus propre, la créativité. [Daniel Andler]: Ah, je ne le pense pas. Euh mais je pense que c'est pas original de pas le penser. On on est généralement réticent à reconnaître que la créativité s'échapperait d'une certaine façon à l'être humain. Il est très frappant que Hervé Le Tellier avec qui j'ai d'ailleurs débattu tout à fait récemment dans un autre lieu ce ce soit en quelque sorte presque d'avance déclaré battu. Mais c'est qu'il est persuadé Hervé Le Tellier qu'effectivement l'intelligence humaine est en déroute en quelque sorte. Ce qui est assez intéressant parce que voilà un écrivain, un homme dont c'est le métier en quelque sorte qui est déjà en quelque sorte acquis à l'idée que l'intelligence \n",
      "=============\n",
      "en déroute en quelque sorte. Ce qui est assez intéressant parce que voilà un écrivain, un homme dont c'est le métier en quelque sorte qui est déjà en quelque sorte acquis à l'idée que l'intelligence artificielle est capable de battre l'homme sur le plan de la créativité. Il faut reconnaître alors il me semble que très rapidement et je veux pas du tout être trop long, mais très rapidement, il s'agit d'un de ce qu'on appelle un exercice de style. C'est-à-dire une une une épreuve assez semblable au fond à une dissertation si vous voulez avec des contraintes et faire la meilleure dissertation possible dans un temps limité et qui euh qui obéissent à ses contraintes, c'est au fond résoudre un problème de créativité. C'est plutôt de l'ordre de l'exercice. Ça si vous voulez, c'est un peu comme résoudre un exercice de mathématiques un petit peu difficile, mais c'est pas résoudre un exercice de mathématiques, c'est pas faire de la recherche en maths, c'est pas faire quelque chose de vraiment \n",
      "=============\n",
      "résoudre un exercice de mathématiques un petit peu difficile, mais c'est pas résoudre un exercice de mathématiques, c'est pas faire de la recherche en maths, c'est pas faire quelque chose de vraiment nouveau. Donc, il me semble que Hervé Le Tellier et d'autres sont trop impressionnés par le résultat et euh ne pensent pas suffisamment au processus lui-même et le processus de la création, me semble-t-il et euh une un un une motivation initiale d'un écrivain ou d'un artiste ou d'un compositeur de faire du mieux qu'il peut pour s'adresser à un public. Donc il y a au départ le créateur, à l'arrivée le public, alors que il y a rien de tout ça pour l'intelligence artificielle. L'intelligence artificielle n'est pas une personne qui cherche à faire quelque chose. C'est une machine extraordinairement bluffante, ça je vous l'avoue, une machine qui répond à une demande, à savoir écrire une nouvelle policière avec cette ce ce début et cette fin. C'est tout à fait autre chose. [Nathan Devert]: \n",
      "=============\n",
      "bluffante, ça je vous l'avoue, une machine qui répond à une demande, à savoir écrire une nouvelle policière avec cette ce ce début et cette fin. C'est tout à fait autre chose. [Nathan Devert]: Est-ce que vous pensez aussi Laurence Devillers que Hervé Le Tellier et ceux qui ont été impressionnés par ce par ce concours, ont cédé en un sens à ce mirage ou à cette illusion du du résultat ? [Laurence Devillers]: Oui, je pense qu'ils ont été usurpés par cette machine qui a vu d'ailleurs toutes leurs textes, qui connaît toutes leurs données, tous les livres préalables et les livres peut-être du monde entier pour créer cette petite nouvelle qui est effectivement un exercice de style qui est moins facile pour un humain puisqu'on donne la première phrase, on donne la dernière et il y a un nombre de caractères. Donc si vous voulez, c'est une optimisation dans un un océan de documents et ça la machine sait très bien faire. Donc c'est pas du tout étonnant qu'elle est qu'elle soit meilleure. Moi, \n",
      "=============\n",
      "caractères. Donc si vous voulez, c'est une optimisation dans un un océan de documents et ça la machine sait très bien faire. Donc c'est pas du tout étonnant qu'elle est qu'elle soit meilleure. Moi, j'aurais dit à l'avance évidemment qu'elles étaient meilleures. C'est la même chose qui voulait que pour les trouver dans les radios des cancers précoces, la machine fait autre chose que nous et elle peut nous surprendre et apporter de l'information et même dans et c'est pas de la créativité là, c'est vraiment de la réutilisation de texte. Et il faut bien comprendre cela, parce que ces machines qui nous bluffent en ce moment, elles sont nourries de toutes nos données. Or actuellement, elles en produisent aussi les données, approximatives, qui demain seront celles qui vont nourrir les prochaines machines. Et là, on va voir baisser gravement la créativité de ces machines. Ça, c'est l'effet qu'on ne voit pas encore aujourd'hui. On est bluffé par le premier step, la première. Parce que c'est \n",
      "=============\n",
      "machines. Et là, on va voir baisser gravement la créativité de ces machines. Ça, c'est l'effet qu'on ne voit pas encore aujourd'hui. On est bluffé par le premier step, la première. Parce que c'est nos données à l'intérieur. Et j'ai et et monsieur Le Tellier comme d'autres, il a été récompensé par un prix, sont des gens brillants qui ont créé des choses assez extraordinaires. Et et c'est ces données-là qui sont à l'intérieur. Et cette machine qui crée, cette IA qui crée les données en ce moment, elle se nourrit de de milliers d'années, enfin de depuis qu'on a des traces en fait, depuis qu'on on sauvegarde des traces et que ensuite on les a numérisées, elle elle patauge dans tout ça. Et là il y a des lumières, il y a des choses intéressantes, il y a des rapprochements. Alors ce qu'elle c'est pas faire, c'est faire des métaphores. Elle fait des analogies, cette machine. Elle a tout ce jeu de données, elle a des contraintes et elle va jouer par analogie, elle va tricoter comme ça. Voilà, \n",
      "=============\n",
      "pas faire, c'est faire des métaphores. Elle fait des analogies, cette machine. Elle a tout ce jeu de données, elle a des contraintes et elle va jouer par analogie, elle va tricoter comme ça. Voilà, bon, c'est pas de l'art, c'est pas de la créativité, c'est de l'optimisation mathématique dans un énorme possible. Mais cet énorme possible, il joue pour beaucoup sur ce qu'on obtient à la fin. Puisque sinon, c'est juste une moulinette algorithmique. Et le jour où les données seront de moins bonne qualité, parce que les gens voudront plus créer ou ce sera les machines qui créeront. Et donc elles vont boucler au bout d'un moment et sans doute descendre en créativité. C'est nous qui apportons cette touche de nouveau, de vision métaphorique, d'inconscient, on en parlera après, qui va venir troubler tout ça, c'est l'humain. [Nathan Devert]: Est-ce qu'on peut pas dire quand même en en un sens que euh dans quand on écrit un un texte et qu'on est un humain euh euh euh la créativité qui est la \n",
      "=============\n",
      "tout ça, c'est l'humain. [Nathan Devert]: Est-ce qu'on peut pas dire quand même en en un sens que euh dans quand on écrit un un texte et qu'on est un humain euh euh euh la créativité qui est la nôtre euh est en un sens une addition de trucs, de méthode, d'habitude, de de de réflexes et que à cet égard, ça révèle aussi ce qu'il y avait peut-être Valentin Husson, de de mécanique dans l'écriture d'un Hervé Le Tellier ou de ou de n'importe qui parmi nous. [Valentin Husson]: Oui, il y a peut-être quelque chose de de mécanique, mais je voudrais revenir sur cette sur cette chose-là. Si Hervé Le Tellier, si l'humain en règle générale se croit d'emblée vaincu et par avance vaincu par l'intelligence artificielle, il y a un billet psychologique. C'est qu'il y a une angoisse fondamentale de l'être humain à l'égard de la machine et cela depuis des siècles. Ça Freud disait, il y a trois blessures narcissiques de l'homme. La première, c'est Copernic, nous ne sommes plus au centre de l'univers, la \n",
      "=============\n",
      "humain à l'égard de la machine et cela depuis des siècles. Ça Freud disait, il y a trois blessures narcissiques de l'homme. La première, c'est Copernic, nous ne sommes plus au centre de l'univers, la seconde c'est Darwin, nous ne sommes plus premiers dans l'ordre de la création, la troisième c'est la psychanalyse, l'homme n'est plus maître dans sa propre maison. J'en ajouterai une quatrième, c'est-à-dire que avec l'intelligence artificielle, nous avons l'impression que notre conscience n'est plus maîtresse d'elle-même. Nous ne sommes plus les les les les maîtres de notre intelligence. Et je crois que c'est une angoisse c'est une angoisse fondamentale de perte de de perte de maîtrise. Alors vous disiez tout à l'heure que en effet quand on est par exemple étudiant, on apprend des mécanismes. mais justement ce qui fait l'humain et je dirais le l'honneur et la beauté de l'humanité, c'est que justement nous arrivons à nous désautomatiser désautomatiser, pardon, comme disait Bernard \n",
      "=============\n",
      "mais justement ce qui fait l'humain et je dirais le l'honneur et la beauté de l'humanité, c'est que justement nous arrivons à nous désautomatiser désautomatiser, pardon, comme disait Bernard Stigler. Nous avons des automatismes, mais il y a quelque chose de formidable chez l'homme, c'est que à un moment donné, il peut faire rupture, il peut bifurquer, ça s'appelle la créativité, ça s'appelle l'inventivité. Quand appeler ça le génie ce qui est inimitable. Et en en ce sens-là, je ne désespère pas de l'être humain quant à la pensée, quant à l'humour, peut-être qu'on en parlera, mais il y a des choses proprement humaines que l'intelligence artificielle ne pourra jamais reproduire. [Nathan Devert]: Laurence Devillers, juste pour pour euh euh continuer cette discussion mais en en en en faisant aussi référence à une autre forme d'inquiétude qui a pu être rapportée par par l'intelligence artificielle. On a assisté en 2023 pendant 6 mois, enfin entre mai et septembre, euh à un mouvement \n",
      "=============\n",
      "aussi référence à une autre forme d'inquiétude qui a pu être rapportée par par l'intelligence artificielle. On a assisté en 2023 pendant 6 mois, enfin entre mai et septembre, euh à un mouvement vraiment inédit euh dans l'histoire sociale américaine, une grève euh de scénaristes euh à Hollywood, euh qui ne s'était pas produite dans de cette ampleur, avec cette ampleur depuis, je crois, 40 ans, euh dont la revendication première était leur inquiétude d'être remplacé euh par l'intelligence artificielle, par les producteurs dans l'écriture de de de de scénarios. Qu'est-ce que ça révèle aussi des inquiétudes professionnelles très concrètes euh que cette révolution technologique peut susciter ? [Laurence Devillers]: Alors, elle suicide effectivement toutes ces inquiétudes avec sûrement des métiers qui vont disparaître. On a tort de penser que on est super intelligent en tout et qu'on va rester là. Dès lors que on fait des copier-coller de ce que font ces machines dans beaucoup de travaux, \n",
      "=============\n",
      "qui vont disparaître. On a tort de penser que on est super intelligent en tout et qu'on va rester là. Dès lors que on fait des copier-coller de ce que font ces machines dans beaucoup de travaux, mes étudiants font ça dans leurs dissertation, et ben c'est une perte de pouvoir. Les ingénieurs qui codes avec des codes faits par des machines, c'est une perte de pouvoir. D'abord parce qu'ils ne comprennent pas tout ce qu'ils font, et puis que ils sont désapprennent. Donc, attention à ce côté-là, là, on pourrait être remplacé. Dès lors que vous utilisez trop ces machines et que vous formatez sur cette information, vous risquez euh d'être remplacé. Alors qu'est-ce que dans cette machine fait que il y a une sorte de créativité artificielle. C'est qu'en fait, on a volontairement. \n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "for c in all_docs[\"documents\"]:\n",
    "    print(c, \"\\n=============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbb31f",
   "metadata": {},
   "source": [
    "#### Création ou chargement d'un graphe existant:\n",
    "Saisir l'action désirée dans le prompt qui s'affiche en haut du notebook, suivre instructions\n",
    "\n",
    "Par défaut prise en compte des 20 premières pages pour l'exemple, modifier ligne 11 `for doc in docx_docs[:20]:`\n",
    "\n",
    "Si vous voulez charger un graphe déjà crée, retrouver son nom dans le fichier `storage/graphrag_hashes.json`, attribut `Nom du doc`\n",
    "\n",
    "Si vous voulez modifier le LLM utilisé pour la création du graphe ou sa lecture, allez dans `pathrag_retriever.py`, ligne 34 et 35, et prenez un modèle valide sur openrouter (attention à prendre un modèle qui supporte les `structured_outputs`, à filtrer à droite dans la liste des `Supported parameters`)\n",
    "\n",
    "**Important**: renseigner votre clé api openrouter sur le script `pathrag_retriever.py`, ligne 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b63b4a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PathRAG:Logger initialized for working directory: /home/robin/MVP/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2\n",
      "INFO:PathRAG:Creating working directory /home/robin/MVP/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2\n",
      "INFO:PathRAG:Load KV llm_response_cache with 0 data\n",
      "INFO:PathRAG:Load KV full_docs with 0 data\n",
      "INFO:PathRAG:Load KV text_chunks with 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/robin/MVP/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2/vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/robin/MVP/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2/vdb_relationships.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/robin/MVP/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2/vdb_chunks.json'} 0 data\n",
      "INFO:PathRAG:[New Docs] inserting 1 docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nom de votre graphe est graph\n",
      "Génération du hash\n",
      "load hashes\n",
      "Chargement de l'historique de hashage Graph RAG\n",
      "check hash\n",
      "Nouveau document identifié\n",
      "Nouveau document identifié\n",
      "da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2\n",
      "Création de la chaîne Graph RAG en cours\n",
      "Temps estimé: 84 secondes\n",
      "Consommation de tokens estimée: 44820 tokens (90% input / 10% output)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|██████████| 1/1 [00:00<00:00, 177.14doc/s]\n",
      "INFO:PathRAG:[New Chunks] inserting 3 chunks\n",
      "INFO:PathRAG:Inserting 3 vectors to chunks\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.02s/batch]\n",
      "INFO:PathRAG:[Entity Extraction]...\n",
      "Extracting entities from chunks:   0%|          | 0/3 [00:00<?, ?chunk/s]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 20 entities(duplicated), 17 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities from chunks:  33%|███▎      | 1/3 [00:26<00:52, 26.48s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 2 chunks, 30 entities(duplicated), 24 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities from chunks:  67%|██████▋   | 2/3 [00:42<00:20, 20.30s/chunk]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ Processed 3 chunks, 49 entities(duplicated), 34 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities from chunks: 100%|██████████| 3/3 [00:54<00:00, 18.11s/chunk]\n",
      "INFO:PathRAG:Inserting entities into storage...\n",
      "Inserting entities: 100%|██████████| 40/40 [00:00<00:00, 4347.78entity/s]\n",
      "INFO:PathRAG:Inserting relationships into storage...\n",
      "Inserting relationships: 100%|██████████| 31/31 [00:00<00:00, 7124.57relationship/s]\n",
      "INFO:PathRAG:Inserting 40 vectors to entities\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:06<00:00,  3.17s/batch]\n",
      "INFO:PathRAG:Inserting 31 vectors to relationships\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.91s/batch]\n",
      "INFO:PathRAG:Writing graph with 40 nodes, 31 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la chaîne Graph RAG en 86 secondes\n",
      "Création du visuel du graphe de connaissances\n"
     ]
    }
   ],
   "source": [
    "# appliquer nest_asyncio uniquement sur notebook pour corriger l'erreur de loop event\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# remetre à plat le text\n",
    "filename=\"data/audio-text.txt\"\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_docs = loader.load()\n",
    "text=\"\"\n",
    "for doc in txt_docs:\n",
    "    text+=doc.page_content\n",
    "\n",
    "\n",
    "r=input(\"Saisir 'C' pour créer un nouveau graphe, 'L' pour charger un graphe existant\")\n",
    "\n",
    "# créer un nouveau graphe\n",
    "messages=None\n",
    "if r=='C':\n",
    "    doc_name_graph=input('Saisir un nom unique pour votre graphe')\n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "    messages= create_graphdb(\n",
    "        text=text, \n",
    "        doc_name=doc_name_graph, # il faut donner un nom unique permettant d'identifier et charger le graph les prochaines fois\n",
    "    )\n",
    "# charger un graphe existant\n",
    "elif r=='L':\n",
    "    doc_name_graph=input('Saisir le nom du graphe à charger')\n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "\n",
    "    messages=load_existing_graphdb(doc_name_graph)\n",
    "else:\n",
    "    print('Option invalide')\n",
    "\n",
    "\n",
    "\n",
    "if messages:\n",
    "    pipeline_args={}\n",
    "    for feedback in messages:\n",
    "        if isinstance(feedback, str):\n",
    "            print(feedback)\n",
    "        elif isinstance(feedback, dict):\n",
    "            pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"]=feedback[\"pipeline_args\"]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6de14e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:PathRAG:kw_prompt result:\n",
      "INFO:PathRAG:{\n",
      "  \"high_level_keywords\": [\"Principaux thèmes\", \"Questions posées\", \"Analyse de texte\"],\n",
      "  \"low_level_keywords\": [\"Thèmes clés\", \"Interrogations\", \"Compréhension du texte\", \"Interprétation\"]\n",
      "}\n",
      "INFO:PathRAG:Local query uses 30 entites, 15 relations, 3 text units\n",
      "INFO:PathRAG:Global query uses 29 entites, 29 relations, 3 text units\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response all ready\n",
      "# Analyse des Thèmes et Questions Soulevées\n",
      "\n",
      "## Principaux Thèmes du Texte\n",
      "\n",
      "### 1. **Limitations de l'IA dans la Créativité**\n",
      "- La discussion souligne que les IA comme ChatGPT excellent dans des **exercices de style contraints** (ex. : nouvelle policière avec un début et une fin imposés), mais ne font qu'**optimiser des données existantes**, sans réelle innovation.\n",
      "- **Exemple concret** : La compétition littéraire entre Hervé Le Tellier et ChatGPT (organisée par *Le Nouvel Obs*), où l'IA a produit un texte perçu comme plus drôle et original, mais basé sur la réutilisation de données préexistantes.\n",
      "- **Critique majeure** : Laurence Devillers et Daniel Andler insistent sur l'absence de **motivation humaine** (lien créateur-public) et de **processus créatif profond** chez l'IA.\n",
      "\n",
      "### 2. **Impact Psychologique et Sociétal de l'IA**\n",
      "- **Anxiété professionnelle** : Illustration avec la **grève des scénaristes à Hollywood (2023)**, craignant le remplacement par l'IA. Laurence Devillers évoque aussi les risques pour les étudiants et ingénieurs dépendants de l'IA (perte de compétences critiques).\n",
      "- **Blessure narcissique** : Valentin Husson étend la théorie de Freud en ajoutant l'IA comme **quatrième blessure** (après Copernic, Darwin, et la psychanalyse), soulignant la peur de perdre le contrôle de son intelligence face aux machines.\n",
      "\n",
      "### 3. **Débat sur la Nature de la Créativité**\n",
      "- **Créativité humaine vs artificielle** : \n",
      "  - L'humain peut **rompre les automatismes** (référence à Bernard Stiegler) et innover, contrairement à l'IA.\n",
      "  - **Risque futur** : Si l'IA génère des données à partir de ses propres sorties, la qualité créative pourrait décliner (\"effet boucle\").\n",
      "\n",
      "---\n",
      "\n",
      "## Questions Clés à Explorer\n",
      "\n",
      "1. **Éthique et Dépendance Technologique** :  \n",
      "   - Dans quels domaines l'IA devrait-elle être limitée pour préserver l'autonomie humaine (ex. : éducation, création artistique) ?  \n",
      "   - Comment réguler l'utilisation des données numérisées pour éviter leur appauvrissement ?\n",
      "\n",
      "2. **Philosophique** :  \n",
      "   - L'IA révèle-t-elle une **mécanisation sous-jacente** de la créativité humaine (ex. : écriture \"mécanique\" d'Hervé Le Tellier) ?  \n",
      "   - Freud aurait-il considéré l'IA comme une menace existentielle ou un outil de connaissance de soi ?\n",
      "\n",
      "3. **Pratique** :  \n",
      "   - Quelles mesures concrètes pourraient atténuer les craintes des professionnels (scénaristes, ingénieurs) face à l'IA ?  \n",
      "   - Comment former les étudiants à utiliser l'IA sans perdre leur esprit critique ?\n",
      "\n",
      "---\n",
      "\n",
      "## Perspectives Complémentaires\n",
      "- **Cas d'usage positif** : Mention des **radios des cancers précoces**, où l'IA surpasse l'humain en diagnostic, montrant que ses forces varient selon les contextes.\n",
      "- **Évolution future** : Le débat sur la **qualité des données** (actuellement riches, mais potentiellement dégradées si générées par l'IA elle-même) reste ouvert.  \n",
      "\n",
      "*Sources citées : Interventions de Laurence Devillers (CNRS/Sorbonne), Daniel Andler, Valentin Husson, et références à Freud et Hervé Le Tellier.*"
     ]
    }
   ],
   "source": [
    "from rag.PathRAG import QueryParam\n",
    "import asyncio\n",
    "\n",
    "def stream_pathRAG_response(stream_resp):\n",
    "    async def stream_response():        \n",
    "        # Process the async generator\n",
    "        async for chunk in stream_resp:\n",
    "            print(chunk or \"\", end=\"\")\n",
    "\n",
    "\n",
    "    # Run in Streamlit's existing event loop\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(stream_response())\n",
    "\n",
    "# question=\"résume ce texte dans sa langue source\"\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "\n",
    "resp=pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"rag\"].query(query= question, param=QueryParam(mode=\"hybrid\", stream=True))\n",
    "\n",
    "stream_pathRAG_response(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f595f8",
   "metadata": {},
   "source": [
    "Renseigner votre clé d'api sur la ligne 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84137027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.schema.document import Document\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY_REF=os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "\n",
    "class RAG_hybrid():\n",
    "    def __init__(self, model):\n",
    "        self.model=model\n",
    "        self.retrieved_docs=[]\n",
    "        self.semantic_retriever_topK=10\n",
    "        self.sparse_retriever_topK=10\n",
    "        self.history=[]\n",
    "        self.llm_client = AsyncOpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=API_KEY_REF,\n",
    "        )\n",
    "        self.reranker_llm=\"mistralai/mistral-small-3.1-24b-instruct\"\n",
    "        self.reranker_score_thresh=5\n",
    "        self.reranked_doc=[]\n",
    "\n",
    "    def semanticRetriever(self):\n",
    "        # 1. Semantic Retriever (Chroma + OllamaEmbeddings)\n",
    "        embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "        chroma_db = Chroma(\n",
    "            persist_directory=f'storage/vector_stores/{doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "            collection_name=doc_name_hybrid.replace(\" \",\"_\"),\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        semantic_retriever=chroma_db.as_retriever(search_type=\"mmr\", k=self.semantic_retriever_topK)\n",
    "\n",
    "        self.chroma_db=chroma_db\n",
    "        self.semantic_retriever=semantic_retriever\n",
    "    \n",
    "    def sparseRetriever(self):\n",
    "        # 2. Sparse Retriever (TF-IDF)\n",
    "\n",
    "        # Récupérer TOUS les documents depuis Chroma\n",
    "        all_data = self.chroma_db.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "        # Convertir en liste de `Document` objects pour LangChain\n",
    "        docs = [\n",
    "            Document(page_content=text, metadata=meta or {})  # <-- Si meta est None, on met {}\n",
    "            for text, meta in zip(all_data[\"documents\"], all_data[\"metadatas\"])\n",
    "        ]\n",
    "\n",
    "        # Créer le retriever TF-IDF\n",
    "        sparse_retriever = TFIDFRetriever.from_documents(\n",
    "            documents=docs,\n",
    "            k=self.sparse_retriever_topK,\n",
    "            tfidf_params={\"min_df\": 1, \"ngram_range\": (1, 2)}\n",
    "        )\n",
    "\n",
    "        self.sparse_retriever= sparse_retriever\n",
    "    \n",
    "    def ensembleRetriever(self):\n",
    "        # 3. Ensemble Retriever (Semantic + Sparse)\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.semantic_retriever, self.sparse_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        self.ensemble_retriever=ensemble_retriever\n",
    "\n",
    "    async def reranker(self, results, query):\n",
    "\n",
    "\n",
    "        async def llm_eval(doc, query):\n",
    "            system_prompt=\"\"\"\n",
    "                You're an expert assistant in reranking documents against a question.\n",
    "                Your role is to compare the question with a document and give a score from 0 to 10, where:\n",
    "                0=document out of context, unable to answer the question\n",
    "                10=highly relevant document, able to answer the question\n",
    "                                \n",
    "                The expected final output is the score in json format\n",
    "                Example:\n",
    "                ```json{\"score\": 5}```\n",
    "                \n",
    "                Always end your answer with this format                \n",
    "            \"\"\"            \n",
    "            response = await self.llm_client.chat.completions.create(\n",
    "                model=self.reranker_llm,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"La question est: {query}\\n Le document à évaluer est le suivant\\n: {doc}\" }\n",
    "                ],\n",
    "                temperature=0,\n",
    "            )\n",
    "            # Post-process to extract only the JSON part if extra text is present\n",
    "            content = response.choices[0].message.content\n",
    "            # Try to extract the JSON block if the model adds extra text\n",
    "            match = re.search(r\"\\{.*?\\}\", content, re.DOTALL)\n",
    "            if match:\n",
    "                content = match.group(0)\n",
    "\n",
    "            # extract score\n",
    "            score=None\n",
    "            try:\n",
    "                score=content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "                \n",
    "                score= json.loads(score)\n",
    "                score=score[\"score\"]\n",
    "            except Exception as e:\n",
    "                print(e)                \n",
    "            \n",
    "            return {\"content\": doc, \"score\": score}\n",
    "\n",
    "\n",
    "        tasks=[llm_eval(doc.page_content, query) for doc in results]\n",
    "        scored_docs= await asyncio.gather(*tasks)\n",
    "        i=1\n",
    "\n",
    "        for doc in scored_docs:\n",
    "          \n",
    "            print(f'chunk {i} score: {doc[\"score\"]}')\n",
    "            i+=1\n",
    "\n",
    "        filtred_docs=[d for d in scored_docs if d[\"score\"]>=self.reranker_score_thresh]\n",
    "        # print(f\"scored docs; \\n{scored_docs}\")\n",
    "        self.reranked_doc=filtred_docs\n",
    "\n",
    "        return filtred_docs\n",
    "\n",
    "    async def ask_llm(self, query):\n",
    "        # 5. Final processing step with an LLM (e.g., OpenAI via OpenRouter)\n",
    "\n",
    "        # init retrievers\n",
    "        self.semanticRetriever()\n",
    "        self.sparseRetriever()\n",
    "        self.ensembleRetriever()\n",
    "\n",
    "        # retrieve relevant docs\n",
    "        results = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        print(f\"Nb of retrieved docs: {len(results)}\")\n",
    "\n",
    "        # rerank\n",
    "        scored_results=await self.reranker(results, query)\n",
    "        \n",
    "        # Concatenate retrieved documents for context\n",
    "        context = \"\\n\".join([f\"Fragment: \\n{doc['content']}\\n\" for doc in scored_results])\n",
    "\n",
    "        print(f\"Context lenght: {len(context.split(' '))} words\")\n",
    "        llm_prompt = f\"\"\"\n",
    "            Answer the question based **only** on the provided context.  \n",
    "\n",
    "            - If the context contains enough information to provide a complete or partial answer, use it to formulate a detailed and factual response.  \n",
    "            - If the context lacks relevant information, respond with: \"I don't know.\"  \n",
    "\n",
    "            ### **Context:**  \n",
    "            {context}  \n",
    "\n",
    "            ### **Question:**  \n",
    "            {query}  \n",
    "\n",
    "            ### **Answer:**  \n",
    "            Provide a clear, factual, and well-structured response based on the available context. Avoid speculation or adding external knowledge.  \n",
    "        \"\"\"\n",
    "\n",
    "        llm_completion = await self.llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in document Q/A and document synthesis\"},\n",
    "                {\"role\": \"user\", \"content\": llm_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        final_answer = \"\"\n",
    "        print(\"Réponse:\\n=========\")\n",
    "        async for chunk in llm_completion:\n",
    "            if hasattr(chunk.choices[0].delta, \"content\") and chunk.choices[0].delta.content:\n",
    "                final_answer += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        \n",
    "        self.history+=[\n",
    "            {\"role\": \"user\", 'content': query},\n",
    "            {\"role\": \"assistant\", \"content\": final_answer}\n",
    "        ]\n",
    "        \n",
    "        return final_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a224490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7941/2437484125.py:145: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = self.ensemble_retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of retrieved docs: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 score: 3\n",
      "chunk 2 score: 2\n",
      "chunk 3 score: 9\n",
      "chunk 4 score: 3\n",
      "chunk 5 score: 3\n",
      "chunk 6 score: 3\n",
      "chunk 7 score: 2\n",
      "chunk 8 score: 3\n",
      "chunk 9 score: 3\n",
      "chunk 10 score: 3\n",
      "chunk 11 score: 2\n",
      "chunk 12 score: 8\n",
      "Context lenght: 309 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse:\n",
      "=========\n",
      "### **Principaux thèmes du texte :**\n",
      "\n",
      "1. **Philosophie de l'esprit et intelligence artificielle (IA)** :\n",
      "   - Le texte explore la question de savoir si l'intelligence peut être implémentée dans des machines, remettant en cause l'idée que l'intelligence est exclusive aux cerveaux biologiques ou à une âme.\n",
      "   - Il aborde la capacité de l'IA à imiter les œuvres de l'esprit humain et la mécanisation de la vie mentale.\n",
      "\n",
      "2. **Singularité humaine vs. machines** :\n",
      "   - Le texte discute de la singularité de la conscience humaine par rapport aux machines, soulignant que l'IA est développée à partir d'une représentation, peut-être incomplète ou biaisée, de la conscience humaine.\n",
      "   - Il invite à explorer à nouveau la singularité humaine face à l'IA.\n",
      "\n",
      "3. **Blessures narcissiques de l'humanité** :\n",
      "   - Le texte mentionne les trois blessures narcissiques de l'homme selon Freud : Copernic (nous ne sommes plus au centre de l'univers), Darwin (nous ne sommes plus premiers dans l'ordre de la création), et la psychanalyse (l'homme n'est plus maître dans sa propre maison).\n",
      "   - Il ajoute une quatrième blessure narcissique : l'IA, qui remet en cause la maîtrise de notre conscience et de notre intelligence.\n",
      "\n",
      "4. **Angoisse de perte de maîtrise** :\n",
      "   - Le texte aborde l'angoisse fondamentale de perte de maîtrise liée à l'IA, soulignant que l'humanité ne serait plus maîtresse de son intelligence.\n",
      "   - Il mentionne la capacité humaine à se désautomatiser, ce qui distingue l'humain des machines.\n",
      "\n",
      "### **Questions qui peuvent être posées :**\n",
      "\n",
      "1. **Philosophie de l'esprit et IA** :\n",
      "   - L'intelligence peut-elle être implémentée dans des machines ?\n",
      "   - L'IA peut-elle remplacer un jour la conscience humaine ?\n",
      "   - Comment l'IA imite-t-elle les œuvres de l'esprit humain ?\n",
      "\n",
      "2. **Singularité humaine vs. machines** :\n",
      "   - Qu'est-ce qui rend la conscience humaine unique par rapport aux machines ?\n",
      "   - Comment la représentation de la conscience humaine influence-t-elle le développement de l'IA ?\n",
      "\n",
      "3. **Blessures narcissiques de l'humanité** :\n",
      "   - Quelles sont les trois blessures narcissiques de l'homme selon Freud ?\n",
      "   - Pourquoi l'IA est-elle considérée comme une quatrième blessure narcissique ?\n",
      "\n",
      "4. **Angoisse de perte de maîtrise** :\n",
      "   - Pourquoi l'IA suscite-t-elle une angoisse de perte de maîtrise ?\n",
      "   - En quoi la capacité humaine à se désautomatiser distingue-t-elle l'humain des machines ?"
     ]
    }
   ],
   "source": [
    "rag_hybrid=RAG_hybrid(model=\"mistralai/mistral-small-3.2-24b-instruct\")\n",
    "# 4. Ask a question\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "results = await rag_hybrid.ask_llm(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
