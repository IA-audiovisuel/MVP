{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b419c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.créer un nouvel env conda à partir du terminal\n",
    "# conda create --name asr_rag python=3.10\n",
    "\n",
    "# 2. activer cet environnement\n",
    "# conda activate asr_rag\n",
    "\n",
    "# 3. installer les dépendences\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# 4. installer ollama: https://www.ollama.com/download\n",
    "\n",
    "\n",
    "# 5. installer le modème d'OllamaEmbeddings\n",
    "# ollama pull embeddinggemma\n",
    "\n",
    "# 6. créer une clé api sur openrouter pour utiliser des llm gratuitement\n",
    "\n",
    "# 7. mettre votre clé dans le fichier .env (racine du repertoire courant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0299e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chougar/miniconda3/envs/pathrag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from pathrag_retriever import create_graphdb, load_existing_graphdb, load_knowledgeGraph_vis\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "OPENROUTER_API_KEY=os.getenv(\"OPENROUTER_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbb31f",
   "metadata": {},
   "source": [
    "#### Création ou chargement d'un graphe existant:\n",
    "> Lorsque vous executer la cellule ci dessous, vous avez 2 options proposées:\n",
    "> 1. Créer un nouveau graphe\n",
    "> 2. Charger un graphe existant\n",
    "> \n",
    "> Un prompt de sélection s'affichera en haut du notebook<br>\n",
    "> Saisir l'action désirée dans le , suivre instructions\n",
    "\n",
    "> Si vous voulez charger un graphe déjà crée et que vous ne savez plus son nom, retrouver le dans le fichier `graphrag_hashes.json` (dans la racine du dossier), attribut `Nom du doc`\n",
    "\n",
    "> Si vous voulez modifier le LLM utilisé pour la création du graphe ou sa lecture, allez dans `pathrag_retriever.py`, sous la ligne 36( Choix du LLM OpenRouter), et prenez un modèle valide sur openrouter (attention à prendre un modèle qui supporte les `structured_outputs`, sur le site openrouter, à filtrer sur le paneau à gauche dans la liste des `Supported parameters`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29614848",
   "metadata": {},
   "source": [
    "#### 1. Créer un graphe, ou charger en un déjà crée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f88c2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom du doc</th>\n",
       "      <th>Titre auto</th>\n",
       "      <th>Taille du texte (en car)</th>\n",
       "      <th>Date de création</th>\n",
       "      <th>doc_category</th>\n",
       "      <th>rag_type</th>\n",
       "      <th>graph_attrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2</th>\n",
       "      <td>IA conscience</td>\n",
       "      <td>None</td>\n",
       "      <td>13582</td>\n",
       "      <td>2025-11-19 14:37:20.456479</td>\n",
       "      <td>None</td>\n",
       "      <td>graph</td>\n",
       "      <td>{'LLM utilisé': 'deepseek/deepseek-chat-v3-032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5</th>\n",
       "      <td>L-IA-notre-deuxieme-conscience</td>\n",
       "      <td>None</td>\n",
       "      <td>46790</td>\n",
       "      <td>2025-11-25 18:17:35.756002</td>\n",
       "      <td>None</td>\n",
       "      <td>graph</td>\n",
       "      <td>{'LLM utilisé': 'deepseek/deepseek-chat-v3-032...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        Nom du doc  \\\n",
       "da824d7bce695f72eb26e2aceeedd4a69a07419e461e226...                   IA conscience   \n",
       "9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabe...  L-IA-notre-deuxieme-conscience   \n",
       "\n",
       "                                                   Titre auto  \\\n",
       "da824d7bce695f72eb26e2aceeedd4a69a07419e461e226...       None   \n",
       "9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabe...       None   \n",
       "\n",
       "                                                   Taille du texte (en car)  \\\n",
       "da824d7bce695f72eb26e2aceeedd4a69a07419e461e226...                    13582   \n",
       "9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabe...                    46790   \n",
       "\n",
       "                                                              Date de création  \\\n",
       "da824d7bce695f72eb26e2aceeedd4a69a07419e461e226...  2025-11-19 14:37:20.456479   \n",
       "9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabe...  2025-11-25 18:17:35.756002   \n",
       "\n",
       "                                                   doc_category rag_type  \\\n",
       "da824d7bce695f72eb26e2aceeedd4a69a07419e461e226...         None    graph   \n",
       "9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabe...         None    graph   \n",
       "\n",
       "                                                                                          graph_attrs  \n",
       "da824d7bce695f72eb26e2aceeedd4a69a07419e461e226...  {'LLM utilisé': 'deepseek/deepseek-chat-v3-032...  \n",
       "9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabe...  {'LLM utilisé': 'deepseek/deepseek-chat-v3-032...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noms des graphs dispo:\n",
      " ['IA conscience' 'L-IA-notre-deuxieme-conscience']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "graphs_available=pandas.read_json(\"graphrag_hashes.json\")\n",
    "display(graphs_available.T)\n",
    "print(\"Noms des graphs dispo:\\n\", graphs_available.T[\"Nom du doc\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63b4a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Control:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chougar/miniconda3/envs/pathrag/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/chougar/miniconda3/envs/pathrag/lib/python3.10/site-packages/ipykernel/control.py\", line 23, in run\n",
      "    self.io_loop.start()\n",
      "  File \"/home/chougar/miniconda3/envs/pathrag/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/chougar/miniconda3/envs/pathrag/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/chougar/miniconda3/envs/pathrag/lib/python3.10/site-packages/nest_asyncio.py\", line 130, in _run_once\n",
      "    curr_task = curr_tasks.pop(self, None)\n",
      "NameError: free variable 'curr_tasks' referenced before assignment in enclosing scope. Did you mean: 'curr_task'?\n",
      "ERROR:asyncio:Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-3' coro=<Kernel.poll_control_queue() running at /home/chougar/miniconda3/envs/pathrag/lib/python3.10/site-packages/ipykernel/kernelbase.py:304> wait_for=<Future finished result=<Future at 0x...state=pending>> cb=[_chain_future.<locals>._call_set_state() at /home/chougar/miniconda3/envs/pathrag/lib/python3.10/asyncio/futures.py:392]>\n",
      "INFO:PathRAG:Logger initialized for working directory: /home/chougar/Documents/GitHub/IA-audiovisue-sync/notebooks/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5\n",
      "INFO:PathRAG:Load KV llm_response_cache with 0 data\n",
      "INFO:PathRAG:Load KV full_docs with 1 data\n",
      "INFO:PathRAG:Load KV text_chunks with 11 data\n",
      "INFO:PathRAG:Loaded graph from /home/chougar/Documents/GitHub/IA-audiovisue-sync/notebooks/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/graph_chunk_entity_relation.graphml with 84 nodes, 56 edges\n",
      "INFO:nano-vectordb:Load (82, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/IA-audiovisue-sync/notebooks/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/vdb_entities.json'} 82 data\n",
      "INFO:nano-vectordb:Load (56, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/IA-audiovisue-sync/notebooks/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/vdb_relationships.json'} 56 data\n",
      "INFO:nano-vectordb:Load (11, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/IA-audiovisue-sync/notebooks/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/vdb_chunks.json'} 11 data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nom de votre graphe est L-IA-notre-deuxieme-conscience\n",
      "\n",
      "        ----------------\n",
      "        #### Graph RAG retriever\n",
      "        Chargement de la base Graph RAG\n",
      "    \n",
      "**✅ Graph RAG chargé**\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# remetre à plat le text\n",
    "filename=\"audio-text.txt\"\n",
    "doc_name_graph='L-IA-notre-deuxieme-conscience'\n",
    "\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_docs = loader.load()\n",
    "text=\"\"\n",
    "for doc in txt_docs:\n",
    "    text+=doc.page_content\n",
    "\n",
    "\n",
    "r=input(\"Saisir 'C' pour créer un nouveau graphe, 'L' pour charger un graphe existant\")\n",
    "\n",
    "# créer un nouveau graphe\n",
    "messages=None\n",
    "if r=='C':    \n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "    messages= create_graphdb(\n",
    "        text=text, \n",
    "        doc_name=doc_name_graph, # il faut donner un nom unique permettant d'identifier et charger le graph les prochaines fois        \n",
    "    )\n",
    "# charger un graphe existant\n",
    "elif r=='L':    \n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "\n",
    "    messages=load_existing_graphdb(\n",
    "        doc_name_graph,\n",
    "        OPENROUTER_MODEL_graph_read=\"deepseek/deepseek-v3.2\")\n",
    "else:\n",
    "    print('Option invalide')\n",
    "\n",
    "\n",
    "\n",
    "if messages:\n",
    "    pipeline_args={}\n",
    "    for feedback in messages:\n",
    "        if isinstance(feedback, str):\n",
    "            print(feedback)\n",
    "        elif isinstance(feedback, dict):\n",
    "            pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"]=feedback[\"pipeline_args\"]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1543579",
   "metadata": {},
   "source": [
    "#### 2. Poser vos questions au graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6de14e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:PathRAG:kw_prompt result:\n",
      "INFO:PathRAG:{\n",
      "  \"high_level_keywords\": [\"Thèmes principaux\", \"Questions\", \"Analyse de texte\"],\n",
      "  \"low_level_keywords\": [\"Thèmes\", \"Questions posées\", \"Compréhension\", \"Interprétation\"]\n",
      "}\n",
      "INFO:PathRAG:Local query uses 40 entites, 15 relations, 4 text units\n",
      "INFO:PathRAG:Global query uses 55 entites, 40 relations, 4 text units\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response all ready\n",
      "==============\n",
      "\n",
      "LLM utilisé pour les réponses;\n",
      " deepseek/deepseek-v3.2\n",
      "### Principaux thèmes du texte\n",
      "\n",
      "Le texte, issu d’une discussion sur France Culture, articule plusieurs thèmes centraux autour de l’intelligence artificielle (IA), de ses limites et de ses implications sociétales.\n",
      "\n",
      "**1. Les limites de l’IA et la distinction avec l’humain**\n",
      "Un thème récurrent est la délimitation entre les capacités de l’IA et les facultés humaines. Plusieurs intervenants, comme Laurence Devillers et Daniel Andler, insistent sur le fait que l’IA, bien qu’impressionnante, n’est pas intelligente, consciente ou créative au sens humain. Elle fonctionne par optimisation statistique et combinatoire sur des données existantes, mais elle manque d’intention, de compréhension réelle, d’émotion et de la capacité à produire de véritables métaphores ou une créativité motivée intrinsèquement. Le paramètre de « température » dans ChatGPT, qui introduit de l’aléatoire pour simuler une créativité artificielle, est cité comme un exemple de cette mécanique dépourvue de véritable intention.\n",
      "\n",
      "**2. La créativité humaine face à l’automatisation**\n",
      "La créativité est présentée comme un domaine exclusivement humain, reposant sur la motivation intrinsèque, l’inconscient, la métaphore et la capacité à « se désautomatiser » (concept évoqué via Bernard Stigler). Valentin Husson souligne que l’humain peut rompre avec ses automatismes, ce qui constitue l’essence de l’inventivité et du génie. À l’inverse, l’IA ne peut produire que des analogies à partir des données qu’elle a ingérées. La grève des scénaristes d’Hollywood en 2023 est mentionnée comme un symptôme concret de la crainte que cette créativité mécanisée ne standardise et ne remplace le travail humain.\n",
      "\n",
      "**3. L’impact psychologique et sociétal : angoisse et manipulation**\n",
      "Valentin Husson introduit le concept freudien des « blessures narcissiques » de l’humanité, en proposant l’IA comme une quatrième blessure qui remet en cause la maîtrise de notre propre intelligence. Parallèlement, Laurence Devillers alerte sur les risques de manipulation par le langage, insistant sur l’urgence d’éduquer le public, notamment les enfants, à ces enjeux. Le texte évoque aussi l’influence quotidienne et souvent inconsciente de l’IA sur les comportements, ainsi que le danger de « coévolution humain-machine » où l’isolement et la dépendance affective envers des agents conversationnels pourraient saper le lien social.\n",
      "\n",
      "**4. Enjeux de pouvoir, responsabilité et déstabilisation politique**\n",
      "Un thème critique porte sur la course à la « super-intelligence » menée par les géants du numérique (les GAFAM). Cette quête est décrite comme un « Graal » marketing et politique, visant moins une avancée scientifique qu’une accumulation de pouvoir. Laurence Devillers dénonce notamment l’idée, évoquée par des figures comme Sam Altman, d’accorder une « personnalité juridique » à l’IA, qu’elle perçoit comme une manœuvre pour éluder les questions de responsabilité. Le texte souligne enfin les risques de déstabilisation des démocraties par des agendas transhumanistes et des campagnes de désinformation appuyées sur l’IA.\n",
      "\n",
      "**5. Perspective historique et philosophique**\n",
      "La discussion replace l’IA dans une longue histoire intellectuelle, du « rêve prométhéen » (analysé par Daniel Andler) qui aspire à reproduire l’intelligence humaine, jusqu’aux inquiétudes anciennes face aux nouvelles technologies. Valentin Husson rappelle ainsi que Socrate, dans le *Phèdre* de Platon, craignait déjà que l’écriture n’affaiblisse la mémoire et ne remplace les maîtres – un parallèle avec les craintes actuelles sur l’IA remplaçant les professeurs ou les créateurs.\n",
      "\n",
      "### Questions soulevées par le texte\n",
      "\n",
      "Le débat fait émerger plusieurs questions fondamentales, tant techniques que philosophiques et éthiques :\n",
      "\n",
      "*   **Sur la nature de l’intelligence et de la créativité** : Comment définir la compréhension, la conscience ou la créativité « vraie » ? Si nous-mêmes peinons à les circonscrire pour l’humain, sur quels critères pouvons-nous affirmer que l’IA en est dépourvue ?\n",
      "*   **Sur l’avenir du travail et de la valeur humaine** : Jusqu’à quel point la standardisation des tâches créatives par l’IA va-t-elle rendre les humains remplaçables ? Comment préserver et valoriser les compétences humaines irréductibles (comme la capacité à créer des métaphores ou à interpréter l’inconscient) dans un marché du travail transformé ?\n",
      "*   **Sur l’éducation et la résilience citoyenne** : Comment éduquer les individus, dès le plus jeune âge, à reconnaître et à se prémunir contre la manipulation par le langage algorithmique ? Quels savoirs et quelles critiques doivent faire partie d’une « littératie numérique » essentielle ?\n",
      "*   **Sur la gouvernance et l’éthique des technologies** : Qui est responsable des impacts sociaux et psychologiques des systèmes d’IA ? Comment réguler la course à la puissance menée par les géants du numérique pour éviter qu’elle ne mine la responsabilité démocratique et n’accentue les fractures sociales ?\n",
      "*   **Sur la condition humaine à l’ère numérique** : L’IA constitue-t-elle une nouvelle « blessure narcissique » pour l’humanité, ou peut-elle être intégrée comme les révolutions technologiques précédentes ? La quête de compagnie et de conseil auprès des agents conversationnels est-elle le symptôme d’une société déjà trop isolée, et quel avenir social cela dessine-t-il ?\n",
      "\n",
      "En conclusion, ce texte dépasse la simple évaluation technique de l’IA pour aborder une réflexion profonde sur ce qui définit l’humain, sur les risques de dépossession cognitive et sociale, et sur les batailles politiques et éducatives à mener pour garder le contrôle sur ces technologies transformatrices."
     ]
    }
   ],
   "source": [
    "from PathRAG import QueryParam\n",
    "import asyncio\n",
    "\n",
    "def stream_pathRAG_response(stream_resp):\n",
    "    async def stream_response():        \n",
    "        # Process the async generator\n",
    "        async for chunk in stream_resp:\n",
    "            print(chunk or \"\", end=\"\")\n",
    "\n",
    "\n",
    "    # Run in Streamlit's existing event loop\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(stream_response())\n",
    "\n",
    "# question=\"résume ce texte dans sa langue source\"\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "\n",
    "resp=pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"rag\"].query(query= question, param=QueryParam(mode=\"hybrid\", stream=True))\n",
    "\n",
    "print(\"==============\\n\")\n",
    "print(f\"LLM utilisé pour les réponses;\\n\", pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"llm_graph_QA\"])\n",
    "stream_pathRAG_response(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f595f8",
   "metadata": {},
   "source": [
    "============================\n",
    "### RAG vectoriel\n",
    "1. Embedding du document -> renseigner le nom de votre fichier dans `filename` et le nom de votre DB dans `doc_name_hybrid`\n",
    "2. Setup du retriever / reranker / llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76565f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from audio-text.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de chuncks: 17\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "# chargement et fragmentation du doc\n",
    "## Nom du doc à traiter\n",
    "filename=\"audio-text.txt\"\n",
    "\n",
    "## Nom pour la base vectorielle\n",
    "doc_name_hybrid=\"L-IA-notre-deuxieme-conscience_sample\" # nom de doc significatif\n",
    "\n",
    "\n",
    "\n",
    "#========= choix du modèle d'embedding\n",
    "\"\"\"\n",
    "    Le modèle choisi impacte la qualité du retriever, mais aussi le temps de traitement\n",
    "    Si le déploiement est prévu sur une VM limitée, un modèle plus petit est nécessaire\n",
    "    Explorer les comparatifs: https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "\"\"\"\n",
    "# Utiliser OllamaEmbeddings avec le modèle local \"embeddinggemma\"\n",
    "embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "\n",
    "\n",
    "# loader = UnstructuredFileLoader(filename)\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_doc = loader.load()\n",
    "print(f\"Loaded {len(txt_doc)} documents from {filename}\")\n",
    "\n",
    "\n",
    "#======== choix des paramètres de fragmentation\n",
    "\"\"\"\n",
    "    la taille du chunck_size est très important dans l'accès à une info précise\n",
    "    une plus petite taille permet de cibler de courts passages contenant l'info nécessaire à des réponses précises:\n",
    "        * lieu du projet\n",
    "        * dates du projet\n",
    "        * budget ...    \n",
    "    l'envoi de passages plus courts au llm évite une dispertion de son attention\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(txt_doc)\n",
    "\n",
    "# Filter out complex metadata (e.g., lists, dicts)\n",
    "docs = [Document(doc.page_content) for doc in docs]\n",
    "\n",
    "\n",
    "# Conversion des docs en embeddings \n",
    "chroma_db = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=f'./storage/vector_scores/{doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "    collection_name=doc_name_hybrid.replace(\" \",\"_\")\n",
    ")\n",
    "\n",
    "retriever=chroma_db.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ...existing code...\n",
    "all_docs = chroma_db.get()\n",
    "print(\"Nb de chuncks:\", len(all_docs['documents']))  # This will print the total number of docs stored\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f9772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nathan Devert]: France Culture. [Nathan Devert]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Comment expliquer les progrès phénoménaux que semble avoir accompli l'intelligence artificielle au cours de ces dernières années ? Depuis l'apparition de ChatGPT en novembre 2022, rapidement suivi par d'autres agents conversationnels, cette révolution technologique aux multiples aspects, paraît désormais capable d'exécuter de nombreuses tâches intellectuelles sur lesquelles l'esprit humain pensait jusqu'alors exercer un monopole. Écrire des articles, synthétiser des documents, traiter des données dans n'importe quel domaine, diagnostiquer une maladie, rédiger une dissertation, ou pourquoi pas, un scénario de film. Non contente de révolutionner le monde du travail, ses prouesses stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et \n",
      "=============\n",
      "stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et encore moins d'une âme, peut s'implémenter dans n'importe quelle machine, voir, comme s'inquiètent ou exultent certains, que l'IA sera un jour en mesure de remplacer notre conscience. À moins qu'il ne faille poser cette question à l'envers. Comment se fait-il que la machine puisse imiter les œuvres de de notre esprit ? Comment est-il possible en d'autres termes que notre vie mentale se révèle comme mécanisable, au même titre que nos facultés manuelles ? L'intelligence artificielle ne doit-elle pas sa capacité d'imitation au fait qu'elle a été théorisée, inventée, développée d'après une certaine représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre \n",
      "=============\n",
      "représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre propre pensée. L'intelligence artificielle, notre deuxième conscience. À l'occasion de la fête de la Science, j'aurai le plaisir d'en débattre sans préjuger aux côtés du mathématicien et philosophe Daniel Andler, de la professeur en intelligence artificielle et chercheuse Laurence Devillers et du philosophe Valentin Husson. [Narrateur radio]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Bonjour Laurence Devillers. [Laurence Devillers]: Bonjour. [Nathan Devert]: Vous êtes professeur en intelligence artificielle à Sorbonne Université, chercheuse au CNRS, auteur de \"L'IA, ange ou démon\" paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur \n",
      "=============\n",
      "paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur émérite à Sorbonne Université, membre de l'Académie des Sciences morales et politiques et auteur de \"Intelligence artificielle, intelligence humaine, la double énigme\" aux éditions Gallimard et bonjour Valentin Husson. [Valentin Husson]: Vous êtes philosophe et auteur de \"Folle résentimentale, petite philosophie des trolls\" chez Philosophie Magazine éditeur qui vient de paraître. Alors pour commencer cette cette discussion sur l'intelligence artificielle, je voulais vous soumettre une histoire qui a eu lieu récemment en mars 2025. Le Nouvel Obs a décidé d'organiser un concours littéraire entre deux auteurs, entre guillemets. Alors le premier était Hervé Le Tellier, pris Goncourt 2020, auteur de du célèbre roman l'anomalie et le deuxième était l'intelligence artificielle et plus précisément \n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "for c in all_docs[\"documents\"][:4]:\n",
    "    print(c, \"\\n=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84137027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.schema.document import Document\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "class RAG_hybrid():\n",
    "    def __init__(self, model):\n",
    "        self.model=model\n",
    "        self.retrieved_docs=[]\n",
    "        self.semantic_retriever_topK=10\n",
    "        self.sparse_retriever_topK=10\n",
    "        self.history=[]\n",
    "        self.llm_client = AsyncOpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "        )\n",
    "        self.reranker_llm=\"mistralai/mistral-small-3.1-24b-instruct\"\n",
    "        self.doc_name_hybrid=doc_name_hybrid\n",
    "        self.reranker_score_thresh=5\n",
    "        self.reranked_doc=[]\n",
    "\n",
    "    def semanticRetriever(self):\n",
    "        # 1. Semantic Retriever (Chroma + OllamaEmbeddings)\n",
    "        embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "        if self.doc_name_hybrid == 'None':\n",
    "            return \"Error: fournir le nom du document\"\n",
    "        \n",
    "        chroma_db = Chroma(\n",
    "            persist_directory=f'./storage/vector_scores/{self.doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "            collection_name=self.doc_name_hybrid.replace(\" \",\"_\"),\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        semantic_retriever=chroma_db.as_retriever(search_type=\"mmr\", k=self.semantic_retriever_topK)\n",
    "\n",
    "        self.chroma_db=chroma_db\n",
    "        self.semantic_retriever=semantic_retriever\n",
    "\n",
    "        return \"Success: ChromaDB setup avec succes\"\n",
    "    \n",
    "    def sparseRetriever(self):\n",
    "        # 2. Sparse Retriever (TF-IDF)\n",
    "\n",
    "        # Récupérer TOUS les documents depuis Chroma\n",
    "        all_data = self.chroma_db.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "        # Convertir en liste de `Document` objects pour LangChain\n",
    "        docs = [\n",
    "            Document(page_content=text, metadata=meta or {})  # <-- Si meta est None, on met {}\n",
    "            for text, meta in zip(all_data[\"documents\"], all_data[\"metadatas\"])\n",
    "        ]\n",
    "\n",
    "        # Créer le retriever TF-IDF\n",
    "        sparse_retriever = TFIDFRetriever.from_documents(\n",
    "            documents=docs,\n",
    "            k=self.sparse_retriever_topK,\n",
    "            tfidf_params={\"min_df\": 1, \"ngram_range\": (1, 2)}\n",
    "        )\n",
    "\n",
    "        self.sparse_retriever= sparse_retriever\n",
    "    \n",
    "    def ensembleRetriever(self):\n",
    "        # 3. Ensemble Retriever (Semantic + Sparse)\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.semantic_retriever, self.sparse_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        self.ensemble_retriever=ensemble_retriever\n",
    "\n",
    "    async def reranker(self, results, query):\n",
    "\n",
    "\n",
    "        async def llm_eval(doc, query):\n",
    "            system_prompt=\"\"\"\n",
    "                You're an expert assistant in reranking documents against a question.\n",
    "                Your role is to compare the question with a document and give a score from 0 to 10, where:\n",
    "                0=document out of context, unable to answer the question\n",
    "                10=highly relevant document, able to answer the question\n",
    "                                \n",
    "                The expected final output is the score in json format\n",
    "                Example:\n",
    "                ```json{\"score\": 5}```\n",
    "                \n",
    "                Always end your answer with this format                \n",
    "            \"\"\"            \n",
    "            response = await self.llm_client.chat.completions.create(\n",
    "                model=self.reranker_llm,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"La question est: {query}\\n Le document à évaluer est le suivant\\n: {doc}\" }\n",
    "                ],\n",
    "                temperature=0,\n",
    "            )\n",
    "            # Post-process to extract only the JSON part if extra text is present\n",
    "            content = response.choices[0].message.content\n",
    "            # Try to extract the JSON block if the model adds extra text\n",
    "            match = re.search(r\"\\{.*?\\}\", content, re.DOTALL)\n",
    "            if match:\n",
    "                content = match.group(0)\n",
    "\n",
    "            # extract score\n",
    "            score=None\n",
    "            try:\n",
    "                score=content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "                \n",
    "                score= json.loads(score)\n",
    "                score=score[\"score\"]\n",
    "            except Exception as e:\n",
    "                print(e)                \n",
    "            \n",
    "            return {\"content\": doc, \"score\": score}\n",
    "\n",
    "\n",
    "        tasks=[llm_eval(doc.page_content, query) for doc in results]\n",
    "        scored_docs= await asyncio.gather(*tasks)\n",
    "        i=1\n",
    "\n",
    "        for doc in scored_docs:\n",
    "          \n",
    "            print(f'chunk {i} score: {doc[\"score\"]}')\n",
    "            i+=1\n",
    "\n",
    "        filtred_docs=[d for d in scored_docs if int(d[\"score\"])>=self.reranker_score_thresh]\n",
    "        # print(f\"scored docs; \\n{scored_docs}\")\n",
    "        self.reranked_doc=filtred_docs\n",
    "\n",
    "        return filtred_docs\n",
    "\n",
    "    async def ask_llm(self, query):\n",
    "        # 5. Final processing step with an LLM (e.g., OpenAI via OpenRouter)\n",
    "\n",
    "        # init retrievers\n",
    "        status=self.semanticRetriever()\n",
    "        if \"Error\" in status:\n",
    "            return status\n",
    "        \n",
    "        self.sparseRetriever()\n",
    "        self.ensembleRetriever()\n",
    "\n",
    "        # retrieve relevant docs\n",
    "        results = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        print(f\"Nb of retrieved docs: {len(results)}\")\n",
    "\n",
    "        # rerank\n",
    "        scored_results=await self.reranker(results, query)\n",
    "        \n",
    "        # Concatenate retrieved documents for context\n",
    "        context = \"\\n\".join([f\"Fragment: \\n{doc['content']}\\n\" for doc in scored_results])\n",
    "\n",
    "        print(f\"Context lenght: {len(context.split(' '))} words\")\n",
    "        llm_prompt = f\"\"\"\n",
    "            Answer the question based **only** on the provided context.  \n",
    "\n",
    "            - If the context contains enough information to provide a complete or partial answer, use it to formulate a detailed and factual response.  \n",
    "            - If the context lacks relevant information, respond with: \"I don't know.\"  \n",
    "\n",
    "            ### **Context:**  \n",
    "            {context}  \n",
    "\n",
    "            ### **Question:**  \n",
    "            {query}  \n",
    "\n",
    "            ### **Answer:**  \n",
    "            Provide a clear, factual, and well-structured response based on the available context. Avoid speculation or adding external knowledge.  \n",
    "        \"\"\"\n",
    "\n",
    "        llm_completion = await self.llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in document Q/A and document synthesis\"},\n",
    "                {\"role\": \"user\", \"content\": llm_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        final_answer = \"\"\n",
    "        print(\"Réponse:\\n=========\")\n",
    "        async for chunk in llm_completion:\n",
    "            if hasattr(chunk.choices[0].delta, \"content\") and chunk.choices[0].delta.content:\n",
    "                final_answer += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        \n",
    "        self.history+=[\n",
    "            {\"role\": \"user\", 'content': query},\n",
    "            {\"role\": \"assistant\", \"content\": final_answer}\n",
    "        ]\n",
    "        \n",
    "        return final_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a224490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262754/2544901256.py:32: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
      "/tmp/ipykernel_262754/2544901256.py:36: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_db = Chroma(\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "/tmp/ipykernel_262754/2544901256.py:150: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = self.ensemble_retriever.get_relevant_documents(query)\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of retrieved docs: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 score: 4\n",
      "chunk 2 score: 4\n",
      "chunk 3 score: 9\n",
      "chunk 4 score: 4\n",
      "chunk 5 score: 6\n",
      "chunk 6 score: 4\n",
      "chunk 7 score: 2\n",
      "chunk 8 score: 6\n",
      "chunk 9 score: 7\n",
      "chunk 10 score: 6\n",
      "chunk 11 score: 2\n",
      "chunk 12 score: 4\n",
      "Context lenght: 819 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse:\n",
      "=========\n",
      "### **Answer:**\n",
      "\n",
      "Les principaux thèmes abordés dans le texte sont les suivants :\n",
      "\n",
      "1. **Philosophie de l'esprit et intelligence artificielle (IA)** :\n",
      "   - Le texte explore la question de savoir si l'intelligence peut être implémentée dans des machines et si l'IA peut un jour remplacer la conscience humaine.\n",
      "   - Il interroge également comment la machine peut imiter les œuvres de l'esprit humain et si cela révèle quelque chose sur la mécanisabilité de la vie mentale.\n",
      "\n",
      "2. **Créativité et IA** :\n",
      "   - Le texte discute de la capacité de l'IA à créer et à imiter la créativité humaine.\n",
      "   - Il souligne que l'IA utilise des analogies et des optimisations mathématiques basées sur des données existantes, mais qu'elle ne possède pas une véritable créativité ou art.\n",
      "   - La créativité humaine est présentée comme quelque chose de distinct et de supérieur, apportant une touche de nouveauté et de vision métaphorique.\n",
      "\n",
      "3. **Impact de l'IA sur la créativité humaine** :\n",
      "   - Le texte aborde la crainte que l'IA puisse remplacer ou diminuer la créativité humaine.\n",
      "   - Il mentionne que l'utilisation excessive de l'IA peut entraîner une perte de pouvoir et de compréhension, notamment chez les étudiants et les ingénieurs.\n",
      "\n",
      "4. **Anxiété humaine face à la machine** :\n",
      "   - Le texte évoque une angoisse fondamentale de l'être humain face à la machine, une thématique qui remonte à des siècles et qui est liée à des blessures narcissiques, comme celles décrites par Freud.\n",
      "\n",
      "### **Questions qui peuvent être posées :**\n",
      "\n",
      "1. **Philosophie de l'esprit et IA** :\n",
      "   - L'intelligence peut-elle être implémentée dans n'importe quelle machine ?\n",
      "   - L'IA peut-elle un jour remplacer la conscience humaine ?\n",
      "   - Comment la machine peut-elle imiter les œuvres de l'esprit humain ?\n",
      "\n",
      "2. **Créativité et IA** :\n",
      "   - L'IA possède-t-elle une véritable créativité ou art ?\n",
      "   - En quoi la créativité humaine diffère-t-elle de celle de l'IA ?\n",
      "   - Comment l'IA utilise-t-elle les données pour créer des analogies et des optimisations mathématiques ?\n",
      "\n",
      "3. **Impact de l'IA sur la créativité humaine** :\n",
      "   - L'utilisation excessive de l'IA peut-elle diminuer la créativité humaine ?\n",
      "   - Quels sont les risques pour les étudiants et les ingénieurs qui utilisent trop l'IA ?\n",
      "   - Comment préserver la créativité humaine face à l'IA ?\n",
      "\n",
      "4. **Anxiété humaine face à la machine** :\n",
      "   - Pourquoi les humains ressentent-ils une angoisse face à la machine ?\n",
      "   - Comment cette angoisse se manifeste-t-elle dans le contexte de l'IA ?\n",
      "   - En quoi l'IA représente-t-elle une menace pour l'estime de soi humaine ?\n",
      "\n",
      "Ces thèmes et questions illustrent les préoccupations et les réflexions soulevées par le texte concernant l'IA et son impact sur la créativité et la philosophie de l'esprit."
     ]
    }
   ],
   "source": [
    "rag_hybrid=RAG_hybrid(model=\"mistralai/mistral-small-3.2-24b-instruct\")\n",
    "\n",
    "# 4. Ask a question\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "results = await rag_hybrid.ask_llm(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
