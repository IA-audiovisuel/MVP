{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b419c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.créer un nouvel env conda à partir du terminal\n",
    "# conda create --name asr_rag python=3.10\n",
    "\n",
    "# 2. activer cet environnement\n",
    "# conda activate asr_rag\n",
    "\n",
    "# 3. installer les dépendences\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# 4. installer ollama: https://www.ollama.com/download\n",
    "\n",
    "\n",
    "# 5. installer le modème d'OllamaEmbeddings\n",
    "# ollama pull embeddinggemma\n",
    "\n",
    "# 6. créer une clé api sur openrouter pour utiliser des llm gratuitement\n",
    "\n",
    "# 7. mettre votre clé dans le fichier .env (racine du repertoire courant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0299e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chougar/miniconda3/envs/pathrag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from pathrag_retriever import create_graphdb, load_existing_graphdb, load_knowledgeGraph_vis\n",
    "from PathRAG import QueryParam\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "OPENROUTER_API_KEY=os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "tokens_counter = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbb31f",
   "metadata": {},
   "source": [
    "#### Création ou chargement d'un graphe existant:\n",
    "> Lorsque vous executer la cellule ci dessous, vous avez 2 options proposées:\n",
    "> 1. Créer un nouveau graphe\n",
    "> 2. Charger un graphe existant\n",
    "> \n",
    "> Un prompt de sélection s'affichera en haut du notebook<br>\n",
    "> Saisir l'action désirée dans le , suivre instructions\n",
    "\n",
    "> Si vous voulez charger un graphe déjà crée et que vous ne savez plus son nom, retrouver le dans le fichier `graphrag_hashes.json` (dans la racine du dossier), attribut `Nom du doc`\n",
    "\n",
    "> Si vous voulez modifier le LLM utilisé pour la création du graphe ou sa lecture, allez dans `pathrag_retriever.py`, sous la ligne 36( Choix du LLM OpenRouter), et prenez un modèle valide sur openrouter (attention à prendre un modèle qui supporte les `structured_outputs`, sur le site openrouter, à filtrer sur le paneau à gauche dans la liste des `Supported parameters`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29614848",
   "metadata": {},
   "source": [
    "#### 1. Créer un graphe, ou charger en un déjà crée\n",
    "Ci dessous un print des noms de docs disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d78ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IA conscience\n",
      "L-IA-notre-deuxieme-conscience\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "graph_names=pd.read_json(\"graphrag_hashes.json\")\n",
    "\n",
    "graph_names=graph_names.loc[\"Nom du doc\", :].values.tolist()\n",
    "for n in graph_names:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63b4a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PathRAG:Logger initialized for working directory: /home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5\n",
      "INFO:PathRAG:Load KV llm_response_cache with 1 data\n",
      "INFO:PathRAG:Load KV full_docs with 1 data\n",
      "INFO:PathRAG:Load KV text_chunks with 11 data\n",
      "INFO:PathRAG:Loaded graph from /home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/graph_chunk_entity_relation.graphml with 136 nodes, 120 edges\n",
      "INFO:nano-vectordb:Load (132, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/vdb_entities.json'} 132 data\n",
      "INFO:nano-vectordb:Load (120, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/vdb_relationships.json'} 120 data\n",
      "INFO:nano-vectordb:Load (11, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/vdb_chunks.json'} 11 data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nom de votre graphe est L-IA-notre-deuxieme-conscience\n",
      "\n",
      "        ----------------\n",
      "        #### Graph RAG retriever\n",
      "        Chargement de la base Graph RAG\n",
      "    \n",
      "**✅ Graph RAG chargé**\n",
      "Confirmation LLM read: deepseek/deepseek-chat-v3.1\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#==== params de base====\n",
    "\n",
    "# Obligatoire: nom des sources\n",
    "filename=\"audio-text.txt\"\n",
    "doc_name_graph=\"L-IA-notre-deuxieme-conscience\"\n",
    "\n",
    "# Optionnels:\n",
    "# 1. modèle pour création du graph\n",
    "# param 'OPENROUTER_MODEL_graph_creation', par défaut \"deepseek/deepseek-chat-v3-0324\"\n",
    "# passer dans la fonction 'create_graphdb' un autre modèle si vous le souhaitez\n",
    "\n",
    "# 2. modèle pour lecture du graph (questions/réponses)\n",
    "# param 'OPENROUTER_MODEL_graph_read', par défaut \"deepseek/deepseek-chat-v3-0324\"\n",
    "# passer dans la fonction 'load_existing_graphdb' un autre modèle si vous le souhaitez\n",
    "\n",
    "#=======================\n",
    "\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_docs = loader.load()\n",
    "text=\"\"\n",
    "for doc in txt_docs:\n",
    "    text+=doc.page_content\n",
    "\n",
    "\n",
    "graphrag_action=input(\"Saisir 'C' pour créer un nouveau graphe, 'L' pour charger un graphe existant\")\n",
    "\n",
    "# créer un nouveau graphe\n",
    "messages=None\n",
    "\n",
    "if graphrag_action=='C':\n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "    messages= create_graphdb(\n",
    "        text=text, \n",
    "        doc_name=doc_name_graph, # il faut donner un nom unique permettant d'identifier et charger le graph les prochaines fois\n",
    "    )\n",
    "# charger un graphe existant\n",
    "elif graphrag_action=='L':    \n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "\n",
    "    messages=load_existing_graphdb(\n",
    "        doc_name_graph, \n",
    "        OPENROUTER_MODEL_graph_read=\"deepseek/deepseek-chat-v3.1\"\n",
    "    )\n",
    "else:\n",
    "    print('Option invalide')\n",
    "\n",
    "\n",
    "\n",
    "if messages:\n",
    "    pipeline_args={}\n",
    "    for feedback in messages:\n",
    "        if isinstance(feedback, str):\n",
    "            print(feedback)\n",
    "        elif isinstance(feedback, dict):\n",
    "            pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"]=feedback[\"pipeline_args\"]\n",
    "            \n",
    "print(\"Confirmation LLM read:\", pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"llm_graph_QA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1543579",
   "metadata": {},
   "source": [
    "#### 2. Poser vos questions au graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6de14e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:PathRAG:kw_prompt result:\n",
      "INFO:PathRAG:```json\n",
      "{\n",
      "  \"high_level_keywords\": [\"Thèmes principaux\", \"Analyse de texte\", \"Contenu\"],\n",
      "  \"low_level_keywords\": [\"Mots-clés\", \"Idées centrales\", \"Sujets abordés\"]\n",
      "}\n",
      "```\n",
      "INFO:PathRAG:Local query uses 40 entites, 15 relations, 3 text units\n",
      "INFO:PathRAG:Global query uses 50 entites, 40 relations, 4 text units\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response all ready\n",
      "D'après les comptes-rendus de la discussion, plusieurs thèmes principaux structurent le débat sur l'intelligence artificielle.\n",
      "\n",
      "### La nature et les limites de l'IA\n",
      "Un thème central est l'évaluation des capacités réelles de l'intelligence artificielle. Les participants distinguent clairement l'optimisation algorithmique, domaine où l'IA excelle, de l'intelligence humaine authentique. L'IA est présentée comme une machine très performante pour traiter d'immenses quantités de données et produire des résultats dans des cadres contraints (comme un « exercice de style » littéraire), mais elle est fondamentalement dépourvue de conscience, d'émotions, de compréhension réelle et de motivation créatrice. Cette illusion de capacité, entretenue par un « storytelling » marketing, est un point crucial du débat.\n",
      "\n",
      "### La créativité : Humaine vs. Artificielle\n",
      "La question de savoir si l'IA peut être vraiment créative est abordée de manière approfondie. Les intervenants, comme Daniel Andler et Laurence Devillers, soutiennent que la créativité humaine est irréductible à un processus algorithmique. Ils soulignent que la création artistique ou littéraire humaine est motivée par le désir de s'adresser à un public et est nourrie par l'inconscient, les émotions et la capacité à forger des métaphores – autant d'aspects que l'IA, qui fonctionne par analogie et réutilisation de données existantes, ne peut pas reproduire. La performance de ChatGPT dans un concours littéraire est analysée comme une optimisation impressionnante, mais pas comme une preuve de créativité authentique.\n",
      "\n",
      "### L'impact psychosocial et philosophique\n",
      "Le débat place la révolution de l'IA dans une longue histoire des techniques (écriture, imprimerie) et interroge son impact sur la psyché humaine. Valentin Husson utilise des concepts psychanalytiques (Freud, Lacan) pour parler d'un « inconscient numérique » et évoque les « blessures narcissiques » que les avancées technologiques infligent à l'ego humain. La discussion porte sur la manière dont l'IA pourrait transformer notre rapport à nous-mêmes, notre langage (la dimension poétique défendue par Bergson) et même modifier nos comportements, avec des risques de manipulation.\n",
      "\n",
      "### Les enjeux sociétaux et économiques\n",
      "Les conséquences pratiques de l'IA sur le monde du travail et la société sont un sujet de désaccord. Alors que Daniel Andler minimise son impact actuel en dehors de tâches spécifiques, Laurence Devillers insiste sur son omniprésence quotidienne et les risques de standardisation et de perte de compétences. Des événements comme la grève des scénaristes d'Hollywood sont cités pour illustrer les inquiétudes des professions créatives. L'urgence d'une éducation critique pour comprendre les véritables enjeux et ne pas se laisser abuser par les discours marketing est également soulignée.\n",
      "\n",
      "En résumé, cette discussion dépasse la simple évaluation technique pour aborder l'IA comme un phénomène civilisationnel, interrogé sous les angles de la philosophie de l'esprit, de la créativité, de la psychanalyse et de ses impacts sociaux concrets."
     ]
    }
   ],
   "source": [
    "\n",
    "import asyncio\n",
    "\n",
    "def stream_pathRAG_response(stream_resp):\n",
    "    async def stream_response():        \n",
    "        # Process the async generator\n",
    "        async for chunk in stream_resp:\n",
    "            print(chunk or \"\", end=\"\")\n",
    "\n",
    "\n",
    "    # Run in Streamlit's existing event loop\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(stream_response())\n",
    "\n",
    "# question=\"résume ce texte dans sa langue source\"\n",
    "question = \"Quels sont les principaux thèmes de ce texte ?\"\n",
    "# question=\"Quelles sont les questions qui peuvent être posées sur ce texte ?\"\n",
    "# question=\"Qui sont les intervenants dans ce texte ? \"\n",
    "\n",
    "resp=pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"rag\"].query(\n",
    "    query= question, \n",
    "    param=QueryParam(mode=\"hybrid\", stream=True,)\n",
    ")\n",
    "\n",
    "stream_pathRAG_response(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f595f8",
   "metadata": {},
   "source": [
    "============================\n",
    "### RAG vectoriel\n",
    "1. Embedding du document -> renseigner le nom de votre fichier dans `filename` et le nom de votre DB dans `doc_name_hybrid`\n",
    "2. Setup du retriever / reranker / llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76565f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 documents from audio-text.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de chuncks: 1082\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "# chargement et fragmentation du doc\n",
    "## Nom du doc à traiter\n",
    "filename=\"audio-text.txt\"\n",
    "\n",
    "## Nom pour la base vectorielle\n",
    "doc_name_hybrid=\"L-IA-notre-deuxieme-conscience_sample\" # nom de doc significatif\n",
    "\n",
    "\n",
    "\n",
    "#========= choix du modèle d'embedding\n",
    "\"\"\"\n",
    "    Le modèle choisi impacte la qualité du retriever, mais aussi le temps de traitement\n",
    "    Si le déploiement est prévu sur une VM limitée, un modèle plus petit est nécessaire\n",
    "    Explorer les comparatifs: https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "\"\"\"\n",
    "# Utiliser OllamaEmbeddings avec le modèle local \"embeddinggemma\"\n",
    "embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "\n",
    "\n",
    "# loader = UnstructuredFileLoader(filename)\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_doc = loader.load()\n",
    "print(f\"Loaded {len(txt_doc)} documents from {filename}\")\n",
    "\n",
    "\n",
    "#======== choix des paramètres de fragmentation\n",
    "\"\"\"\n",
    "    la taille du chunck_size est très important dans l'accès à une info précise\n",
    "    une plus petite taille permet de cibler de courts passages contenant l'info nécessaire à des réponses précises:\n",
    "        * lieu du projet\n",
    "        * dates du projet\n",
    "        * budget ...    \n",
    "    l'envoi de passages plus courts au llm évite une dispertion de son attention\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(txt_doc)\n",
    "\n",
    "# Filter out complex metadata (e.g., lists, dicts)\n",
    "docs = [Document(doc.page_content) for doc in docs]\n",
    "\n",
    "\n",
    "# Conversion des docs en embeddings \n",
    "chroma_db = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=f'./storage/vector_scores/{doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "    collection_name=doc_name_hybrid.replace(\" \",\"_\")\n",
    ")\n",
    "\n",
    "retriever=chroma_db.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ...existing code...\n",
    "all_docs = chroma_db.get()\n",
    "print(\"Nb de chuncks:\", len(all_docs['documents']))  # This will print the total number of docs stored\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f9772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nathan Devert]: France Culture. [Nathan Devert]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Comment expliquer les progrès phénoménaux que semble avoir accompli l'intelligence artificielle au cours de ces dernières années ? Depuis l'apparition de ChatGPT en novembre 2022, rapidement suivi par d'autres agents conversationnels, cette révolution technologique aux multiples aspects, paraît désormais capable d'exécuter de nombreuses tâches intellectuelles sur lesquelles l'esprit humain pensait jusqu'alors exercer un monopole. Écrire des articles, synthétiser des documents, traiter des données dans n'importe quel domaine, diagnostiquer une maladie, rédiger une dissertation, ou pourquoi pas, un scénario de film. Non contente de révolutionner le monde du travail, ses prouesses stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et \n",
      "taille en car: 999 \n",
      "taille en tokens: 210 \n",
      "=============\n",
      "stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et encore moins d'une âme, peut s'implémenter dans n'importe quelle machine, voir, comme s'inquiètent ou exultent certains, que l'IA sera un jour en mesure de remplacer notre conscience. À moins qu'il ne faille poser cette question à l'envers. Comment se fait-il que la machine puisse imiter les œuvres de de notre esprit ? Comment est-il possible en d'autres termes que notre vie mentale se révèle comme mécanisable, au même titre que nos facultés manuelles ? L'intelligence artificielle ne doit-elle pas sa capacité d'imitation au fait qu'elle a été théorisée, inventée, développée d'après une certaine représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre \n",
      "taille en car: 998 \n",
      "taille en tokens: 231 \n",
      "=============\n",
      "représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre propre pensée. L'intelligence artificielle, notre deuxième conscience. À l'occasion de la fête de la Science, j'aurai le plaisir d'en débattre sans préjuger aux côtés du mathématicien et philosophe Daniel Andler, de la professeur en intelligence artificielle et chercheuse Laurence Devillers et du philosophe Valentin Husson. [Narrateur radio]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Bonjour Laurence Devillers. [Laurence Devillers]: Bonjour. [Nathan Devert]: Vous êtes professeur en intelligence artificielle à Sorbonne Université, chercheuse au CNRS, auteur de \"L'IA, ange ou démon\" paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur \n",
      "taille en car: 999 \n",
      "taille en tokens: 237 \n",
      "=============\n",
      "paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur émérite à Sorbonne Université, membre de l'Académie des Sciences morales et politiques et auteur de \"Intelligence artificielle, intelligence humaine, la double énigme\" aux éditions Gallimard et bonjour Valentin Husson. [Valentin Husson]: Vous êtes philosophe et auteur de \"Folle résentimentale, petite philosophie des trolls\" chez Philosophie Magazine éditeur qui vient de paraître. Alors pour commencer cette cette discussion sur l'intelligence artificielle, je voulais vous soumettre une histoire qui a eu lieu récemment en mars 2025. Le Nouvel Obs a décidé d'organiser un concours littéraire entre deux auteurs, entre guillemets. Alors le premier était Hervé Le Tellier, pris Goncourt 2020, auteur de du célèbre roman l'anomalie et le deuxième était l'intelligence artificielle et plus précisément \n",
      "taille en car: 994 \n",
      "taille en tokens: 228 \n",
      "=============\n",
      "deux auteurs, entre guillemets. Alors le premier était Hervé Le Tellier, pris Goncourt 2020, auteur de du célèbre roman l'anomalie et le deuxième était l'intelligence artificielle et plus précisément ChatGPT. Le l'objet de ce concours était de demander à l'un comme à l'autre entre guillemets, de rédiger une nouvelle et puis avec la première phrase qui était indiquée c'était quelqu'un qui découvrait un cadavre et la dernière phrase de la nouvelle aussi. Hervé Le Tellier a rendu sa copie. ChatGPT aussi et de l'aveu même de Hervé Le Tellier, il a dit franchement, je suis absolument bluffé par la créativité littéraire entre guillemets de ChatGPT. Peut-être même que ChatGPT m'a battu, que la nouvelle qui a été rendue par l'intelligence artificielle était plus drôle, plus originale, plus vive que celle que j'ai écrite et peut-être donc que ça inaugure un monde où les écrivains seront moins moins présents que l'intelligence artificielle. Daniel Andler, une telle histoire qu'est-ce qu'elle \n",
      "taille en car: 996 \n",
      "taille en tokens: 236 \n",
      "=============\n",
      "que celle que j'ai écrite et peut-être donc que ça inaugure un monde où les écrivains seront moins moins présents que l'intelligence artificielle. Daniel Andler, une telle histoire qu'est-ce qu'elle révèle ? Est-ce qu'elle signifie que l'IA peut battre l'esprit humain dans ce qu'il a de plus propre, la créativité. [Daniel Andler]: Ah, je ne le pense pas. Euh mais je pense que c'est pas original de pas le penser. On on est généralement réticent à reconnaître que la créativité s'échapperait d'une certaine façon à l'être humain. Il est très frappant que Hervé Le Tellier avec qui j'ai d'ailleurs débattu tout à fait récemment dans un autre lieu ce ce soit en quelque sorte presque d'avance déclaré battu. Mais c'est qu'il est persuadé Hervé Le Tellier qu'effectivement l'intelligence humaine est en déroute en quelque sorte. Ce qui est assez intéressant parce que voilà un écrivain, un homme dont c'est le métier en quelque sorte qui est déjà en quelque sorte acquis à l'idée que l'intelligence \n",
      "taille en car: 997 \n",
      "taille en tokens: 230 \n",
      "=============\n",
      "en déroute en quelque sorte. Ce qui est assez intéressant parce que voilà un écrivain, un homme dont c'est le métier en quelque sorte qui est déjà en quelque sorte acquis à l'idée que l'intelligence artificielle est capable de battre l'homme sur le plan de la créativité. Il faut reconnaître alors il me semble que très rapidement et je veux pas du tout être trop long, mais très rapidement, il s'agit d'un de ce qu'on appelle un exercice de style. C'est-à-dire une une une épreuve assez semblable au fond à une dissertation si vous voulez avec des contraintes et faire la meilleure dissertation possible dans un temps limité et qui euh qui obéissent à ses contraintes, c'est au fond résoudre un problème de créativité. C'est plutôt de l'ordre de l'exercice. Ça si vous voulez, c'est un peu comme résoudre un exercice de mathématiques un petit peu difficile, mais c'est pas résoudre un exercice de mathématiques, c'est pas faire de la recherche en maths, c'est pas faire quelque chose de vraiment \n",
      "taille en car: 996 \n",
      "taille en tokens: 213 \n",
      "=============\n",
      "résoudre un exercice de mathématiques un petit peu difficile, mais c'est pas résoudre un exercice de mathématiques, c'est pas faire de la recherche en maths, c'est pas faire quelque chose de vraiment nouveau. Donc, il me semble que Hervé Le Tellier et d'autres sont trop impressionnés par le résultat et euh ne pensent pas suffisamment au processus lui-même et le processus de la création, me semble-t-il et euh une un un une motivation initiale d'un écrivain ou d'un artiste ou d'un compositeur de faire du mieux qu'il peut pour s'adresser à un public. Donc il y a au départ le créateur, à l'arrivée le public, alors que il y a rien de tout ça pour l'intelligence artificielle. L'intelligence artificielle n'est pas une personne qui cherche à faire quelque chose. C'est une machine extraordinairement bluffante, ça je vous l'avoue, une machine qui répond à une demande, à savoir écrire une nouvelle policière avec cette ce ce début et cette fin. C'est tout à fait autre chose. [Nathan Devert]: \n",
      "taille en car: 994 \n",
      "taille en tokens: 227 \n",
      "=============\n",
      "bluffante, ça je vous l'avoue, une machine qui répond à une demande, à savoir écrire une nouvelle policière avec cette ce ce début et cette fin. C'est tout à fait autre chose. [Nathan Devert]: Est-ce que vous pensez aussi Laurence Devillers que Hervé Le Tellier et ceux qui ont été impressionnés par ce par ce concours, ont cédé en un sens à ce mirage ou à cette illusion du du résultat ? [Laurence Devillers]: Oui, je pense qu'ils ont été usurpés par cette machine qui a vu d'ailleurs toutes leurs textes, qui connaît toutes leurs données, tous les livres préalables et les livres peut-être du monde entier pour créer cette petite nouvelle qui est effectivement un exercice de style qui est moins facile pour un humain puisqu'on donne la première phrase, on donne la dernière et il y a un nombre de caractères. Donc si vous voulez, c'est une optimisation dans un un océan de documents et ça la machine sait très bien faire. Donc c'est pas du tout étonnant qu'elle est qu'elle soit meilleure. Moi, \n",
      "taille en car: 997 \n",
      "taille en tokens: 225 \n",
      "=============\n",
      "caractères. Donc si vous voulez, c'est une optimisation dans un un océan de documents et ça la machine sait très bien faire. Donc c'est pas du tout étonnant qu'elle est qu'elle soit meilleure. Moi, j'aurais dit à l'avance évidemment qu'elles étaient meilleures. C'est la même chose qui voulait que pour les trouver dans les radios des cancers précoces, la machine fait autre chose que nous et elle peut nous surprendre et apporter de l'information et même dans et c'est pas de la créativité là, c'est vraiment de la réutilisation de texte. Et il faut bien comprendre cela, parce que ces machines qui nous bluffent en ce moment, elles sont nourries de toutes nos données. Or actuellement, elles en produisent aussi les données, approximatives, qui demain seront celles qui vont nourrir les prochaines machines. Et là, on va voir baisser gravement la créativité de ces machines. Ça, c'est l'effet qu'on ne voit pas encore aujourd'hui. On est bluffé par le premier step, la première. Parce que c'est \n",
      "taille en car: 996 \n",
      "taille en tokens: 220 \n",
      "=============\n",
      "machines. Et là, on va voir baisser gravement la créativité de ces machines. Ça, c'est l'effet qu'on ne voit pas encore aujourd'hui. On est bluffé par le premier step, la première. Parce que c'est nos données à l'intérieur. Et j'ai et et monsieur Le Tellier comme d'autres, il a été récompensé par un prix, sont des gens brillants qui ont créé des choses assez extraordinaires. Et et c'est ces données-là qui sont à l'intérieur. Et cette machine qui crée, cette IA qui crée les données en ce moment, elle se nourrit de de milliers d'années, enfin de depuis qu'on a des traces en fait, depuis qu'on on sauvegarde des traces et que ensuite on les a numérisées, elle elle patauge dans tout ça. Et là il y a des lumières, il y a des choses intéressantes, il y a des rapprochements. Alors ce qu'elle c'est pas faire, c'est faire des métaphores. Elle fait des analogies, cette machine. Elle a tout ce jeu de données, elle a des contraintes et elle va jouer par analogie, elle va tricoter comme ça. Voilà, \n",
      "taille en car: 998 \n",
      "taille en tokens: 247 \n",
      "=============\n",
      "pas faire, c'est faire des métaphores. Elle fait des analogies, cette machine. Elle a tout ce jeu de données, elle a des contraintes et elle va jouer par analogie, elle va tricoter comme ça. Voilà, bon, c'est pas de l'art, c'est pas de la créativité, c'est de l'optimisation mathématique dans un énorme possible. Mais cet énorme possible, il joue pour beaucoup sur ce qu'on obtient à la fin. Puisque sinon, c'est juste une moulinette algorithmique. Et le jour où les données seront de moins bonne qualité, parce que les gens voudront plus créer ou ce sera les machines qui créeront. Et donc elles vont boucler au bout d'un moment et sans doute descendre en créativité. C'est nous qui apportons cette touche de nouveau, de vision métaphorique, d'inconscient, on en parlera après, qui va venir troubler tout ça, c'est l'humain. [Nathan Devert]: Est-ce qu'on peut pas dire quand même en en un sens que euh dans quand on écrit un un texte et qu'on est un humain euh euh euh la créativité qui est la \n",
      "taille en car: 994 \n",
      "taille en tokens: 246 \n",
      "=============\n",
      "tout ça, c'est l'humain. [Nathan Devert]: Est-ce qu'on peut pas dire quand même en en un sens que euh dans quand on écrit un un texte et qu'on est un humain euh euh euh la créativité qui est la nôtre euh est en un sens une addition de trucs, de méthode, d'habitude, de de de réflexes et que à cet égard, ça révèle aussi ce qu'il y avait peut-être Valentin Husson, de de mécanique dans l'écriture d'un Hervé Le Tellier ou de ou de n'importe qui parmi nous. [Valentin Husson]: Oui, il y a peut-être quelque chose de de mécanique, mais je voudrais revenir sur cette sur cette chose-là. Si Hervé Le Tellier, si l'humain en règle générale se croit d'emblée vaincu et par avance vaincu par l'intelligence artificielle, il y a un billet psychologique. C'est qu'il y a une angoisse fondamentale de l'être humain à l'égard de la machine et cela depuis des siècles. Ça Freud disait, il y a trois blessures narcissiques de l'homme. La première, c'est Copernic, nous ne sommes plus au centre de l'univers, la \n",
      "taille en car: 996 \n",
      "taille en tokens: 260 \n",
      "=============\n",
      "humain à l'égard de la machine et cela depuis des siècles. Ça Freud disait, il y a trois blessures narcissiques de l'homme. La première, c'est Copernic, nous ne sommes plus au centre de l'univers, la seconde c'est Darwin, nous ne sommes plus premiers dans l'ordre de la création, la troisième c'est la psychanalyse, l'homme n'est plus maître dans sa propre maison. J'en ajouterai une quatrième, c'est-à-dire que avec l'intelligence artificielle, nous avons l'impression que notre conscience n'est plus maîtresse d'elle-même. Nous ne sommes plus les les les les maîtres de notre intelligence. Et je crois que c'est une angoisse c'est une angoisse fondamentale de perte de de perte de maîtrise. Alors vous disiez tout à l'heure que en effet quand on est par exemple étudiant, on apprend des mécanismes. mais justement ce qui fait l'humain et je dirais le l'honneur et la beauté de l'humanité, c'est que justement nous arrivons à nous désautomatiser désautomatiser, pardon, comme disait Bernard \n",
      "taille en car: 991 \n",
      "taille en tokens: 233 \n",
      "=============\n",
      "mais justement ce qui fait l'humain et je dirais le l'honneur et la beauté de l'humanité, c'est que justement nous arrivons à nous désautomatiser désautomatiser, pardon, comme disait Bernard Stigler. Nous avons des automatismes, mais il y a quelque chose de formidable chez l'homme, c'est que à un moment donné, il peut faire rupture, il peut bifurquer, ça s'appelle la créativité, ça s'appelle l'inventivité. Quand appeler ça le génie ce qui est inimitable. Et en en ce sens-là, je ne désespère pas de l'être humain quant à la pensée, quant à l'humour, peut-être qu'on en parlera, mais il y a des choses proprement humaines que l'intelligence artificielle ne pourra jamais reproduire. [Nathan Devert]: Laurence Devillers, juste pour pour euh euh continuer cette discussion mais en en en en faisant aussi référence à une autre forme d'inquiétude qui a pu être rapportée par par l'intelligence artificielle. On a assisté en 2023 pendant 6 mois, enfin entre mai et septembre, euh à un mouvement \n",
      "taille en car: 992 \n",
      "taille en tokens: 246 \n",
      "=============\n",
      "aussi référence à une autre forme d'inquiétude qui a pu être rapportée par par l'intelligence artificielle. On a assisté en 2023 pendant 6 mois, enfin entre mai et septembre, euh à un mouvement vraiment inédit euh dans l'histoire sociale américaine, une grève euh de scénaristes euh à Hollywood, euh qui ne s'était pas produite dans de cette ampleur, avec cette ampleur depuis, je crois, 40 ans, euh dont la revendication première était leur inquiétude d'être remplacé euh par l'intelligence artificielle, par les producteurs dans l'écriture de de de de scénarios. Qu'est-ce que ça révèle aussi des inquiétudes professionnelles très concrètes euh que cette révolution technologique peut susciter ? [Laurence Devillers]: Alors, elle suicide effectivement toutes ces inquiétudes avec sûrement des métiers qui vont disparaître. On a tort de penser que on est super intelligent en tout et qu'on va rester là. Dès lors que on fait des copier-coller de ce que font ces machines dans beaucoup de travaux, \n",
      "taille en car: 997 \n",
      "taille en tokens: 228 \n",
      "=============\n",
      "qui vont disparaître. On a tort de penser que on est super intelligent en tout et qu'on va rester là. Dès lors que on fait des copier-coller de ce que font ces machines dans beaucoup de travaux, mes étudiants font ça dans leurs dissertation, et ben c'est une perte de pouvoir. Les ingénieurs qui codes avec des codes faits par des machines, c'est une perte de pouvoir. D'abord parce qu'ils ne comprennent pas tout ce qu'ils font, et puis que ils sont désapprennent. Donc, attention à ce côté-là, là, on pourrait être remplacé. Dès lors que vous utilisez trop ces machines et que vous formatez sur cette information, vous risquez euh d'être remplacé. Alors qu'est-ce que dans cette machine fait que il y a une sorte de créativité artificielle. C'est qu'en fait, on a volontairement. \n",
      "taille en car: 781 \n",
      "taille en tokens: 180 \n",
      "=============\n",
      "00:00:00 ---> 00:00:12 Annonce: France Culture France Culture sans préjugés, Nathan Devert. \n",
      "taille en car: 91 \n",
      "taille en tokens: 28 \n",
      "=============\n",
      "00:00:20 ---> 00:02:03 Nathan Devers: Comment expliquer les progrès phénoménaux que semble avoir accomplis l'intelligence artificielle au cours de ces dernières années. Depuis l'apparition de chat GPT en novembre 2022, rapidement suivi par d'autres agents conversationnels. Cette révolution technologique aux multiples aspects paraît désormais capable d'exécuter de nombreuses tâches intellectuelles sur lesquelles l'esprit humain pensait jusqu'à alors exercer un monopole. Écrire des articles, synthétiser des documents, traiter des données dans n'importe quel domaine, diagnostiquer une maladie, rédiger une dissertation, ou pourquoi pas, un scénario de film. Non contente de révolutionner le monde du travail, ses prouesses stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et encore moins d'une âme, peut s'implémenter dans n'importe quelle machine, \n",
      "taille en car: 1000 \n",
      "taille en tokens: 213 \n",
      "=============\n",
      "de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et encore moins d'une âme, peut s'implémenter dans n'importe quelle machine, voir, comme s'inquiètent ou exultent certains, que l'IA sera un jour en mesure de remplacer notre conscience. À moins qu'il ne faille poser cette question à l'envers. Comment se fait-il que la machine puisse imiter les œuvres de de notre esprit. Comment est-il possible en d'autres termes que notre vie mentale se révèle comme mécanisble au même titre que nos facultés manuelles. L'intelligence artificielle ne doit-elle pas sa capacité d'imitation au fait qu'elle a été théorisée, inventée, développée d'après une certaine représentation, peut-être incomplète ou biaisée des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre propre pensée. L'intelligence artificielle, notre deuxième conscience, à l'occasion de \n",
      "taille en car: 999 \n",
      "taille en tokens: 230 \n",
      "=============\n",
      "bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre propre pensée. L'intelligence artificielle, notre deuxième conscience, à l'occasion de la fête de la science, j'aurai le plaisir d'en débattre sans préjugés au côté du mathématicien et philosophe Daniel Angler, de la professeur en intelligence artificielle et chercheuse Laurence de Ville et du philosophe Valentin Husson. \n",
      "taille en car: 433 \n",
      "taille en tokens: 99 \n",
      "=============\n",
      "00:02:04 ---> 00:02:07 Annonce: France Culture, sans préjuger. Nathan Devers. \n",
      "taille en car: 77 \n",
      "taille en tokens: 27 \n",
      "=============\n",
      "00:02:07 ---> 00:04:08 Nathan Devers: Bonjour Laurence de Villers. Bonjour. Vous êtes professeur en intelligence artificielle à Sorbonne Université, chercheuse au CNRS, auteur de Lia Ange ou démon paru en 2025 aux éditions du CER et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. Bonjour. Vous êtes mathématicien, philosophe, professeur émérite à Sorbonne Université, membre de l'Académie des sciences morales et politiques et auteur de intelligence artificielle, intelligence humaine, la double énigme aux éditions Gallimard et bonjour Valentin Husson. Vous êtes philosophe et auteur de fou le ressentimentale petite philosophie des Trolls chez philosophie Magazine éditeur qui vient de paraître. Alors pour commencer cette cette discussion sur l'intelligence artificielle, je voulais vous vous soumettre une histoire qui a eu lieu récemment en mars 2025. Le Nouvel Obs a décidé d'organiser un concours littéraire entre deux auteurs entre guillemets. Alors le premier était Hervé \n",
      "taille en car: 998 \n",
      "taille en tokens: 221 \n",
      "=============\n",
      "vous vous soumettre une histoire qui a eu lieu récemment en mars 2025. Le Nouvel Obs a décidé d'organiser un concours littéraire entre deux auteurs entre guillemets. Alors le premier était Hervé Letellier pris en cours 2020 auteur de du du célèbre roman l'anomalie et le deuxième était l'intelligence artificielle et plus précisément de chat GPT. Le l'objet de ce concours était de demander à l'un comme à l'autre entre guillemets de rédiger une nouvelle et puis avec la première phrase qui était indiquée c'était quelqu'un qui découvrait un cadavre et la dernière phrase de la nouvelle aussi. Hervé Letellier a rendu sa copie. Chat GPT aussi et de la ve même de Hervé Letellier il a dit franchement je suis absolument bluffé par la créativité littéraire entre guillemets de chat GPT peut-être même que Chat GPT m'a battu, que la nouvelle qui a été rendue par l'intelligence artificielle était plus drôle, plus originale, plus vive que celle que j'ai écrite et peut-être donc que ça inaugure un \n",
      "taille en car: 994 \n",
      "taille en tokens: 230 \n",
      "=============\n",
      "que Chat GPT m'a battu, que la nouvelle qui a été rendue par l'intelligence artificielle était plus drôle, plus originale, plus vive que celle que j'ai écrite et peut-être donc que ça inaugure un monde où les écrivains seront moins moins présents que l'intelligence artificielle. Daniel Andler une telle histoire qu'est-ce qu'elle révèle ? Est-ce qu'elle signifie que Lia peut battre l'esprit humain dans ce qu'il a de plus propre, la créativité. \n",
      "taille en car: 446 \n",
      "taille en tokens: 100 \n",
      "=============\n",
      "00:04:08 ---> 00:06:39 Daniel Andler: Ah, je ne le pense pas. Euh, mais je pense que c'est pas original de pas le penser. On est généralement réticent à euh reconnaître que la créativité échapperait d'une certaine façon à l'être humain. Euh, il est très frappant que Hervé Le Tellier, avec qui j'ai d'ailleurs débattu tout à fait récemment dans un autre lieu, euh, se ce soit en quelque sorte presque d'avance déclaré battu. Mais c'est qu'il est persuadé, Hervé Le Tellier, qu'effectivement, l'intelligence humaine étant déroute en quelque sorte. Euh, ce qui est assez intéressant parce que voilà un écrivain, hein, dont c'est le métier en quelque sorte, qui est déjà en quelque sorte acquis l'idée que l'intelligence artificielle euh, est capable de battre l'homme sur le plan de la créativité. Il faut reconnaître, alors, il me semble que très rapidement, je veux pas du tout être trop long, mais très rapidement, il s'agit d'un de ce qu'on appelle un exercice de style, c'est-à-dire, euh, une une \n",
      "taille en car: 999 \n",
      "taille en tokens: 252 \n",
      "=============\n",
      "reconnaître, alors, il me semble que très rapidement, je veux pas du tout être trop long, mais très rapidement, il s'agit d'un de ce qu'on appelle un exercice de style, c'est-à-dire, euh, une une une épreuve assez semblable au fond à une dissertation, si vous voulez, avec des contraintes, et faire la meilleure dissertation possible dans un temps limité et qui euh, qui obéissent à ses contraintes, c'est au fond résoudre un problème de créativité. C'est plutôt de l'ordre de l'exercice. Euh, ça si vous voulez, c'est un peu comme résoudre un exercice de mathématiques un petit peu difficile, mais c'est résoudre un exercice de mathématiques, c'est pas faire de la recherche en maths, c'est pas faire quelque chose de vraiment nouveau. Donc, il me semble que Hervé Le Tellier et d'autres sont trop impressionnés par le résultat et euh, ne pensent pas suffisamment au processus lui-même. Et le processus de la création, me semble-t-il, est euh, une un un une motivation initiale d'un écrivain ou \n",
      "taille en car: 995 \n",
      "taille en tokens: 232 \n",
      "=============\n",
      "impressionnés par le résultat et euh, ne pensent pas suffisamment au processus lui-même. Et le processus de la création, me semble-t-il, est euh, une un un une motivation initiale d'un écrivain ou d'un artiste ou d'un compositeur, de faire du mieux qu'il peut pour s'adresser à un public. Donc, il y a au départ, le créateur, à l'arrivée, le public, alors que il y a rien de tout ça pour l'intelligence artificielle. L'intelligence artificielle n'est pas une personne qui cherche à faire quelque chose. C'est une machine extraordinairement bluffante, ça, je vous l'avoue. Une machine qui répond à une demande, à savoir, écrire un une nouvelle policière avec cette ce début et cette fin. C'est tout à fait autre chose. \n",
      "taille en car: 717 \n",
      "taille en tokens: 174 \n",
      "=============\n",
      "00:06:39 ---> 00:06:48 Nathan Devers: Est-ce que vous pensez aussi Laurence Deviller que Hervé Le Tellier et ceux qui ont été impressionnés par ce par ce concours ont cédé en un sens à ce mirage ou à cette illusion du du résultat ? \n",
      "taille en car: 231 \n",
      "taille en tokens: 61 \n",
      "=============\n",
      "00:06:49 ---> 00:09:11 Laurence Devillers: Oui, je pense qu'ils ont été euh usurpés par cette machine qui a vu d'ailleurs toutes leurs textes, qui connaît toutes leurs données, tous les livres préalables et les livres peut-être du monde entier pour créer cette petite nouvelle qui est effectivement un exercice de style, qui est moins facile pour un humain, puisqu'on donne la première phrase, on donne la dernière et il y a un nombre de caractères. Donc si vous voulez, c'est une optimisation dans un un océan de documents et ça, la machine sait très bien faire. Donc, c'est pas du tout étonnant que elle est qu'elle soit meilleure. Moi, j'aurais dit à l'avance, évidemment, qu'elles étaient meilleures. C'est la même chose, vous voulez que pour aller trouver dans des radios, des cancers précoces, la machine fait autre chose que nous et elle peut nous surprendre et apporter de l'information. Et même et c'est pas de la créativité là, c'est vraiment de la réutilisation de texte. Il faut bien \n",
      "taille en car: 995 \n",
      "taille en tokens: 226 \n",
      "=============\n",
      "la machine fait autre chose que nous et elle peut nous surprendre et apporter de l'information. Et même et c'est pas de la créativité là, c'est vraiment de la réutilisation de texte. Il faut bien comprendre cela, parce que ces machines qui nous bluffent en ce moment, elles sont nourries de toutes nos données. Or actuellement, elles en produisent aussi des données approximatives qui demain seront celles qui vont nourrir les prochaines machines. Et là, on va voir baisser gravement la créativité de ces machines. Ça, c'est l'effet qu'on ne voit pas encore aujourd'hui. On est bluffé par le premier step, la première, parce que c'est nos données à l'intérieur. Et j'ai et monsieur Letellier, comme d'autres et il a été ré compensé par un prix, sont des gens brillants qui ont créé des choses assez extraordinaires. Et et c'est ces données-là qui sont à l'intérieur. Et cette machine qui crée cette IA qui crée des données en ce moment, elle se nourrit de de milliers d'années, enfin, depuis qu'on a \n",
      "taille en car: 999 \n",
      "taille en tokens: 226 \n",
      "=============\n",
      "Et et c'est ces données-là qui sont à l'intérieur. Et cette machine qui crée cette IA qui crée des données en ce moment, elle se nourrit de de milliers d'années, enfin, depuis qu'on a des traces, en fait, depuis qu'on on sauvegarde des traces et que ensuite, on les a numérisées, elle elle patoge dans tout ça. Et là, il y a des lumières, il y a des choses intéressantes, il y a des rapprochements. Alors, ce qu'elle ne sait pas faire, c'est faire des métaphores. Elle fait des analogies, cette machine. Elle a tout ce jeu de données, elle a des contraintes et elle va jouer par analogie, elle va tricoter comme ça. Voilà, bon, c'est pas de l'art, c'est pas de la créativité, c'est de l'optimisation mathématique dans un énorme possible. Mais cet énorme possible, il joue pour beaucoup sur ce qu'on obtient à la fin. Puisque sinon, c'est juste une moulinete algorithmique. Et le jour où les données seront de moins bonne qualité parce que les gens voudront plus créer où ce sera les machines qui \n",
      "taille en car: 995 \n",
      "taille en tokens: 245 \n",
      "=============\n",
      "obtient à la fin. Puisque sinon, c'est juste une moulinete algorithmique. Et le jour où les données seront de moins bonne qualité parce que les gens voudront plus créer où ce sera les machines qui créeront et donc elles vont boucler au bout d'un moment et sans doute descendre en créativité. C'est nous qui apportons cette touche de nouveau, de vision métaphorique, d'inconscient, on en parlera après, qui va venir troubler tout ça, c'est l'humain. \n",
      "taille en car: 448 \n",
      "taille en tokens: 107 \n",
      "=============\n",
      "00:09:12 ---> 00:09:36 Nathan Devers: Est-ce qu'on peut pas dire quand même en en un sens que euh dans quand on écrit un un texte et qu'on est un humain euh euh la créativité qui est la nôtre euh est en un sens une addition de trucs, de méthode, d'habitude, de de de réflexes et que à cet égard, ça révèle aussi ce qu'il y avait peut-être Valentin Husson de de mécanique dans l'écriture d'un Hervé Letellier ou de ou de n'importe qui euh parmi nous. \n",
      "taille en car: 449 \n",
      "taille en tokens: 125 \n",
      "=============\n",
      "00:09:36 ---> 00:11:13 Valentin Husson: Oui, il y a peut-être quelque chose de de mécanique mais je voudrais revenir sur cette sur cette chose-là. Si Harvé Letellier, si l'humain en règle générale se croit d'emblée vaincu et par avance vaincu par l'intelligence artificielle, il y a un biais psychologique. C'est que il y a une angoisse fondamentale de l'être humain à l'égard de la machine et cela depuis des siècles. Ça Freud disait, il y a trois blessures narcissiques de l'homme. La première, c'est Copernic, nous ne sommes plus au centre de l'univers, la seconde c'est Darwin, nous ne sommes plus premiers dans l'ordre de la création. La troisième c'est la psychanalyse, le moindre est plus maître dans sa propre maison j'en ajouterai une quatrième, c'est-à-dire que, avec l'intelligence artificielle, nous avons l'impression que notre conscience n'est plus maîtresse d'elle-même. Nous ne sommes plus les les les les maîtres de notre intelligence et je crois que c'est une angoisse c'est une \n",
      "taille en car: 996 \n",
      "taille en tokens: 242 \n",
      "=============\n",
      "nous avons l'impression que notre conscience n'est plus maîtresse d'elle-même. Nous ne sommes plus les les les les maîtres de notre intelligence et je crois que c'est une angoisse c'est une angoisse fondamentale de perte de de perte de maîtrise. Alors vous disiez tout à l'heure que, en effet, quand on est par exemple étudiant, on apprend des mécanismes. Mais justement ce qui fait l'humain et je dirais le l'honneur et la beauté de l'humanité, c'est que justement nous arrivons à nous désautomatiser, à nous désautomatiser, pardon, comme disait Bernard Stigler. Nous avons des automatismes, mais il y a quelque chose de formidable chez l'homme, c'est que un moment donné il peut faire rupture. Il peut bifurquer. Ça s'appelle la créativité, ça s'appelle l'inventivité. Quand appeler ça le génist, qui est inimitable. Et en ce ce sens-là, je ne désespère pas de l'être humain, quand à la pensée, quand à l'humour peut-être qu'on n'en parlera, mais il y a des choses proprement humaines que \n",
      "taille en car: 990 \n",
      "taille en tokens: 243 \n",
      "=============\n",
      "génist, qui est inimitable. Et en ce ce sens-là, je ne désespère pas de l'être humain, quand à la pensée, quand à l'humour peut-être qu'on n'en parlera, mais il y a des choses proprement humaines que l'intelligence artificielle ne pourra jamais reproduire. Leur \n",
      "taille en car: 261 \n",
      "taille en tokens: 69 \n",
      "=============\n",
      "00:11:14 ---> 00:11:58 Nathan Devers: Laurence de Vilard, juste pour pour euh euh euh continuer cette discussion, mais en en en en faisant aussi référence à une autre forme d'inquiétude qui a pu être apportée par par l'intelligence artificielle. On a assisté en 2023, pendant euh 6 mois, enfin entre mai et septembre, euh à un mouvement euh vraiment inédit euh dans l'histoire sociale américaine, une grève euh de scénaristes euh à Hollywood euh qui ne s'était pas produite dans de cette ampleur, avec cette ampleur depuis, je crois, 40 ans. Euh dont la revendication première était leur inquiétude d'être remplacé euh par l'intelligence artificielle, par les producteurs dans l'écriture de de de de scénario. Qu'est-ce que ça révèle aussi des inquiétudes professionnelles très concrètes euh que cette révolution technologique peut susciter ? Alors, \n",
      "taille en car: 849 \n",
      "taille en tokens: 209 \n",
      "=============\n",
      "00:11:58 ---> 00:13:55 Laurence Devillers: Alors, elle s'essuie, effectivement, toutes ces inquiétudes avec sûrement des métiers qui vont disparaître. On a tort de penser que on est super intelligent en tout et qu'on va rester là. Dès lors que on fait des copier-coller de ce que font ces machines dans beaucoup de travaux. Regardez, nos étudiants fondent ça dans leur dissertation, et ben c'est une perte de pouvoir. Les ingénieurs qui codent avec des codes faits par des machines, c'est une perte de pouvoir. D'abord parce qu'ils comprennent pas tout ce qu'ils font et puis que ils sont désapprennent. Donc attention à ce côté-là, là on pourrait être remplacé. Dès lors que vous utilisez trop ces machines et que vous formatez sur cette standardisation, vous risquez d'être remplacé. Alors, qu'est-ce que dans cette machine fait que il y a une sorte de créativité artificielle ? C'est qu'en fait, on a volontairement rajouté du hasard. Il y a par exemple la température, lorsque vous parlez de \n",
      "taille en car: 995 \n",
      "taille en tokens: 232 \n",
      "=============\n",
      "que dans cette machine fait que il y a une sorte de créativité artificielle ? C'est qu'en fait, on a volontairement rajouté du hasard. Il y a par exemple la température, lorsque vous parlez de ChatGPT, il a un facteur qui fait qu'il peut aller chercher au hasard des choses. Il, on dit il maximise les probabilités, il va vers des statistiques, mais il a des mécanismes pour aller au-delà et ça c'est quelque chose qu'on c'est un curseur, on décide d'en mettre ou pas en mettre. Et nous, dans les mains, avec ChatGPT le premier et ses outils qui ont été utilisés pour faire cette cette cette ce test avec avec le telier, c'est ce sont des outils qui ont cette particularité d'avoir un peu d'aléatoire, qui va sortir un aléatoire proche de ce qu'on a dit et qui apporte une sorte de lumière différente un peu métaphorique par hasard, mais il faut le rappeler, ce n'est pas de l'intelligence. Ce n'est pas de la conscience, il y a aucune intention de la machine de faire quoi que ce soit, c'est piloté \n",
      "taille en car: 999 \n",
      "taille en tokens: 238 \n",
      "=============\n",
      "total_tokens: 7926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total_tokens=0\n",
    "for c in all_docs[\"documents\"][:40]:\n",
    "    num_tokens = (tokens_counter.encode(c))\n",
    "    print(c, f\"\\ntaille en car: {len(c)}\", f\"\\ntaille en tokens: {len(num_tokens)}\", \"\\n=============\")\n",
    "    total_tokens+=len(num_tokens)\n",
    "\n",
    "print(\"total_tokens:\", total_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84137027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.schema.document import Document\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "class RAG_hybrid():\n",
    "    def __init__(self, model):\n",
    "        self.model=model\n",
    "        self.retrieved_docs=[]\n",
    "        self.semantic_retriever_topK=60\n",
    "        self.sparse_retriever_topK=60\n",
    "        self.reranker_topK=30\n",
    "        self.history=[]\n",
    "        self.llm_client = AsyncOpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "        )\n",
    "        self.reranker_llm=\"mistralai/mistral-small-3.1-24b-instruct\"\n",
    "        self.doc_name_hybrid=doc_name_hybrid\n",
    "        self.reranker_score_thresh=5\n",
    "        self.reranked_doc=[]\n",
    "\n",
    "    def semanticRetriever(self):\n",
    "        # 1. Semantic Retriever (Chroma + OllamaEmbeddings)\n",
    "        embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "        if self.doc_name_hybrid == 'None':\n",
    "            return \"Error: fournir le nom du document\"\n",
    "        \n",
    "        chroma_db = Chroma(\n",
    "            persist_directory=f'./storage/vector_scores/{self.doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "            collection_name=self.doc_name_hybrid.replace(\" \",\"_\"),\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        semantic_retriever=chroma_db.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                'k': self.semantic_retriever_topK, \n",
    "                'fetch_k': 60000\n",
    "            }\n",
    "        )    \n",
    "\n",
    "\n",
    "        self.chroma_db=chroma_db\n",
    "        self.semantic_retriever=semantic_retriever\n",
    "\n",
    "        return \"Success: ChromaDB setup avec succes\"\n",
    "    \n",
    "    def sparseRetriever(self):\n",
    "        # 2. Sparse Retriever (TF-IDF)\n",
    "\n",
    "        # Récupérer TOUS les documents depuis Chroma\n",
    "        all_data = self.chroma_db.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "        # Convertir en liste de `Document` objects pour LangChain\n",
    "        docs = [\n",
    "            Document(page_content=text, metadata=meta or {})  # <-- Si meta est None, on met {}\n",
    "            for text, meta in zip(all_data[\"documents\"], all_data[\"metadatas\"])\n",
    "        ]\n",
    "\n",
    "        # Créer le retriever TF-IDF\n",
    "        sparse_retriever = TFIDFRetriever.from_documents(\n",
    "            documents=docs,\n",
    "            k=self.sparse_retriever_topK,\n",
    "            tfidf_params={\"min_df\": 1, \"ngram_range\": (1, 2)}\n",
    "        )\n",
    "\n",
    "        self.sparse_retriever= sparse_retriever\n",
    "    \n",
    "    def ensembleRetriever(self):\n",
    "        # 3. Ensemble Retriever (Semantic + Sparse)\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.semantic_retriever, self.sparse_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        self.ensemble_retriever=ensemble_retriever\n",
    "\n",
    "    async def reranker(self, results, query):\n",
    "\n",
    "\n",
    "        async def llm_eval(doc, query):\n",
    "            # system_prompt=\"\"\"\n",
    "            #     You're an expert assistant in reranking documents against a question.\n",
    "            #     Your role is to compare the question with a document and give a score from 0 to 10, where:\n",
    "            #     0=document out of context, unable to answer the question\n",
    "            #     10=highly relevant document, able to answer the question\n",
    "                                \n",
    "            #     The expected final output is the score in json format\n",
    "            #     Example:\n",
    "            #     ```json{\"score\": 5}```\n",
    "                \n",
    "            #     It is very IMPORTANT to follow these instructions:\n",
    "            #     * Reply only with the score\n",
    "            #     * Always end your answer with this json format\n",
    "            # \"\"\"\n",
    "            \n",
    "            system_prompt=\"\"\"\n",
    "                Tu es un assistant expert chargé d’évaluer la pertinence d’un fragment de document par rapport à une question.\n",
    "\n",
    "                Ton objectif est d’attribuer un score de 0 à 10 selon la capacité de ce fragment à contribuer à une réponse correcte et utile à la question, même s’il ne suffit pas à lui seul.\n",
    "\n",
    "                Barème :\n",
    "                0 = Hors sujet, aucune utilité pour répondre à la question  \n",
    "                2 = Très faible lien thématique  \n",
    "                4 = Lien partiel ou contextuel faible  \n",
    "                6 = Apporte une information utile mais incomplète  \n",
    "                8 = Très pertinent, information clé pour répondre  \n",
    "                10 = Suffisant à lui seul pour répondre correctement  \n",
    "\n",
    "                Consignes importantes :\n",
    "                - Le fragment peut être incomplet mais néanmoins très pertinent s’il apporte une information essentielle.\n",
    "                - Suppose que plusieurs fragments seront combinés pour produire la réponse finale.\n",
    "                - Ne pénalise pas un document uniquement parce qu’il ne permet pas une réponse complète isolément.\n",
    "                - Évalue la similarité sémantique réelle, pas seulement les mots-clés.\n",
    "                - Ne fais aucune supposition au-delà du contenu explicite ou clairement implicite du fragment.\n",
    "                - Ignore le style, la forme et la qualité rédactionnelle.\n",
    "\n",
    "                Format de sortie strict :\n",
    "                Réponds uniquement avec le score au format JSON suivant, sans aucun texte supplémentaire :\n",
    "\n",
    "                ```json\n",
    "                {\"score\": X}\n",
    "            \"\"\"            \n",
    "            response = await self.llm_client.chat.completions.create(\n",
    "                model=self.reranker_llm,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                        La question est: {query}\\n Le document à évaluer est le suivant\\n: {doc}\n",
    "                    \"\"\"\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0,\n",
    "                extra_headers={\n",
    "                    \"HTTP-Referer\": \"audio-hybrid-rag-reranker\",  # Optional for rankings\n",
    "                    \"X-Title\": \"audio-hybrid-rag-reranker\",  # Optional for rankings\n",
    "                },\n",
    "                extra_body={\n",
    "                    \"user\": \"audio-hybrid-rag-reranker\"\n",
    "                }                \n",
    "            )\n",
    "            # Post-process to extract only the JSON part if extra text is present\n",
    "            content = response.choices[0].message.content\n",
    "            # Try to extract the JSON block if the model adds extra text\n",
    "            match = re.search(r\"\\{.*?\\}\", content, re.DOTALL)\n",
    "            if match:\n",
    "                content = match.group(0)\n",
    "\n",
    "            # extract score\n",
    "            score=None\n",
    "            try:\n",
    "                score=content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "                \n",
    "                score= json.loads(score)\n",
    "                score=score[\"score\"]\n",
    "            except Exception as e:\n",
    "                print(e)                \n",
    "            \n",
    "            return {\"content\": doc, \"score\": score}\n",
    "\n",
    "\n",
    "        tasks=[llm_eval(doc.page_content, query) for doc in results]\n",
    "        scored_docs= await asyncio.gather(*tasks)\n",
    "        i=1\n",
    "\n",
    "        for doc in scored_docs:\n",
    "          \n",
    "            print(f'chunk {i} score: {doc[\"score\"]}')\n",
    "            i+=1\n",
    "\n",
    "        filtred_docs=[d for d in scored_docs if int(d[\"score\"])>=self.reranker_score_thresh]\n",
    "        # print(f\"scored docs; \\n{scored_docs}\")\n",
    "        self.reranked_doc=filtred_docs\n",
    "        filtred_docs=filtred_docs[:self.reranker_topK]\n",
    "        print(\n",
    "            \"================================\\n\",\n",
    "            \"Nb of docs kept after the reranking:\", \n",
    "            len(filtred_docs),\n",
    "            \"\\n==============================\"\n",
    "        )\n",
    "        return filtred_docs\n",
    "\n",
    "    async def ask_llm(self, query):\n",
    "        # 5. Final processing step with an LLM (e.g., OpenAI via OpenRouter)\n",
    "\n",
    "        # init retrievers\n",
    "        status=self.semanticRetriever()\n",
    "        if \"Error\" in status:\n",
    "            return status\n",
    "        \n",
    "        self.sparseRetriever()\n",
    "        self.ensembleRetriever()\n",
    "\n",
    "        # retrieve relevant docs\n",
    "        results = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        print(f\"Nb of retrieved docs: {len(results)}\")\n",
    "\n",
    "        # rerank\n",
    "        scored_results=await self.reranker(results, query)\n",
    "        \n",
    "        # Concatenate retrieved documents for context\n",
    "        context = \"\\n\".join([f\"Fragment: \\n{doc['content']}\\n\" for doc in scored_results])\n",
    "        \n",
    "        tokens_counter = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "        num_tokens = (tokens_counter.encode(context))\n",
    "        print(f\"Context lenght: {len(num_tokens)} tokens\")\n",
    "\n",
    "        llm_prompt = f\"\"\"\n",
    "            Answer the question based **only** on the provided context.  \n",
    "\n",
    "            - If the context contains enough information to provide a complete or partial answer, use it to formulate a detailed and factual response.  \n",
    "            - If the context lacks relevant information, respond with: \"I don't know.\"  \n",
    "\n",
    "            ### **Context:**  \n",
    "            {context}  \n",
    "\n",
    "            ### **Question:**  \n",
    "            {query}  \n",
    "\n",
    "            ### **Answer:**  \n",
    "            Provide a clear, factual, and well-structured response based on the available context. Avoid speculation or adding external knowledge.  \n",
    "        \"\"\"\n",
    "\n",
    "        llm_completion = await self.llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in document Q/A and document synthesis\"},\n",
    "                {\"role\": \"user\", \"content\": llm_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            stream=True,\n",
    "            extra_headers={\n",
    "                \"HTTP-Referer\": \"audio-hybrid-rag-generation\",  # Optional for rankings\n",
    "                \"X-Title\": \"audio-hybrid-rag-generation\",  # Optional for rankings\n",
    "            },\n",
    "            extra_body={\n",
    "                \"user\": \"audio-hybrid-rag-generation\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        final_answer = \"\"\n",
    "        print(\"Réponse:\\n=========\")\n",
    "        async for chunk in llm_completion:\n",
    "            if hasattr(chunk.choices[0].delta, \"content\") and chunk.choices[0].delta.content:\n",
    "                final_answer += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        \n",
    "        self.history+=[\n",
    "            {\"role\": \"user\", 'content': query},\n",
    "            {\"role\": \"assistant\", \"content\": final_answer}\n",
    "        ]\n",
    "        \n",
    "        return final_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a224490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of retrieved docs: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 score: 4\n",
      "chunk 2 score: 6\n",
      "chunk 3 score: 4\n",
      "chunk 4 score: 6\n",
      "chunk 5 score: 4\n",
      "chunk 6 score: 2\n",
      "chunk 7 score: 4\n",
      "chunk 8 score: 4\n",
      "chunk 9 score: 4\n",
      "chunk 10 score: 6\n",
      "chunk 11 score: 4\n",
      "chunk 12 score: 6\n",
      "chunk 13 score: 4\n",
      "chunk 14 score: 6\n",
      "chunk 15 score: 2\n",
      "chunk 16 score: 4\n",
      "chunk 17 score: 6\n",
      "chunk 18 score: 6\n",
      "chunk 19 score: 6\n",
      "chunk 20 score: 6\n",
      "chunk 21 score: 6\n",
      "chunk 22 score: 2\n",
      "chunk 23 score: 6\n",
      "chunk 24 score: 6\n",
      "chunk 25 score: 4\n",
      "chunk 26 score: 6\n",
      "chunk 27 score: 6\n",
      "chunk 28 score: 6\n",
      "chunk 29 score: 4\n",
      "chunk 30 score: 6\n",
      "chunk 31 score: 6\n",
      "chunk 32 score: 6\n",
      "chunk 33 score: 6\n",
      "chunk 34 score: 6\n",
      "chunk 35 score: 8\n",
      "chunk 36 score: 6\n",
      "chunk 37 score: 6\n",
      "chunk 38 score: 6\n",
      "chunk 39 score: 4\n",
      "chunk 40 score: 6\n",
      "chunk 41 score: 4\n",
      "chunk 42 score: 6\n",
      "chunk 43 score: 6\n",
      "chunk 44 score: 6\n",
      "chunk 45 score: 4\n",
      "chunk 46 score: 4\n",
      "chunk 47 score: 6\n",
      "chunk 48 score: 4\n",
      "chunk 49 score: 6\n",
      "chunk 50 score: 6\n",
      "chunk 51 score: 6\n",
      "chunk 52 score: 8\n",
      "chunk 53 score: 2\n",
      "chunk 54 score: 8\n",
      "chunk 55 score: 6\n",
      "chunk 56 score: 4\n",
      "chunk 57 score: 6\n",
      "chunk 58 score: 6\n",
      "chunk 59 score: 6\n",
      "chunk 60 score: 6\n",
      "================================\n",
      " Nb of docs kept after the reranking: 30 \n",
      "==============================\n",
      "Context lenght: 6056 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse:\n",
      "=========\n",
      "### Synthèse du texte\n",
      "\n",
      "#### 1. Les idées principales à retenir\n",
      "- **Dévaluation de l'intelligence humaine** : L'intelligence artificielle (IA) pourrait dévaluer l'intelligence humaine, similaire à la dévaluation du travail humain par la révolution industrielle.\n",
      "- **Limites de l'IA** : L'IA ne comprend pas, n'a pas de conscience, ni d'intentions, et ne peut pas reproduire la dimension poétique du langage humain.\n",
      "- **Impact sur la société** : L'IA pourrait avoir des effets négatifs sur la société, notamment en infantilisant les humains et en les isolant.\n",
      "- **Domestication de l'IA** : L'IA pourrait devenir une technologie normale, domestiquée, et ne pas remplacer complètement l'intelligence humaine.\n",
      "- **Illusion de la conscience de l'IA** : La question de la conscience de l'IA est souvent exagérée et utilisée à des fins marketing.\n",
      "\n",
      "#### 2. Faits / chiffres cités qui soutiennent ces idées\n",
      "- **85 % des jeunes** trouvent que l'IA peut être un interlocuteur intime à leurs problèmes (Valentin Husson).\n",
      "- **80 % des adolescents** et une bonne partie des adultes, surtout aux États-Unis, utilisent des compagnons IA (Nathan Devers).\n",
      "- **Hervé Le Tellier** a déclaré que l'IA pourrait battre l'homme sur le plan de la créativité (Daniel Andler).\n",
      "\n",
      "#### 3. Positions de chaque intervenant\n",
      "- **Nathan Devers** : Pose des questions sur la dévaluation de l'intelligence humaine et l'illusion de la conscience de l'IA.\n",
      "- **Valentin Husson** : Considère que l'IA pourrait avoir des effets négatifs sur l'inconscient humain et infantiliser les humains.\n",
      "- **Laurence Devillers** : Pense que l'IA ne comprend pas, n'a pas de conscience, ni d'intentions, et que la question de la conscience de l'IA est souvent exagérée.\n",
      "- **Daniel Andler** : Croit que l'IA deviendra une technologie normale, domestiquée, et ne remplacera pas complètement l'intelligence humaine.\n",
      "\n",
      "#### 4. Principaux points de désaccord\n",
      "- **Sur la conscience de l'IA** :\n",
      "  - **Laurence Devillers** : \"La machine ne comprend pas, n'a pas de conscience, n'a pas d'intention, n'a pas d'émotion.\"\n",
      "  - **Daniel Andler** : \"Je pense que l'intelligence artificielle va devenir au fond, et je suis pas le seul à le dire, une technologie normale, en quelque sorte, qu'on va peu à peu domestiquer.\"\n",
      "- **Sur l'impact de l'IA sur la société** :\n",
      "  - **Valentin Husson** : \"L'intelligence algorithmique désormais capte cette énergie pulsionnelle et transforme et infantilise l'être humain pour satisfaire immédiatement sa pulsion.\"\n",
      "  - **Daniel Andler** : \"Je pense que l'intelligence artificielle va devenir au fond, et je suis pas le seul à le dire, une technologie normale, en quelque sorte, qu'on va peu à peu domestiquer.\""
     ]
    }
   ],
   "source": [
    "rag_hybrid=RAG_hybrid(model=\"mistralai/mistral-small-3.2-24b-instruct\")\n",
    "# rag_hybrid=RAG_hybrid(model=\"mistralai/mistral-large-2512\")\n",
    "# rag_hybrid=RAG_hybrid(model=\"deepseek/deepseek-chat-v3-0324\")\n",
    "# 4. Ask a question\n",
    "question = \"\"\"\n",
    "    Peux tu me faire une synthèse de ce texte en 4 parties : \n",
    "    1. Les idées principales à retenir.\n",
    "    2. les faits / chiffres cités qui soutiennent ces idées  (en précisant qui les mentionne).\n",
    "    3. les positions de chaque intervenant (en 1 à 2 phrases max pour chacun).\n",
    "    4. les principaux points de désaccord, avec une courte citation à chaque fois.\n",
    "\"\"\"\n",
    "results = await rag_hybrid.ask_llm(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc293fe8",
   "metadata": {},
   "source": [
    "#### Optimisation de l'IR avec HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af5cede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.schema.document import Document\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class RAG_hybrid_HyDE():\n",
    "    def __init__(self, model):\n",
    "        self.model=model\n",
    "        self.retrieved_docs=[]\n",
    "        self.semantic_retriever_topK=60\n",
    "        self.sparse_retriever_topK=60\n",
    "        self.reranker_topK=30\n",
    "        self.history=[]\n",
    "        self.llm_client = AsyncOpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "        )\n",
    "        self.reranker_llm=\"mistralai/mistral-small-3.1-24b-instruct\"\n",
    "        self.doc_name_hybrid=doc_name_hybrid\n",
    "        self.reranker_score_thresh=5\n",
    "        self.reranked_doc=[]\n",
    "\n",
    "    async def generate_hypothetical_document(self, query):\n",
    "        system_prompt = \"\"\"\n",
    "            You are an expert assistant.\n",
    "            Given a question, generate a hypothetical document\n",
    "            that would correctly answer the question.\n",
    "            The document must be factual, neutral, and informative.\n",
    "            Do not mention that this is a hypothetical document.\n",
    "        \"\"\"\n",
    "\n",
    "        response = await self.llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            extra_headers={\n",
    "                \"HTTP-Referer\": \"audio-hyde-rag\",  # Optional for rankings\n",
    "                \"X-Title\": \"audio-hyde-rag\",  # Optional for rankings\n",
    "            },\n",
    "            extra_body={\n",
    "                \"user\": \"audio-hyde-rag-query-document\"\n",
    "            }                            \n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "    def semanticRetriever(self):\n",
    "        # 1. Semantic Retriever (Chroma + OllamaEmbeddings)\n",
    "        embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "        if self.doc_name_hybrid == 'None':\n",
    "            return \"Error: fournir le nom du document\"\n",
    "        \n",
    "        chroma_db = Chroma(\n",
    "            persist_directory=f'./storage/vector_scores/{self.doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "            collection_name=self.doc_name_hybrid.replace(\" \",\"_\"),\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        semantic_retriever=chroma_db.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={'k': self.semantic_retriever_topK, 'fetch_k': 60000}\n",
    "        )    \n",
    "\n",
    "\n",
    "        self.chroma_db=chroma_db\n",
    "        self.semantic_retriever=semantic_retriever\n",
    "\n",
    "        return \"Success: ChromaDB setup avec succes\"\n",
    "    \n",
    "    def sparseRetriever(self):\n",
    "        # 2. Sparse Retriever (TF-IDF)\n",
    "\n",
    "        # Récupérer TOUS les documents depuis Chroma\n",
    "        all_data = self.chroma_db.get(include=[\"documents\", \"metadatas\"])\n",
    "        self.all_docs=all_data\n",
    "\n",
    "        # Convertir en liste de `Document` objects pour LangChain\n",
    "        docs = [\n",
    "            Document(page_content=text, metadata=meta or {})  # <-- Si meta est None, on met {}\n",
    "            for text, meta in zip(all_data[\"documents\"], all_data[\"metadatas\"])\n",
    "        ]\n",
    "\n",
    "        # Créer le retriever TF-IDF\n",
    "        sparse_retriever = TFIDFRetriever.from_documents(\n",
    "            documents=docs,\n",
    "            k=self.sparse_retriever_topK,\n",
    "            tfidf_params={\"min_df\": 1, \"ngram_range\": (1, 2)}\n",
    "        )\n",
    "\n",
    "        self.sparse_retriever= sparse_retriever\n",
    "    \n",
    "    def ensembleRetriever(self):\n",
    "        # 3. Ensemble Retriever (Semantic + Sparse)\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.semantic_retriever, self.sparse_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        self.ensemble_retriever=ensemble_retriever\n",
    "\n",
    "    async def reranker(self, results, query):\n",
    "\n",
    "\n",
    "        async def llm_eval(doc, query):\n",
    "            system_prompt=\"\"\"\n",
    "                Tu es un assistant expert chargé d’évaluer la pertinence d’un fragment de document par rapport à une question.\n",
    "\n",
    "                Ton objectif est d’attribuer un score de 0 à 10 selon la capacité de ce fragment à contribuer à une réponse correcte et utile à la question, même s’il ne suffit pas à lui seul.\n",
    "\n",
    "                Barème :\n",
    "                0 = Hors sujet, aucune utilité pour répondre à la question  \n",
    "                2 = Très faible lien thématique  \n",
    "                4 = Lien partiel ou contextuel faible  \n",
    "                6 = Apporte une information utile mais incomplète  \n",
    "                8 = Très pertinent, information clé pour répondre  \n",
    "                10 = Suffisant à lui seul pour répondre correctement  \n",
    "\n",
    "                Consignes importantes :\n",
    "                - Le fragment peut être incomplet mais néanmoins très pertinent s’il apporte une information essentielle.\n",
    "                - Suppose que plusieurs fragments seront combinés pour produire la réponse finale.\n",
    "                - Ne pénalise pas un document uniquement parce qu’il ne permet pas une réponse complète isolément.\n",
    "                - Évalue la similarité sémantique réelle, pas seulement les mots-clés.\n",
    "                - Ne fais aucune supposition au-delà du contenu explicite ou clairement implicite du fragment.\n",
    "                - Ignore le style, la forme et la qualité rédactionnelle.\n",
    "\n",
    "                Format de sortie strict :\n",
    "                Réponds uniquement avec le score au format JSON suivant, sans aucun texte supplémentaire :\n",
    "\n",
    "                ```json\n",
    "                {\"score\": X}\n",
    "\n",
    "            \"\"\"\n",
    "            response = await self.llm_client.chat.completions.create(\n",
    "                model=self.reranker_llm,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                        La question est: {query}\\n Le document à évaluer est le suivant\\n: {doc}\n",
    "                    \"\"\"\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0,\n",
    "                extra_headers={\n",
    "                    \"HTTP-Referer\": \"audio-hyde-rag-reranker\",  # Optional for rankings\n",
    "                    \"X-Title\": \"audio-hyde-rag-reranker\",  # Optional for rankings\n",
    "                },\n",
    "                extra_body={\n",
    "                    \"user\": \"audio-hyde-rag-reranker\"\n",
    "                }                \n",
    "            )\n",
    "            \n",
    "            # Post-process to extract only the JSON part if extra text is present\n",
    "            content = response.choices[0].message.content\n",
    "            # Try to extract the JSON block if the model adds extra text\n",
    "            match = re.search(r\"\\{.*?\\}\", content, re.DOTALL)\n",
    "            if match:\n",
    "                content = match.group(0)\n",
    "\n",
    "            # extract score\n",
    "            score=None\n",
    "            try:\n",
    "                score=content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "                \n",
    "                score= json.loads(score)\n",
    "                score=score[\"score\"]\n",
    "            except Exception as e:\n",
    "                print(e)                \n",
    "            \n",
    "            return {\"content\": doc, \"score\": score}\n",
    "\n",
    "\n",
    "        tasks=[llm_eval(doc.page_content, query) for doc in results]\n",
    "        scored_docs= await asyncio.gather(*tasks)\n",
    "        i=1\n",
    "\n",
    "        for doc in scored_docs:\n",
    "          \n",
    "            print(f'chunk {i} score: {doc[\"score\"]}')\n",
    "            i+=1\n",
    "\n",
    "        filtred_docs=[d for d in scored_docs if int(d[\"score\"])>=self.reranker_score_thresh]\n",
    "        # print(f\"scored docs; \\n{scored_docs}\")\n",
    "        self.reranked_doc=filtred_docs\n",
    "\n",
    "        filtred_docs=filtred_docs[:self.reranker_topK]\n",
    "        print(\n",
    "            \"================================\\n\",\n",
    "            \"Nb of docs kept after the reranking:\", \n",
    "            len(filtred_docs),\n",
    "            \"\\n==============================\"\n",
    "        )\n",
    "\n",
    "        return filtred_docs\n",
    "\n",
    "    def init_retrievers(self):\n",
    "        # init retrievers\n",
    "        status=self.semanticRetriever()\n",
    "        if \"Error\" in status:\n",
    "            return status\n",
    "        \n",
    "        self.sparseRetriever()\n",
    "        self.ensembleRetriever()\n",
    "\n",
    "\n",
    "    async def ask_llm(self, query):\n",
    "        # 5. Final processing step with an LLM (e.g., OpenAI via OpenRouter)\n",
    "\n",
    "        self.init_retrievers()\n",
    "\n",
    "        # HyDE\n",
    "        context=\"\"\n",
    "        for c in self.all_docs[\"documents\"][:6]:\n",
    "            context+=c\n",
    "        context_and_query=f\"\"\"\n",
    "            The query:\n",
    "            {query}\n",
    "\n",
    "            The context of the corpus:\n",
    "            {context}\n",
    "\n",
    "            Now generate a corresponding hypothetical document\n",
    "        \"\"\"\n",
    "        hypothetical_doc = await self.generate_hypothetical_document(context_and_query)\n",
    "\n",
    "        self.hypothetical_document=hypothetical_doc\n",
    "\n",
    "        # double retireval (relevant docs to query + HyDE)\n",
    "        results_query = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        results_hyde = self.ensemble_retriever.get_relevant_documents(hypothetical_doc)\n",
    "\n",
    "        # Fusion + déduplication\n",
    "        all_results = {doc.page_content: doc for doc in results_query + results_hyde}\n",
    "        results = list(all_results.values())\n",
    "\n",
    "        print(f\"Nb of retrieved docs: {len(results)}\")\n",
    "\n",
    "        # rerank\n",
    "        scored_results=await self.reranker(results, query)\n",
    "        \n",
    "        # Concatenate retrieved documents for context\n",
    "        context = \"\\n\".join([f\"Fragment: \\n{doc['content']}\\n\" for doc in scored_results])\n",
    "        \n",
    "        llm_prompt = f\"\"\"\n",
    "            Answer the question based **only** on the provided context.  \n",
    "\n",
    "            - If the context contains enough information to provide a complete or partial answer, use it to formulate a detailed and factual response.  \n",
    "            - If the context lacks relevant information, respond with: \"I don't know.\"  \n",
    "\n",
    "            ### **Context:**  \n",
    "            {context}  \n",
    "\n",
    "            ### **Question:**  \n",
    "            {query}  \n",
    "\n",
    "            ### **Answer:**  \n",
    "            Provide a clear, factual, and well-structured response based on the available context. Avoid speculation or adding external knowledge.  \n",
    "        \"\"\"\n",
    "\n",
    "        num_tokens = (tokens_counter.encode(llm_prompt))\n",
    "        \n",
    "        print(f\"Context lenght: {len(num_tokens)} tokens\")\n",
    "\n",
    "\n",
    "        llm_completion = await self.llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in document Q/A and document synthesis\"},\n",
    "                {\"role\": \"user\", \"content\": llm_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            stream=True,\n",
    "            extra_headers={\n",
    "                \"HTTP-Referer\": \"audio-hyde-rag-generation\",  # Optional for rankings\n",
    "                \"X-Title\": \"audio-hyde-rag-generation\",  # Optional for rankings\n",
    "            },\n",
    "            extra_body={\n",
    "                \"user\": \"audio-hyde-rag-generation\"\n",
    "            }                            \n",
    "        )\n",
    "\n",
    "        final_answer = \"\"\n",
    "        print(\"Réponse:\\n=========\")\n",
    "        async for chunk in llm_completion:\n",
    "            if hasattr(chunk.choices[0].delta, \"content\") and chunk.choices[0].delta.content:\n",
    "                final_answer += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        \n",
    "        self.history+=[\n",
    "            {\"role\": \"user\", 'content': query},\n",
    "            {\"role\": \"assistant\", \"content\": final_answer}\n",
    "        ]\n",
    "        \n",
    "        return final_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfdd5055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of retrieved docs: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 score: 4\n",
      "chunk 2 score: 6\n",
      "chunk 3 score: 4\n",
      "chunk 4 score: 6\n",
      "chunk 5 score: 4\n",
      "chunk 6 score: 2\n",
      "chunk 7 score: 4\n",
      "chunk 8 score: 4\n",
      "chunk 9 score: 4\n",
      "chunk 10 score: 6\n",
      "chunk 11 score: 4\n",
      "chunk 12 score: 6\n",
      "chunk 13 score: 4\n",
      "chunk 14 score: 6\n",
      "chunk 15 score: 2\n",
      "chunk 16 score: 4\n",
      "chunk 17 score: 6\n",
      "chunk 18 score: 6\n",
      "chunk 19 score: 6\n",
      "chunk 20 score: 6\n",
      "chunk 21 score: 6\n",
      "chunk 22 score: 2\n",
      "chunk 23 score: 6\n",
      "chunk 24 score: 6\n",
      "chunk 25 score: 4\n",
      "chunk 26 score: 6\n",
      "chunk 27 score: 6\n",
      "chunk 28 score: 6\n",
      "chunk 29 score: 4\n",
      "chunk 30 score: 6\n",
      "chunk 31 score: 6\n",
      "chunk 32 score: 6\n",
      "chunk 33 score: 6\n",
      "chunk 34 score: 6\n",
      "chunk 35 score: 8\n",
      "chunk 36 score: 6\n",
      "chunk 37 score: 6\n",
      "chunk 38 score: 6\n",
      "chunk 39 score: 4\n",
      "chunk 40 score: 6\n",
      "chunk 41 score: 4\n",
      "chunk 42 score: 6\n",
      "chunk 43 score: 6\n",
      "chunk 44 score: 6\n",
      "chunk 45 score: 4\n",
      "chunk 46 score: 4\n",
      "chunk 47 score: 6\n",
      "chunk 48 score: 4\n",
      "chunk 49 score: 6\n",
      "chunk 50 score: 6\n",
      "chunk 51 score: 6\n",
      "chunk 52 score: 6\n",
      "chunk 53 score: 2\n",
      "chunk 54 score: 8\n",
      "chunk 55 score: 6\n",
      "chunk 56 score: 4\n",
      "chunk 57 score: 6\n",
      "chunk 58 score: 6\n",
      "chunk 59 score: 6\n",
      "chunk 60 score: 6\n",
      "chunk 61 score: 4\n",
      "chunk 62 score: 4\n",
      "chunk 63 score: 4\n",
      "chunk 64 score: 6\n",
      "chunk 65 score: 6\n",
      "chunk 66 score: 6\n",
      "chunk 67 score: 6\n",
      "chunk 68 score: 6\n",
      "chunk 69 score: 6\n",
      "chunk 70 score: 4\n",
      "chunk 71 score: 8\n",
      "chunk 72 score: 6\n",
      "chunk 73 score: 8\n",
      "================================\n",
      " Nb of docs kept after the reranking: 30 \n",
      "==============================\n",
      "Context lenght: 6259 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse:\n",
      "=========\n",
      "### Synthèse du texte\n",
      "\n",
      "#### 1. Les idées principales à retenir\n",
      "- **Dévaluation de l'intelligence humaine** : L'IA pourrait dévaluer l'intelligence humaine, similaire à la dévaluation du travail humain par la révolution industrielle.\n",
      "- **Limites de l'IA** : L'IA ne comprend pas, n'a pas de conscience, ni d'intentions, et ne peut pas reproduire la dimension poétique du langage humain.\n",
      "- **Impact sur la société** : L'IA pourrait affecter l'inconscient humain et les relations sociales, notamment chez les adolescents et les adultes qui s'y confient.\n",
      "- **Domestication de l'IA** : L'IA pourrait devenir une technologie normale, domestiquée, mais avec des usages à limiter, comme les compagnons IA.\n",
      "- **Créativité et IA** : L'IA peut exceller dans des exercices de style, mais ne possède pas une véritable créativité humaine.\n",
      "- **Illusion de la conscience de l'IA** : Il y a une illusion marketing autour de la conscience de l'IA, qui n'est pas réelle.\n",
      "\n",
      "#### 2. Faits / chiffres cités\n",
      "- **85 % des jeunes** trouvent que l'IA peut être un interlocuteur intime à leurs problèmes (Valentin Husson).\n",
      "- **80 % des adolescents** et une bonne partie des adultes, surtout aux États-Unis, utilisent des compagnons IA (Nathan Devers).\n",
      "- **Hervé Le Tellier** pense que l'IA peut battre l'homme sur le plan de la créativité (Daniel Andler).\n",
      "\n",
      "#### 3. Positions de chaque intervenant\n",
      "- **Nathan Devers** : Met en lumière les progrès de l'IA et les questions philosophiques qu'elle soulève, tout en soulignant les limites de l'IA.\n",
      "- **Valentin Husson** : Considère que l'IA pourrait affecter l'inconscient humain et infantiliser les êtres humains en captant leur énergie pulsionnelle.\n",
      "- **Laurence Devillers** : Pense que l'IA ne comprend pas, n'a pas de conscience, et que les humains anthropomorphisent à tort l'IA.\n",
      "- **Daniel Andler** : A une vision plus sobre de l'IA, qu'il considère comme une technologie normale qui sera domestiquée, et nie que l'IA puisse battre l'homme en créativité.\n",
      "\n",
      "#### 4. Principaux points de désaccord\n",
      "- **Sur la conscience de l'IA** :\n",
      "  - **Laurence Devillers** : \"La machine ne comprend pas, n'a pas de conscience, n'a pas d'intention, n'a pas d'émotion.\"\n",
      "  - **Daniel Andler** : \"Je ne le pense pas. Euh, mais je pense que c'est pas original de pas le penser.\"\n",
      "\n",
      "- **Sur l'impact de l'IA sur l'inconscient** :\n",
      "  - **Valentin Husson** : \"L'intelligence artificielle ne s'adresse plus à la conscience au langage qui peut-être poétique comme je le disais tout à l'heure. Mais à l'inconscient, c'est-à-dire à ce que nous avons de plus archaïque en nous.\"\n",
      "  - **Daniel Andler** : \"Je crois pas que ça va modifier, enfin, l'inconscient numérique.\"\n",
      "\n",
      "- **Sur la créativité de l'IA** :\n",
      "  - **Hervé Le Tellier** (cité par Daniel Andler) : \"L'intelligence humaine étant déroute en quelque sorte.\"\n",
      "  - **Daniel Andler** : \"Je ne le pense pas. Euh, mais je pense que c'est pas original de pas le penser.\""
     ]
    }
   ],
   "source": [
    "rag_hyde=RAG_hybrid_HyDE(model=\"mistralai/mistral-small-3.2-24b-instruct\")\n",
    "# rag_hyde=RAG_hybrid_HyDE(model=\"mistralai/mistral-large-2512\")\n",
    "# rag_hyde=RAG_hybrid_HyDE(model=\"deepseek/deepseek-chat-v3-0324\")\n",
    "\n",
    "# 4. Ask a question\n",
    "question = \"\"\"\n",
    "    Peux tu me faire une synthèse de ce texte en 4 parties : \n",
    "    1. Les idées principales à retenir.\n",
    "    2. les faits / chiffres cités qui soutiennent ces idées  (en précisant qui les mentionne).\n",
    "    3. les positions de chaque intervenant (en 1 à 2 phrases max pour chacun).\n",
    "    4. les principaux points de désaccord, avec une courte citation à chaque fois.\n",
    "\"\"\"\n",
    "results = await rag_hyde.ask_llm(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "900faf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Synthèse du débat sur l'intelligence artificielle**\n",
      "\n",
      "**1. Les idées principales à retenir**\n",
      "\n",
      "- L'intelligence artificielle (IA) a accompli des progrès remarquables ces dernières années, notamment avec l'avènement de ChatGPT, et semble capable d'exécuter des tâches intellectuelles complexes.\n",
      "- L'IA soulève des questions philosophiques majeures sur la nature de l'intelligence et de la conscience humaine.\n",
      "- L'IA pourrait être un miroir de notre propre intelligence, révélant des aspects mécanisables de notre pensée.\n",
      "- Un concours littéraire entre un humain et une IA a montré des résultats surprenants, remettant en question la singularité de la créativité humaine.\n",
      "\n",
      "**2. Faits et chiffres cités**\n",
      "\n",
      "- **Nathan Devert** mentionne l'apparition de ChatGPT en novembre 2022 et ses capacités à exécuter des tâches intellectuelles variées.\n",
      "- **Nathan Devert** rapporte que Hervé Le Tellier, auteur primé, a déclaré être \"absolument bluffé par la créativité littéraire de ChatGPT\" et a même suggéré que l'IA pourrait le battre.\n",
      "- **Daniel Andler** note que Hervé Le Tellier s'est \"presque d'avance déclaré battu\" face à l'IA.\n",
      "\n",
      "**3. Positions de chaque intervenant**\n",
      "\n",
      "- **Laurence Devillers**: Professeur en IA et chercheuse, présidente de la Fondation Blaise Pascal, auteure de \"L'IA, ange ou démon\".\n",
      "- **Daniel Andler**: Mathématicien, philosophe, professeur émérite, membre de l'Académie des Sciences morales et politiques, auteur de \"Intelligence artificielle, intelligence humaine, la double énigme\".\n",
      "- **Valentin Husson**: Philosophe, auteur de \"Folle résentimentale, petite philosophie des trolls\".\n",
      "\n",
      "**4. Principaux points de désaccord**\n",
      "\n",
      "- **Daniel Andler** sur la créativité de l'IA: \"Je ne le pense pas. [...] On est généralement réticent à reconnaître que la créativité s'échapperait d'une certaine façon à l'être humain.\"\n",
      "- **Hervé Le Tellier** (rapporté par Nathan Devert) sur la créativité de l'IA: \"Peut-être même que ChatGPT m'a battu, que la nouvelle qui a été rendue par l'intelligence artificielle était plus drôle, plus originale, plus vive que celle que j'ai écrite.\"\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "print(rag_hyde.hypothetical_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fbae41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph-RAG:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'creation_tokens_input': 88941,\n",
       " 'creation_tokens_output': 16238,\n",
       " 'creation_cost_total': 0.05,\n",
       " 'qa_tokens_input': 17606,\n",
       " 'qa_tokens_output': 938,\n",
       " 'qa_cost_total': 0.0046}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid-RAG:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reranker_tokens_input': 5468,\n",
       " 'reranker_tokens_output': 150,\n",
       " 'generation_tokens_input': 871,\n",
       " 'generation_tokens_output': 348,\n",
       " 'total_tokens_input': 6339,\n",
       " 'total_tokens_output': 498,\n",
       " 'cost_total': 0.0029}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid-HyDE-RAG:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hyde_doc_generation_tokens_input': 1785,\n",
       " 'hyde_doc_generation_tokens_output': 1127,\n",
       " 'reranker_tokens_input': 10425,\n",
       " 'reranker_tokens_output': 280,\n",
       " 'generation_tokens_input': 2280,\n",
       " 'generation_tokens_output': 619,\n",
       " 'total_tokens_input': 14490,\n",
       " 'total_tokens_output': 2026,\n",
       " 'cost_total': 0.0063}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Couts Graph VS vectoriel\n",
    "import pandas\n",
    "\n",
    "df_activity=pandas.read_csv(\"openrouter_activity.csv\")\n",
    "\n",
    "display(df_activity.shape)\n",
    "\n",
    "#===== graph rag\n",
    "graph_rag_activity={}\n",
    "df_activity_graphrag_creation=df_activity[df_activity[\"user\"]==\"audio-graphrag-creation\"]\n",
    "graph_rag_activity[\"creation_tokens_input\"]=df_activity_graphrag_creation[\"tokens_prompt\"].sum().item()\n",
    "graph_rag_activity[\"creation_tokens_output\"]=(df_activity_graphrag_creation[\"tokens_reasoning\"].sum()+df_activity_graphrag_creation[\"tokens_completion\"].sum()).item()\n",
    "graph_rag_activity[\"creation_cost_total\"]=round(df_activity_graphrag_creation[\"cost_total\"].sum().item(),2)\n",
    "\n",
    "df_activity_graphrag_qa=df_activity[df_activity[\"user\"]==\"audio-graphrag-qa\"]\n",
    "graph_rag_activity[\"qa_tokens_input\"]=df_activity_graphrag_qa[\"tokens_prompt\"].sum().item()\n",
    "graph_rag_activity[\"qa_tokens_output\"]=(df_activity_graphrag_qa[\"tokens_reasoning\"].sum()+df_activity_graphrag_qa[\"tokens_completion\"].sum()).item()\n",
    "graph_rag_activity[\"qa_cost_total\"]=round(df_activity_graphrag_qa[\"cost_total\"].sum().item(),4)\n",
    "\n",
    "print(\"Graph-RAG:\")\n",
    "display(graph_rag_activity)\n",
    "\n",
    "\n",
    "# ===== hybrid RAG \n",
    "hybrid_rag_activity={}\n",
    "df_activity_hybrid_global=df_activity[df_activity[\"user\"].str.contains(\"audio-hybrid\")]\n",
    "df_activity_hybrid_reranker=df_activity_hybrid_global[df_activity_hybrid_global[\"user\"].str.contains(\"reranker\")]\n",
    "df_activity_hybrid_generation=df_activity_hybrid_global[df_activity_hybrid_global[\"user\"].str.contains(\"generation\")]\n",
    "\n",
    "hybrid_rag_activity[\"reranker_tokens_input\"]=df_activity_hybrid_reranker[\"tokens_prompt\"].sum().item()\n",
    "hybrid_rag_activity[\"reranker_tokens_output\"]=(df_activity_hybrid_reranker[\"tokens_reasoning\"].sum()+df_activity_hybrid_reranker[\"tokens_completion\"].sum()).item()\n",
    "hybrid_rag_activity[\"generation_tokens_input\"]=df_activity_hybrid_generation[\"tokens_prompt\"].sum().item()\n",
    "hybrid_rag_activity[\"generation_tokens_output\"]=(df_activity_hybrid_generation[\"tokens_reasoning\"].sum()+df_activity_hybrid_generation[\"tokens_completion\"].sum()).item()\n",
    "hybrid_rag_activity[\"total_tokens_input\"]=hybrid_rag_activity[\"reranker_tokens_input\"]+hybrid_rag_activity[\"generation_tokens_input\"]\n",
    "hybrid_rag_activity[\"total_tokens_output\"]=hybrid_rag_activity[\"reranker_tokens_output\"]+hybrid_rag_activity[\"generation_tokens_output\"]\n",
    "\n",
    "hybrid_rag_activity[\"cost_total\"]=round(df_activity_hybrid_global[\"cost_total\"].sum().item(),4)\n",
    "\n",
    "\n",
    "print(\"Hybrid-RAG:\")\n",
    "display(hybrid_rag_activity)\n",
    "\n",
    "\n",
    "#======= hybrid RAG with HyDE\n",
    "hybrid_hyde_rag_activity={}\n",
    "df_activity_hyde_global=df_activity[df_activity[\"user\"].str.contains(\"audio-hyde\")]\n",
    "df_activity_hyde_query_document=df_activity_hyde_global[df_activity_hyde_global[\"user\"].str.contains(\"query-document\")]\n",
    "df_activity_hyde_reranker=df_activity_hyde_global[df_activity_hyde_global[\"user\"].str.contains(\"reranker\")]\n",
    "df_activity_hyde_generation=df_activity_hyde_global[df_activity_hyde_global[\"user\"].str.contains(\"generation\")]\n",
    "\n",
    "\n",
    "hybrid_hyde_rag_activity[\"hyde_doc_generation_tokens_input\"]=df_activity_hyde_query_document[\"tokens_prompt\"].sum().item()\n",
    "hybrid_hyde_rag_activity[\"hyde_doc_generation_tokens_output\"]=df_activity_hyde_query_document[\"tokens_reasoning\"].sum().item()+df_activity_hyde_query_document[\"tokens_completion\"].sum().item()\n",
    "\n",
    "hybrid_hyde_rag_activity[\"reranker_tokens_input\"]=df_activity_hyde_reranker[\"tokens_prompt\"].sum().item()\n",
    "hybrid_hyde_rag_activity[\"reranker_tokens_output\"]=(df_activity_hyde_reranker[\"tokens_reasoning\"].sum()+df_activity_hyde_reranker[\"tokens_completion\"].sum()).item()\n",
    "\n",
    "hybrid_hyde_rag_activity[\"generation_tokens_input\"]=df_activity_hyde_generation[\"tokens_prompt\"].sum().item()\n",
    "hybrid_hyde_rag_activity[\"generation_tokens_output\"]=(df_activity_hyde_generation[\"tokens_reasoning\"].sum()+df_activity_hyde_generation[\"tokens_completion\"].sum()).item()\n",
    "\n",
    "hybrid_hyde_rag_activity[\"total_tokens_input\"]=hybrid_hyde_rag_activity[\"hyde_doc_generation_tokens_input\"]+ hybrid_hyde_rag_activity[\"reranker_tokens_input\"]+hybrid_hyde_rag_activity[\"generation_tokens_input\"]\n",
    "hybrid_hyde_rag_activity[\"total_tokens_output\"]=hybrid_hyde_rag_activity[\"hyde_doc_generation_tokens_output\"]+ hybrid_hyde_rag_activity[\"reranker_tokens_output\"]+hybrid_hyde_rag_activity[\"generation_tokens_output\"]\n",
    "\n",
    "hybrid_hyde_rag_activity[\"cost_total\"]=round(df_activity_hyde_global[\"cost_total\"].sum().item(),4)\n",
    "\n",
    "\n",
    "print(\"Hybrid-HyDE-RAG:\")\n",
    "display(hybrid_hyde_rag_activity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75069eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "méthodes = [\"Graph-RAG\", 'Hybrid-RAG', 'Hybrid-HyDE-RAG']\n",
    "coûts = [graph_rag_activity[\"qa_cost_total\"], hybrid_rag_activity[\"cost_total\"], hybrid_hyde_rag_activity[\"cost_total\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(méthodes, coûts, color=['#2E86AB', '#A23B72', '#F18F01'])\n",
    "ax.set_ylabel('Coût total ($)', fontsize=12)\n",
    "ax.set_title('Comparaison des coûts totaux par méthode (QA)', fontsize=14, pad=20)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, coût in zip(bars, coûts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.0000,\n",
    "            f'${coût:.4f}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14268d0",
   "metadata": {},
   "source": [
    "Commentaires:\n",
    "* le même llm a été utilisé sur toutes les méthodes, sauf le reranker (mistral small 24b)\n",
    "* Au démarrage le graph rag est le + cher à cause de la création du graphe\n",
    "* Hors création du graph, la méthode hyde semble la plus chère, mais essentiellement à cause de la longueur de la réponse:\n",
    "    * hyde: 2000 tokens \n",
    "    * graph: 1000 tokens \n",
    "    * hybrid simple: 500 tokens\n",
    "* Classement fait avec gemini 3 pro:\n",
    "\n",
    "| Réponse | Score | Verdict |\n",
    "| :--- | :---: | :--- |\n",
    "| **Graph RAG** | **8.5/10** | Très bonne, excellente structure, mais peut-être moins précise sur le concept clé du texte. |\n",
    "| **Hybrid simple** | **6.0/10** | Trop synthétique, structure confuse entre thèmes et questions. |\n",
    "| **Hybrid HyDE** | **9.5/10** | **Gagnante.** Complète, précise, structure logique liant thèmes et questions. |\n",
    "\n",
    "\n",
    "* En utilisant mistral 3 large (sur le QA) le Graph RAG prend la place de HyDE, et hybrid simple gagne 1 point\n",
    "* Avec HyDE on peut gagner marginalement avec un + petit LLM (ministral 14b ou gemma 12b), voir un reranker cross encoder, mais avec risque de scores moins pertinents (surtout avec cross encoder)\n",
    "* L'input tokens sur le graph RAG lors des QA me semble trop élevé:\n",
    "> Pour une question simple et directe `Qui sont les intervenants dans ce texte ?`, l'input token est de 21k (vs 17k pour la question de référence)\n",
    ">> Il faudra regarder le détail de la constitution du contexte par la librairie graph\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f2574",
   "metadata": {},
   "source": [
    "==============================\n",
    "\n",
    "### Evaluation des réponses\n",
    "#### <u>Le cadre général:</u>\n",
    "**1. Construire les réponses de référence:**\n",
    "* Utiliser la question cadre \"Quels sont les principaux thèmes de ce texte ?\" \n",
    "* Charger le transcript du podcast et produire la réponse avec les modèles suivant:\n",
    "    * Dans le chat openrouter ou par API:\n",
    "        * openai/gpt-5.2\n",
    "        * google/gemini-3-pro-preview\n",
    "        * anthropic/claude-sonnet-4.5\n",
    "    * Dans NotebookLM (solution spécialisée dans l'analyse des documents)\n",
    "    \n",
    "**Important**: relire et comparer les réponses avec votre compréhension du podcast, corriger et valider\n",
    "\n",
    "**2. Utiliser les réponses de référence pour évaluer les RAGs:**\n",
    "\n",
    "Pour la question cadre ci dessus, le graphrag est plus apte que le vectoriel\n",
    "\n",
    "Il existe 2 paramètres techniques principaux qui influencent nettement la qualité des réponses:\n",
    "* Le modèle d'embedding: \n",
    "    * Utilisé lors de la création du graphe dans la structuration des noeuds et leur connexion (par rapprochement cosine sim)\n",
    "    * Utilisé lors de la lecture du graphe pour répondre à une question, qui mets en relation avec la question avec les noeuds les plus pertinent (cosine sim) pour sélectionner un/des point(s) de démarrage pour l'exploration du graphe\n",
    "* Le LLM:\n",
    "    * Utilisé lors de la création du graphe pour la reconnaissance des entités, leur définition a rôles, leurs catégories, les relations entre les entités, la normalisation, déduplication et unification des entités et des catégories ...\n",
    "    * Utilisé lors de la génération de la réponse, en recevant en entrée les pertinentes parties du graphe comme contexte\n",
    "\n",
    "#### Un tableau de combinaisons pour l'évaluation\n",
    "\n",
    "Exemple de grille systématique pour tester les combinaisons de modèles d'embedding et de LLM :\n",
    "\n",
    "### **Paramètres de Test**\n",
    "\n",
    "#### **Modèles d'Embedding**\n",
    "1. **Embedding bon marché**:\n",
    "* gemma embeddign 300m, ...\n",
    "2. **Embeddings premium**:\n",
    "* gemini embedding (via openrouter)\n",
    "* openai text-embedding-3-large\n",
    "\n",
    "\n",
    "### **Modèles LLM**\n",
    "**Pour la création de graphe:**\n",
    "#### Modèles `performants` et bon marché, éligible pour création de graphe:\n",
    "* deepseek/deepseek-chat-v3-0324: $0.20/M input tokens | $0.88/M output tokens\n",
    "* deepseek/deepseek-v3.1-terminus: $0.21/M input tokens | $0.79/M output tokens\n",
    "* prime-intellect/intellect-3: $0.20/M input tokens | $1.10/M output tokens\n",
    "* openai/gpt-5-mini: $0,25/M input tokens | $2/M output tokens\n",
    "* mistralai/mistral-large-2512: $0.50/M input tokens | $1.50/M output tokens\n",
    "* mistralai/mistral-medium-3.1; $0.40/M input tokens | $2/M output tokens\n",
    "\n",
    "#### Modèles `frontière`, meilleure qualité attendue, mais prix élevé pour modèles US:\n",
    "<div class=\"alert\" style=\"color: red\">Attention au prix de l'output variable</div>\n",
    "\n",
    "* google/gemini-2.5-pro: Starting at $1.25/M input tokens | Starting at $10/M output tokens\n",
    "* openai/gpt-5.1: $1.25/M input tokens | $10/M output tokens\n",
    "* deepseek/deepseek-v3.2: $0.28/M input tokens | $0.42/M output tokens\n",
    "* moonshotai/kimi-k2-thinking: $0.45/M input tokens | $2.35/M output tokens\n",
    "* z-ai/glm-4.6; $0.39/M input tokens | $1.90/M output tokens\n",
    "\n",
    "**Pour la génération de réponse:**\n",
    "En plus des modèles `tiers 2` ci dessus, envisager les modèles suivants, assez performants pour des questions de moindre complexité, et particulièrement peu chers;\n",
    "\n",
    "* mistralai/mistral-small-3.2-24b-instruct: $0.06/M input tokens | $0.18/M output tokens\n",
    "* meta-llama/llama-4-maverick: $0.15/M input tokens | $0.60/M output tokens\n",
    "* z-ai/glm-4.5-air: $0.104/M input tokens | $0.68/M output tokens\n",
    "* google/gemma-3-27b-it: $0.04/M input tokens | $0.15/M output tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Tableau de combinaisons (Grid Search)**\n",
    "\n",
    "| Combinaison | Embedding | LLM (Création) | LLM (Génération) | Coût Relatif | Gestion de la complexité |\n",
    "|-------------|-----------|----------------|------------------|--------------|------------|\n",
    "| **C1 - Baseline Économique** | gemma emb | google/gemma-3-27b-it | google/gemma-3-27b-it | Très faible | Simple |\n",
    "| **C2 - Hybride Léger** | gemma emb | deepseek-v3.1-terminus | mistral small | Faible | moyenne |\n",
    "| **C3 - Optimal Qualité** | gemma emb | deepseek/deepseek-v3.2 | deepseek/deepseek-v3.2 | Moyen | Elevée |\n",
    "| **C4 - Maximum Performance** | gemini ou openai large | openai/gpt-5.1 | openai/gpt-5.1 | Très élevé | Elevée|\n",
    "\n",
    "\n",
    "### **Métriques d'Évaluation par combinaison**\n",
    "\n",
    "#### **Pour chaque combinaison, mesurer :**\n",
    "1. **Qualité RAG**\n",
    "   - Précision et couverture de la réponse VS référence\n",
    "   - RAGAS (Faithfulness, Answer Relevancy)\n",
    "   - Hallucinations\n",
    "\n",
    "2. **Performance Technique**\n",
    "   - Temps création graphe / génération réponses\n",
    "   - Coût pour création de graphe\n",
    "   - Coût par requête\n",
    "\n",
    "3. **Qualité Graphe**\n",
    "   - Nb de catégories formées\n",
    "   - Nb de noeuds formés\n",
    "   - Nombre de connexions\n",
    "\n",
    "\n",
    "\n",
    "### **Matrice de Décision**\n",
    "\n",
    "| Critère | Poids | C1 | C2 | C3 | C4 |\n",
    "|---------|-------|----|----|----|----|\n",
    "| **Qualité Réponse** | 70% | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| **Coût création graphe** | 10% | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐ |\n",
    "| **Coût génération réponse** | 15% | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐ |\n",
    "| **Temps** | 5% | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ |\n",
    "| **Score Total** | 100% | **58** | **68** | **72** | **77** |\n",
    "\n",
    "### **Recommandations**\n",
    "\n",
    "1. **Démarrer avec C3** (Optimal Qualité)\n",
    "* Si objectif qualité atteint (proche de C4 - Maximum Performance), tester C3 et mesurer la dégradation\n",
    "* Si objectif qualité non atteint passer à C4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705bfad",
   "metadata": {},
   "source": [
    "## Implémentation\n",
    "Pour un bon stress test du RAG il faut aller au delà de la question \"quels sont les thèmes principaux du texte\" \n",
    "\n",
    "Je propose n questions plus exigeantes et dont les sorties pourront être plus facilement comparables:\n",
    "### 1. Question cadre:\n",
    "> ```bloc\n",
    "> Peux tu me faire une synthèse de ce texte en 4 parties : \n",
    ">     1. Les idées principales à retenir.\n",
    ">     2. les faits / chiffres cités qui soutiennent ces idées  (en précisant qui les mentionne).\n",
    ">     3. les positions de chaque intervenant (en 1 à 2 phrases max pour chacun).\n",
    ">     4. les principaux points de désaccord, avec une courte citation à chaque fois.\n",
    "\n",
    "> Si une information n’est pas clairement dans le texte, dis simplement que ce n’est pas précisé.\n",
    "> ```\n",
    "\n",
    "### 2. Questions spécifiques:\n",
    "#### 1. Evaluer la resitution des positions des interlocuteurs:\n",
    "> ```bloc\n",
    "> Pour chaque intervenant, peux-tu résumer son point de vue en 2–3 phrases, et dire ce qu’il conteste ou reproche aux autres (avec un court extrait si possible) ?\n",
    "> ```\n",
    "\n",
    "#### 2. Evaluer la distinction entre faits et opinions:\n",
    "> ```bloc\n",
    "> Dans ce qui est dit, qu’est-ce qui relève plutôt de faits vérifiables (données, études, événements) et qu’est-ce qui relève plutôt de l’opinion / interprétation ?\n",
    "> Donne des exemples précis, en indiquant qui le dit, et si possible une courte citation. \n",
    "> ```\n",
    "\n",
    "#### 3. Vérifiabilité / traçabilité  pour tester la précision du retriever:\n",
    "> ```bloc\n",
    "> Quels sont les 3 arguments les plus importants avancés dans le débat ?\n",
    "> Peux-tu me dire qui les dit et où ça apparaît (une phrase ou un court extrait) ? \n",
    "> Y a-t-il des incohérences ou des contradictions dans les arguments présentés ?\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49c9b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_cadre={\n",
    "    \"question_cadre\": \"\"\"\n",
    "        Peux tu me faire une synthèse de ce texte en 4 parties : \n",
    "            1. Les idées principales à retenir.\n",
    "            2. les faits / chiffres cités qui soutiennent ces idées  (en précisant qui les mentionne).\n",
    "            3. les positions de chaque intervenant (en 1 à 2 phrases max pour chacun).\n",
    "            4. les principaux points de désaccord, avec une courte citation à chaque fois.\n",
    "    \"\"\",\n",
    "    \"reponse_cadre\": \"\"\"\n",
    "        Voici la synthèse structurée du débat retranscrit :\n",
    "\n",
    "        ### 1. Les idées principales à retenir\n",
    "\n",
    "        *   **L'illusion de la créativité :** Bien que l'IA puisse produire des textes bluffants (comme le concours littéraire évoqué), les intervenants s'accordent à dire qu'il s'agit d'optimisation mathématique, de combinatoire et d'imitation, et non de véritable créativité ou intention artistique.\n",
    "        *   **L'anthropomorphisme et le leurre de la conscience :** Il existe un danger majeur à projeter des émotions ou une conscience sur l'IA. La machine ne \"comprend\" pas ce qu'elle dit ; elle n'a ni corps, ni émotions, ni vécu.\n",
    "        *   **La blessure narcissique et l'inconscient numérique :** L'IA représente une nouvelle blessure pour l'orgueil humain (après Copernic, Darwin et Freud). Elle pourrait agir comme un \"inconscient numérique\" qui nous enferme dans la répétition (boucles algorithmiques) et la satisfaction pulsionnelle immédiate, au détriment de la poésie et de la nuance.\n",
    "        *   **L'impact sociétal et politique :** Au-delà de la technique, l'IA pose des problèmes de pouvoir (monopole des géants du numérique), de responsabilité juridique et de manipulation des comportements humains via le langage.\n",
    "\n",
    "        <details>\n",
    "        <summary>Détails supplémentaires sur les idées</summary>\n",
    "\n",
    "        *   **L'évolution technique :** Le passage d'une IA symbolique (logique) à une IA connexionniste (basée sur l'apprentissage par l'exemple et les réseaux de neurones) explique les progrès récents.\n",
    "        *   **L'usage au quotidien :** Le débat questionne si l'IA va devenir une technologie banale et \"domestiquée\" ou si elle est déjà un outil de manipulation omniprésent qui nous isole.\n",
    "\n",
    "        </details>\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### 2. Les faits et chiffres cités\n",
    "\n",
    "        | Fait / Chiffre | Contexte | Mentionné par |\n",
    "        | :--- | :--- | :--- |\n",
    "        | **Concours \"Nouvel Obs\" (Mars 2025)** | L'écrivain Hervé Le Tellier a été \"bluffé\" par une nouvelle écrite par ChatGPT, la jugeant peut-être meilleure que la sienne. | **Nathan Devers** |\n",
    "        | **Grève à Hollywood (2023)** | Grève des scénaristes pendant 6 mois craignant d'être remplacés par l'IA. | **Nathan Devers** |\n",
    "        | **85 % des jeunes** | Pourcentage de jeunes considérant l'IA comme un interlocuteur intime car elle ne pose \"pas de jugement moral\". | **Valentin Husson** |\n",
    "        | **Radiologues** | Prédiction faite il y a 10 ans sur leur disparition cause de l'IA versus la réalité actuelle où il y a plus de postes que jamais. | **Daniel Andler** |\n",
    "        | **Références historiques** | Cybernétique (1948, Norbert Wiener), Invention de l'écriture (Phèdre de Platon). | **Nathan Devers / Valentin Husson** |\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### 3. Les positions de chaque intervenant\n",
    "\n",
    "        *   **Nathan Devers (Animateur) :** Il interroge la nature philosophique de l'IA (est-elle notre deuxième conscience ?) et joue le rôle de médiateur en apportant des contextes historiques et culturels.\n",
    "        *   **Daniel Andler (Mathématicien et philosophe) :** Sceptique et rationnel, il démystifie la \"créativité\" de l'IA (c'est un exercice de style) et pense qu'elle ne remplacera pas l'homme car elle deviendra une technologie \"normale\" et utile, mais limitée.\n",
    "        *   **Laurence Devillers (Professeure en IA et chercheuse) :** Alerte sur les dangers éthiques et politiques ; elle insiste sur le fait que l'IA est un \"leurre\" sans conscience qui manipule nos comportements et critique le pouvoir des géants du numérique.\n",
    "        *   **Valentin Husson (Philosophe) :** Analyse l'IA sous l'angle psychanalytique ; il la voit comme une machine pulsionnelle qui structure notre inconscient numérique par la répétition, menaçant la poésie et la singularité humaine.\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### 4. Les principaux points de désaccord\n",
    "\n",
    "        Trois points de friction majeurs émergent de la discussion :\n",
    "\n",
    "        **1. L'IA nous remplace-t-elle au quotidien ?**\n",
    "        *   **Le désaccord :** Daniel Andler pense que l'usage de l'IA reste rare dans nos problèmes quotidiens réels, tandis que Laurence Devillers affirme qu'elle est déjà partout (reconnaissance faciale, téléphones).\n",
    "        *   **La citation :**\n",
    "            > *Daniel Andler :* \"Dans d'innombrables fonctions, l'IA n'a pas d'aide à nous apporter.\"\n",
    "            > *Laurence Devillers :* \"Je suis pas du tout d'accord avec ce point de vue... On est au quotidien avec des outils partout d'IA et on se rend pas compte.\"\n",
    "\n",
    "        **2. L'analogie avec l'invention de l'écriture**\n",
    "        *   **Le désaccord :** Valentin Husson compare les peurs actuelles sur l'IA aux peurs de Socrate sur l'écriture (déclin de la mémoire), suggérant que l'humanité s'adaptera. Laurence Devillers rejette cette comparaison, jugeant l'IA intrinsèquement manipulatrice.\n",
    "        *   **La citation :**\n",
    "            > *Valentin Husson :* \"Je crois que 2500 ans plus tard l'écriture n'a rien changé [en mal] à l'humanité.\"\n",
    "            > *Laurence Devillers :* \"Non, et l'IA, ça a rien à voir avec l'écriture... C'est quelque chose qui manipule.\"\n",
    "\n",
    "        **3. La capitulation face à la créativité de l'IA**\n",
    "        *   **Le désaccord :** Hervé Le Tellier (cité) s'est senti \"battu\" par la machine. Daniel Andler et Laurence Devillers critiquent cette posture, estimant que c'est une erreur de jugement sur la nature de l'exercice (l'optimisation n'est pas la création).\n",
    "        *   **La citation :**\n",
    "            > *Daniel Andler :* \"Il est très frappant que Hervé Le Tellier... se soit en quelque sorte presque d'avance déclaré battu.\"\n",
    "    \"\"\",\n",
    "    \"model_reference\": \"gemini 3 pro\",\n",
    "    \"questions_specifiques\": [\n",
    "        {\n",
    "            \"label\": \"positions_interlocuteurs\", \n",
    "            \"question\": \"\"\" Pour chaque intervenant, peux-tu résumer son point de vue en 2–3 phrases, et dire ce qu’il conteste ou reproche aux autres (avec un court extrait si possible) ?\"\"\",\n",
    "            \"reponse_reference\": \"\"\"\n",
    "                Voici le résumé des positions et des critiques de chaque intervenant, basé sur la synthèse fournie :\n",
    "\n",
    "                ### 1. Nathan Devers (Animateur)\n",
    "                *   **Son point de vue :** Il pose le cadre philosophique du débat en se demandant si l'IA représente une nouvelle forme de conscience ou une simple imitation. Il observe les craintes culturelles et professionnelles (comme à Hollywood) suscitées par la capacité de l'IA à produire des œuvres qui semblent rivaliser avec l'humain.\n",
    "                *   **Ce qu'il conteste / questionne :** En tant que médiateur, il ne critique pas directement, mais il pousse les autres à réagir sur la **\"victoire\" apparente de l'IA sur l'écrivain**. Il challenge l'idée que l'IA ne soit qu'un outil froid en rapportant l'expérience troublante d'Hervé Le Tellier.\n",
    "                    > *Contexte :* Il cite l'exemple du concours du \"Nouvel Obs\" pour provoquer la discussion sur la supériorité artistique de la machine.\n",
    "\n",
    "                ### 2. Daniel Andler (Mathématicien et philosophe)\n",
    "                *   **Son point de vue :** Il adopte une approche rationaliste et sceptique : pour lui, l'IA est une pure optimisation mathématique incapable de véritable intention ou de compréhension. Il croit en une \"domestication\" future de l'IA, qui deviendra un outil banal comme l'électricité, sans remplacer l'intelligence humaine globale.\n",
    "                *   **Ce qu'il conteste / reproche :** Il critique vivement l'idée la **capitulation intellectuelle** face à la machine, notamment celle d'Hervé Le Tellier, et s'oppose à Laurence Devillers sur **l'omniprésence supposée** de l'IA dans nos problèmes quotidiens concrets.\n",
    "                    > *Extrait :* \"Dans d'innombrables fonctions, l'IA n'a pas d'aide à nous apporter.\" / Il juge \"très frappant\" que Le Tellier se soit \"déclaré battu\".\n",
    "\n",
    "                ### 3. Laurence Devillers (Professeure en IA)\n",
    "                *   **Son point de vue :** Elle dénonce l'anthropomorphisme : l'IA est un \"leurre\" puissant qui n'a ni corps ni conscience, mais qui manipule nos comportements via le langage. Son inquiétude est avant tout éthique et politique, ciblant le pouvoir démesuré des géants du numérique (GAFAM) et l'isolement social que crée la technologie.\n",
    "                *   **Ce qu'elle conteste / reproche :** Elle s'oppose frontalement à Daniel Andler sur l'idée que l'IA n'est pas déjà partout, et conteste vivement l'analogie de Valentin Husson avec **l'invention de l'écriture**, jugeant l'IA bien plus dangereuse car activement manipulatrice.\n",
    "                    > *Extrait :* \"Je suis pas du tout d'accord avec ce point de vue... On est au quotidien avec des outils partout d'IA et on se rend pas compte.\" / \"L'IA, ça a rien à voir avec l'écriture... C'est quelque chose qui manipule.\"\n",
    "\n",
    "                ### 4. Valentin Husson (Philosophe)\n",
    "                *   **Son point de vue :** Il analyse l'IA comme un \"inconscient numérique\" freudien : une machine à répéter des boucles (pulsions de mort) qui nous enferme dans la satisfaction immédiate et le conformisme. Il craint que l'usage de l'IA n'appauvrisse notre langage et notre capacité à la poésie et à la nuance.\n",
    "                *   **Ce qu'il conteste / reproche :** Il tente de relativiser la panique morale en la comparant aux craintes historiques (comme Platon face à l'écriture), une position que Laurence Devillers rejette. Il critique aussi implicitement l'idée que **l'IA puisse être un \"tuteur\" moral**, notant que les jeunes l'aiment justement parce qu'elle ne juge pas, ce qui est un piège narcissique.\n",
    "                    > *Contexte :* Sa comparaison avec le \"Phèdre\" de Platon vise à nuancer la nouveauté du danger, ce qui crée une friction avec la vision plus alarmiste de Devillers sur la manipulation active.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"distinction entre faits et opinions\",\n",
    "            \"question\": '''\n",
    "                Dans ce qui est dit, qu’est-ce qui relève plutôt de faits vérifiables (données, études, événements) et qu’est-ce qui relève plutôt de l’opinion / interprétation ?\n",
    "                Donne des exemples précis, en indiquant qui le dit, et si possible une courte citation. \n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            'label': \"verifiabilite_precision_retriever\",\n",
    "            \"question\": \"\"\"\n",
    "                Quels sont les 3 arguments les plus importants avancés dans le débat ?\n",
    "                Peux-tu me dire qui les dit et où ça apparaît (une phrase ou un court extrait) ? \n",
    "                Y a-t-il des incohérences ou des contradictions dans les arguments présentés ?\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_as_judge(question: str, reference: str, candidat: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    import json\n",
    "\n",
    "    llm= OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "    )\n",
    "    resp = llm.chat.completions.create(\n",
    "        model=\"google/gemini-3-pro-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                Tu es un évaluateur strict et impartial. Ta tâche est d’évaluer une réponse produite par une chaîne RAG en la comparant à une réponse de référence (gold) pour une même question, et de produire une évaluation structurée.\n",
    "\n",
    "                ENTRÉES\n",
    "                - Question: {QUESTION}\n",
    "                - Réponse_RAG: {RAG_ANSWER}\n",
    "                - Réponse_Référence: {REFERENCE_ANSWER}\n",
    "\n",
    "                PRINCIPES (à suivre strictement)\n",
    "                1) Base d’évaluation:\n",
    "                - Évalue par rapport à la Question et à la Réponse_Référence.\n",
    "                - N’utilise pas de connaissances externes pour trancher la factualité.\n",
    "                - Utilise la Réponse_Référence comme ancre, mais accepte que la Réponse_RAG puisse être meilleure/plus complète.\n",
    "\n",
    "                2) Informations supplémentaires dans la Réponse_RAG (important):\n",
    "                - Si la Réponse_RAG ajoute des informations ABSENTES de la Réponse_Référence:\n",
    "                    a) Si elles sont cohérentes avec la Question, non contradictoires avec la Réponse_Référence, et utiles pour mieux répondre, considère-les comme un PLUS (meilleure complétude).\n",
    "                    b) Si elles contredisent la Réponse_Référence, sont hors-sujet, trop affirmatives sans appui dans la Question/Référence, ou invérifiables dans ce cadre, classe-les comme RISQUE (ajouts non étayés / possibles hallucinations).\n",
    "                - N’appelle “hallucination” que ce qui est incohérent, contradictoire, ou gratuit au regard de la Question/Référence.\n",
    "\n",
    "                3) Écarts à analyser (version simplifiée):\n",
    "                - Manques: éléments attendus (selon la Référence et les exigences de la Question) absents ou trop vagues.\n",
    "                - Erreurs: contradictions ou contresens par rapport à la Référence; mauvaise réponse à la Question.\n",
    "                - Ajouts utiles: apports pertinents qui améliorent la réponse sans contredire la Référence.\n",
    "                - Ajouts risqués: apports non étayés, trop spécifiques, ou potentiellement inventés dans ce cadre.\n",
    "                - Structure: respect des contraintes explicites de la Question (format, sections, nombre d’éléments, concision, etc.).\n",
    "\n",
    "                BARÈME (score de 1 à 10)\n",
    "                - 10: répond parfaitement à la Question; couvre la Référence; structure conforme; ajouts utiles éventuels; aucun ajout risqué notable.\n",
    "                - 8–9: très bon; quelques oublis mineurs ou structure légèrement perfectible; ajouts majoritairement utiles.\n",
    "                - 6–7: correct mais incomplet; plusieurs manques; quelques ajouts risqués ou formulations trop vagues.\n",
    "                - 4–5: faible; manques importants; erreurs notables; structure peu respectée; ajouts risqués fréquents.\n",
    "                - 1–3: très faible; contresens majeurs; ne répond pas à la Question; nombreuses inventions/contradictions.\n",
    "\n",
    "                FORMAT DE SORTIE (JSON UNIQUEMENT — aucun texte hors JSON)\n",
    "                {\n",
    "                \"score\": <entier 1..10>,\n",
    "                \"evaluation\": {\n",
    "                    \"verdict\": \"<2–4 phrases résumant la qualité globale>\",\n",
    "                    \"coherence\": \"<bon/moyen/faible + 1 phrase>\",\n",
    "                    \"exhaustivite\": \"<bon/moyen/faible + 1 phrase (par rapport à Question + Référence)>\",\n",
    "                    \"structure\": \"<bon/moyen/faible + 1 phrase (respect du format/contraintes)>\",\n",
    "                    \"ecarts\": {\n",
    "                    \"manques\": [\"<liste courte de manques clés>\"],\n",
    "                    \"erreurs\": [\"<liste courte d’erreurs/contradictions vs Référence>\"],\n",
    "                    \"ajouts_utiles\": [\"<liste courte d’ajouts pertinents>\"],\n",
    "                    \"ajouts_risques\": [\"<liste courte d’ajouts non étayés/invérifiables>\"]\n",
    "                    },\n",
    "                    \"recommandations\": [\"<3 actions concrètes max pour améliorer la réponse RAG>\"]\n",
    "                }\n",
    "                }\n",
    "\n",
    "                NOTES\n",
    "                - Si une catégorie ne s’applique pas, mets une liste vide [].\n",
    "                - Reste concis: listes courtes, formulations directes.\n",
    "                \n",
    "             \"\"\"},\n",
    "            {   \"role\": \"user\", \n",
    "                \"content\": f\"\"\"\n",
    "                    ENTRÉES\n",
    "                    - Question: {question}\n",
    "                    - Réponse_RAG: {candidat}\n",
    "                    - Réponse_Référence: {reference}\n",
    "             \n",
    "                    Evalue la Réponse_RAG\n",
    "                \"\"\"\n",
    "            }\n",
    "        ],        \n",
    "        stream=False,\n",
    "        extra_headers={\n",
    "            \"HTTP-Referer\": \"audio-hybrid-rag-evaluation\",  # Optional for rankings\n",
    "            \"X-Title\": \"audio-hybrid-rag-evaluation\",  # Optional for rankings\n",
    "        },\n",
    "        extra_body={\n",
    "            \"user\": \"audio-hybrid-rag-evaluation\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        structured_resp= json.loads(resp.choices[0].message.content.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "        return structured_resp\n",
    "    except Exception as e:\n",
    "        print(\"evaluation format json incorrect:\", e)\n",
    "        return {\"score\": 0, \"evaluation\": \"format error\"}\n",
    "    \n",
    "async def pipeline_qa_evaluation(rag_a_evaluer: dict, query: str, reference: str) -> dict:\n",
    "    if rag_a_evaluer[\"rag_type\"]=='graph':\n",
    "        # question cadre\n",
    "        resp= rag_a_evaluer[\"instance\"].query(\n",
    "            query= query, \n",
    "            param=QueryParam(mode=\"hybrid\", stream=False,)\n",
    "        )\n",
    "\n",
    "        resp=resp.replace(\"```markdown\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "        evaluation=llm_as_judge(\n",
    "            question=query,\n",
    "            reference=reference,\n",
    "            candidat=resp\n",
    "        )\n",
    "\n",
    "        return evaluation\n",
    "\n",
    "    if rag_a_evaluer[\"rag_type\"].index(\"hybrid\")>=0:\n",
    "        # question cadre\n",
    "        resp= await rag_a_evaluer[\"instance\"].ask_llm(query)\n",
    "\n",
    "        resp=resp.replace(\"```markdown\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "        evaluation=llm_as_judge(\n",
    "            question=query,\n",
    "            reference=reference,\n",
    "            candidat=resp\n",
    "        )\n",
    "\n",
    "        return evaluation    \n",
    "\n",
    "    \n",
    "\n",
    "# model=\"deepseek/deepseek-v3.2\"\n",
    "model=\"mistralai/mistral-small-3.2-24b-instruct\"\n",
    "\n",
    "\n",
    "\n",
    "rag_pipelines=[\n",
    "    {\"rag_type\": \"graph\", \"instance\": pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"rag\"]},\n",
    "    {\"rag_type\": \"hybrid\", \"instance\": RAG_hybrid(model=model)},\n",
    "    {\"rag_type\": \"hybrid_hyde\", \"instance\": RAG_hybrid_HyDE(model=model)}\n",
    "]\n",
    "\n",
    "evaluations_results=[]\n",
    "for rag in rag_pipelines:\n",
    "    evaluation=await pipeline_qa_evaluation(rag, evaluation_cadre[\"question_cadre\"], evaluation_cadre[\"reponse_cadre\"])\n",
    "    evaluations_results.append(\n",
    "        {\n",
    "            \"question\": evaluation_cadre[\"question_cadre\"],\n",
    "            \"question_level\": \"cadre\",\n",
    "            \"reponse_reference\": evaluation_cadre[\"reponse_cadre\"],\n",
    "            \"evaluation_score\": evaluation[\"score\"],\n",
    "            \"evaluation_text\": evaluation[\"evaluation\"],\n",
    "            \"rag_type\": rag[\"rag_type\"]\n",
    "        }\n",
    "    )\n",
    "    # questions spécifiques\n",
    "    for el in evaluation_cadre[\"questions_specifiques\"]:\n",
    "        if \"reponse_reference\" in el and len(el[\"reponse_reference\"])>50:\n",
    "\n",
    "            evaluation=await pipeline_qa_evaluation(rag, el[\"question\"], el[\"reponse_reference\"])\n",
    "            \n",
    "            evaluations_results.append(\n",
    "                {\n",
    "                    \"question\": el[\"question\"],\n",
    "                    \"question_level\": \"specifique\",\n",
    "                    \"reponse_reference\": el[\"reponse_reference\"],\n",
    "                    \"evaluation_score\": evaluation[\"score\"],\n",
    "                    \"evaluation_text\": evaluation[\"evaluation\"],\n",
    "                    \"rag_type\": rag[\"rag_type\"]\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af54c127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evaluations_results.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(evaluations_results, filename='evaluations_results.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
