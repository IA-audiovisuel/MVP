{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c89db4",
   "metadata": {},
   "source": [
    "Conda pour le venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b419c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.créer un nouvel env conda à partir du terminal\n",
    "# conda create --name asr_rag python=3.10\n",
    "\n",
    "# 2. activer cet environnement\n",
    "# conda activate asr_rag\n",
    "\n",
    "# 3. installer les dépendences\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# 4. installer ollama: https://www.ollama.com/download\n",
    "\n",
    "\n",
    "# 5. installer le modème d'OllamaEmbeddings\n",
    "# ollama pull embeddinggemma\n",
    "\n",
    "# 6. créer une clé api sur openrouter pour utiliser des llm gratuitement\n",
    "\n",
    "# 7. mettre votre clé dans le fichier .env (racine du repertoire courant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69208d6",
   "metadata": {},
   "source": [
    "UV pour le venv (plus rapide et efficace dans la résolution des conflits de versions)\n",
    "\n",
    "# -------------------------------------------------\n",
    "1. Installer uv (si ce n’est pas déjà fait)\n",
    "```bash\n",
    "curl -LsSf https://uv.run\n",
    "```\n",
    "\n",
    "# -------------------------------------------------\n",
    "2. Aller dans la racine du repo (ici MVP) et initialiser le projet uv\n",
    "```bash\n",
    "uv init\n",
    "```\n",
    "\n",
    "# -------------------------------------------------\n",
    "3. Créer l’environnement virtuel\n",
    "```bash\n",
    "uv venv --python 3.11\n",
    "```\n",
    "\n",
    "# -------------------------------------------------\n",
    "4. activer cet environnement\n",
    "> Linux / macOS\n",
    "> ```bash\n",
    "> source .venv/bin/activate\n",
    "> ```\n",
    "\n",
    "# -------------------------------------------------\n",
    "5. aller avec le terminal dans le dossier RAG installer les dépendences\n",
    "```bash\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "# -------------------------------------------------\n",
    "6. installer ollama: https://www.ollama.com/download\n",
    "\n",
    "# -------------------------------------------------\n",
    "7. installer le modème d'OllamaEmbeddings\n",
    "```bash\n",
    "ollama pull embeddinggemma\n",
    "```\n",
    "\n",
    "# -------------------------------------------------\n",
    "8. créer une clé api sur openrouter pour utiliser des llm gratuitement\n",
    "\n",
    "9. mettre votre clé dans le fichier .env (racine du repertoire courant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0299e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chougar/miniconda3/envs/pathrag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from pathrag_retriever import create_graphdb, load_existing_graphdb, load_knowledgeGraph_vis\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "OPENROUTER_API_KEY=os.getenv(\"OPENROUTER_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbb31f",
   "metadata": {},
   "source": [
    "#### Création ou chargement d'un graphe existant:\n",
    "> Lorsque vous executer la cellule ci dessous, vous avez 2 options proposées:\n",
    "> 1. Créer un nouveau graphe\n",
    "> 2. Charger un graphe existant\n",
    "> \n",
    "> Un prompt de sélection s'affichera en haut du notebook<br>\n",
    "> Saisir l'action désirée dans le , suivre instructions\n",
    "\n",
    "> Si vous voulez charger un graphe déjà crée et que vous ne savez plus son nom, retrouver le dans le fichier `graphrag_hashes.json` (dans la racine du dossier), attribut `Nom du doc`\n",
    "\n",
    "> Si vous voulez modifier le LLM utilisé pour la création du graphe ou sa lecture, allez dans `pathrag_retriever.py`, sous la ligne 36( Choix du LLM OpenRouter), et prenez un modèle valide sur openrouter (attention à prendre un modèle qui supporte les `structured_outputs`, sur le site openrouter, à filtrer sur le paneau à gauche dans la liste des `Supported parameters`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29614848",
   "metadata": {},
   "source": [
    "#### 1. Créer un graphe, ou charger en un déjà crée\n",
    "Ci dessous un print des noms de docs disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d78ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IA conscience\n",
      "L-IA-notre-deuxieme-conscience\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "graph_names=pd.read_json(\"graphrag_hashes.json\")\n",
    "\n",
    "graph_names=graph_names.loc[\"Nom du doc\", :].values.tolist()\n",
    "for n in graph_names:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b63b4a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PathRAG:Logger initialized for working directory: /home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5\n",
      "INFO:PathRAG:Load KV llm_response_cache with 0 data\n",
      "INFO:PathRAG:Load KV full_docs with 1 data\n",
      "INFO:PathRAG:Load KV text_chunks with 11 data\n",
      "INFO:PathRAG:Loaded graph from /home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/graph_chunk_entity_relation.graphml with 84 nodes, 56 edges\n",
      "INFO:nano-vectordb:Load (82, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/vdb_entities.json'} 82 data\n",
      "INFO:nano-vectordb:Load (56, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/vdb_relationships.json'} 56 data\n",
      "INFO:nano-vectordb:Load (11, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/experiments/associatif/IA audiovisuel/RAG/storage/graph_stores/9fa530c1fd2fc3cee4831f8edcd4822b9353bd24d1dbabef4a12af5a9f4501f5/vdb_chunks.json'} 11 data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nom de votre graphe est L-IA-notre-deuxieme-conscience\n",
      "\n",
      "        ----------------\n",
      "        #### Graph RAG retriever\n",
      "        Chargement de la base Graph RAG\n",
      "    \n",
      "**✅ Graph RAG chargé**\n",
      "Confirmation LLM read: mistralai/mistral-large-2512\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#==== params de base====\n",
    "\n",
    "# Obligatoire: nom des sources\n",
    "filename=\"audio-text.txt\"\n",
    "doc_name_graph=\"L-IA-notre-deuxieme-conscience\"\n",
    "\n",
    "# Optionnels:\n",
    "# 1. modèle pour création du graph\n",
    "# param 'OPENROUTER_MODEL_graph_creation', par défaut \"deepseek/deepseek-chat-v3-0324\"\n",
    "# passer dans la fonction 'create_graphdb' un autre modèle si vous le souhaitez\n",
    "\n",
    "# 2. modèle pour lecture du graph (questions/réponses)\n",
    "# param 'OPENROUTER_MODEL_graph_read', par défaut \"deepseek/deepseek-chat-v3-0324\"\n",
    "# passer dans la fonction 'load_existing_graphdb' un autre modèle si vous le souhaitez\n",
    "\n",
    "#=======================\n",
    "\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_docs = loader.load()\n",
    "text=\"\"\n",
    "for doc in txt_docs:\n",
    "    text+=doc.page_content\n",
    "\n",
    "\n",
    "graphrag_action=input(\"Saisir 'C' pour créer un nouveau graphe, 'L' pour charger un graphe existant\")\n",
    "\n",
    "# créer un nouveau graphe\n",
    "messages=None\n",
    "\n",
    "if graphrag_action=='C':\n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "    messages= create_graphdb(\n",
    "        text=text, \n",
    "        doc_name=doc_name_graph, # il faut donner un nom unique permettant d'identifier et charger le graph les prochaines fois\n",
    "    )\n",
    "# charger un graphe existant\n",
    "elif graphrag_action=='L':    \n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "\n",
    "    messages=load_existing_graphdb(\n",
    "        doc_name_graph, \n",
    "        OPENROUTER_MODEL_graph_read=\"mistralai/mistral-large-2512\"\n",
    "    )\n",
    "else:\n",
    "    print('Option invalide')\n",
    "\n",
    "\n",
    "\n",
    "if messages:\n",
    "    pipeline_args={}\n",
    "    for feedback in messages:\n",
    "        if isinstance(feedback, str):\n",
    "            print(feedback)\n",
    "        elif isinstance(feedback, dict):\n",
    "            pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"]=feedback[\"pipeline_args\"]\n",
    "            \n",
    "print(\"Confirmation LLM read:\", pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"llm_graph_QA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1543579",
   "metadata": {},
   "source": [
    "#### 2. Poser vos questions au graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de14e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:PathRAG:kw_prompt result:\n",
      "INFO:PathRAG:```json\n",
      "{\n",
      "  \"high_level_keywords\": [\n",
      "    \"Thèmes principaux\",\n",
      "    \"Analyse de texte\",\n",
      "    \"Compréhension de contenu\",\n",
      "    \"Interprétation littéraire\",\n",
      "    \"Questions pertinentes\"\n",
      "  ],\n",
      "  \"low_level_keywords\": [\n",
      "    \"Idées centrales\",\n",
      "    \"Sujets abordés\",\n",
      "    \"Problématiques soulevées\",\n",
      "    \"Axes de réflexion\",\n",
      "    \"Exemples de questions\",\n",
      "    \"Structure du texte\",\n",
      "    \"Messages clés\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "INFO:PathRAG:Local query uses 40 entites, 15 relations, 4 text units\n",
      "INFO:PathRAG:Global query uses 55 entites, 40 relations, 3 text units\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response all ready\n",
      "Ce texte, issu d’une discussion animée sur *France Culture*, explore les enjeux philosophiques, techniques et sociétaux de l’intelligence artificielle (IA) à travers les échanges entre plusieurs intervenants, dont **Nathan Devers** (animateur), **Laurence Devillers** (spécialiste de l’IA et de la manipulation), **Valentin Husson** (philosophe) et **Daniel Andler** (mathématicien et philosophe). Les thèmes abordés sont riches et multidimensionnels, mêlant analyses historiques, critiques technologiques et réflexions sur l’avenir de l’humanité. Voici une synthèse des principaux thèmes et des questions qu’ils soulèvent.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Les limites et illusions de l’intelligence artificielle**\n",
      "Un des fils conducteurs du débat est la **démystification de l’IA**, souvent présentée comme une révolution capable de rivaliser avec l’intelligence humaine. Les intervenants insistent sur ses **limites fondamentales** :\n",
      "- **Absence de conscience et d’intention** : Laurence Devillers souligne que les machines comme *ChatGPT* n’ont ni conscience, ni intention, ni émotion. Elles fonctionnent grâce à des algorithmes et à des données préexistantes, sans compréhension réelle du langage ou de la psyché humaine. Par exemple, elles ne peuvent pas saisir les nuances inconscientes (refoulement, lapsus, silences) qui font la richesse des interactions humaines.\n",
      "- **Créativité artificielle vs. créativité humaine** : L’IA peut produire des textes ou des diagnostics médicaux impressionnants (comme la détection de cancers précoces), mais elle le fait par **optimisation statistique** et non par une démarche créative intrinsèque. Comme le note Daniel Andler, un écrivain ou un compositeur est motivé par le désir de s’adresser à un public, une dimension absente chez l’IA. Les métaphores, l’humour ou l’inconscient – éléments clés de la créativité humaine – lui échappent totalement.\n",
      "- **Le paramètre de \"température\"** : Laurence Devillers explique que l’illusion de créativité dans des outils comme *ChatGPT* provient d’un paramètre technique (la \"température\") qui introduit du hasard dans les réponses. Cela simule une forme d’originalité, mais reste une mécanique algorithmique.\n",
      "\n",
      "**Questions posées** :\n",
      "- Jusqu’où peut-on parler de \"créativité\" pour une machine qui ne fait que recombiner des données existantes ?\n",
      "- Comment distinguer une performance technique (comme écrire une nouvelle policière) d’une véritable création artistique ?\n",
      "- L’IA peut-elle un jour comprendre l’inconscient humain, ou restera-t-elle à jamais un \"miroir déformant\" de nos propres productions ?\n",
      "\n",
      "---\n",
      "\n",
      "### **2. L’impact sociétal et professionnel de l’IA**\n",
      "Le texte aborde les **conséquences concrètes** de l’IA sur le travail, l’éducation et les comportements humains :\n",
      "- **Remplacement des métiers et standardisation** : La grève des scénaristes à Hollywood en 2023 illustre les craintes de voir l’IA remplacer des professions créatives. Laurence Devillers met en garde contre la **standardisation** des tâches : si les humains se contentent de copier-coller les productions des machines (comme les étudiants avec leurs dissertations), ils risquent de perdre leur expertise et leur pouvoir de création. Cela rejoint la notion de **\"désapprentissage\"** : en déléguant trop à l’IA, les humains pourraient perdre leurs compétences critiques.\n",
      "- **Manipulation et éducation** : Devillers insiste sur le **pouvoir manipulateur du langage** utilisé par l’IA, notamment auprès des enfants. Elle appelle à une **éducation urgente** pour apprendre à décrypter ces mécanismes. Par exemple, les agents conversationnels pourraient isoler les individus ou répéter des biais sans que ceux-ci en aient conscience.\n",
      "- **Coévolution humain-machine** : Le débat évoque l’idée d’une **symbiose** entre humains et machines, où les deux s’influencent mutuellement. Mais cette relation soulève des questions éthiques : qui contrôle qui ? Comment éviter que l’IA ne façonne nos comportements de manière invisible ?\n",
      "\n",
      "**Questions posées** :\n",
      "- Quels métiers sont réellement menacés par l’IA, et comment s’adapter à cette transformation ?\n",
      "- Comment former les jeunes générations à utiliser l’IA de manière critique, sans tomber dans la dépendance ou la manipulation ?\n",
      "- Faut-il réguler l’IA pour limiter son impact sur les comportements humains, et si oui, comment ?\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Les racines historiques et philosophiques de l’IA**\n",
      "Les intervenants ancrent leur réflexion dans une **généalogie intellectuelle** qui permet de mieux comprendre les enjeux actuels :\n",
      "- **Le \"rêve prométhéen\"** : Daniel Andler décrit l’IA comme l’héritière d’un rêve ancien : **voler aux dieux le secret de l’intelligence humaine**. Ce rêve, né avec la cybernétique dans les années 1940 (notamment avec Norbert Wiener), oscille entre une quête scientifique légitime et une ambition démiurgique. Wiener lui-même alertait sur les risques de \"dévaluation du cerveau humain\" par les machines.\n",
      "- **Les \"blessures narcissiques\" de l’humanité** : Valentin Husson reprend la théorie freudienne des trois blessures narcissiques (Copernic, Darwin, Freud) pour y ajouter une **quatrième blessure** : l’IA. Après avoir perdu notre place au centre de l’univers, notre statut d’espèce supérieure et notre maîtrise de notre inconscient, l’humanité craint désormais de ne plus être maîtresse de son intelligence. Cette angoisse explique en partie les réactions de rejet ou de fascination excessive envers l’IA.\n",
      "- **Parallèles historiques** : Husson cite le *Phèdre* de Platon, où Socrate s’inquiétait de l’invention de l’écriture, craignant qu’elle ne rende les hommes \"oubliux\" de la parole des maîtres. Ce parallèle montre que **chaque révolution technologique suscite des craintes similaires**, souvent exagérées. Pourtant, l’écriture a finalement enrichi l’humanité sans la détruire – l’IA suivra-t-elle le même chemin ?\n",
      "\n",
      "**Questions posées** :\n",
      "- L’IA est-elle une révolution comparable à l’invention de l’écriture, ou représente-t-elle une rupture plus profonde ?\n",
      "- Comment éviter de répéter les erreurs du passé (comme la surestimation des capacités des machines) tout en tirant parti des progrès technologiques ?\n",
      "- Le \"rêve prométhéen\" de l’IA est-il compatible avec une approche éthique et responsable ?\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Les enjeux éthiques et politiques**\n",
      "Enfin, le texte soulève des **défis éthiques et politiques** liés à l’IA :\n",
      "- **Responsabilité et pouvoir des géants du numérique** : Les \"tech giants\" (Meta, Google, etc.) sont critiqués pour leur course à la **\"super intelligence\"**, présentée comme un \"Graal\" mais dénoncée comme une stratégie de **marketing et de pouvoir**. Laurence Devillers et Valentin Husson mettent en garde contre une **délégation excessive** aux machines, qui pourrait mener à une perte de contrôle démocratique.\n",
      "- **Statut juridique de l’IA** : La proposition de donner une **personnalité juridique** aux IA (évoquée par Sam Altman) est vivement critiquée. Elle reviendrait à exonérer les humains de leur responsabilité dans les actions des machines, tout en conférant des droits à des entités non conscientes.\n",
      "- **Risque totalitaire** : Le texte suggère que l’IA pourrait être utilisée pour **contrôler les populations** (via la manipulation du langage ou la surveillance), rappelant les dérives des régimes autoritaires. Cela pose la question de la **régulation** : comment encadrer l’IA sans étouffer l’innovation ?\n",
      "\n",
      "**Questions posées** :\n",
      "- Faut-il accorder des droits aux IA, ou cela reviendrait-il à légitimer une forme de \"déshumanisation\" ?\n",
      "- Comment concilier innovation technologique et protection des libertés individuelles ?\n",
      "- Quels garde-fous mettre en place pour éviter que l’IA ne devienne un outil de domination ?\n",
      "\n",
      "---\n",
      "\n",
      "### **5. La place de l’humain face à l’IA**\n",
      "Au cœur du débat se trouve une question fondamentale : **qu’est-ce qui fait la singularité de l’humain ?**\n",
      "- **La créativité comme essence humaine** : Contrairement aux machines, les humains sont capables de **bifurquer**, de créer des métaphores, de puiser dans leur inconscient. Comme le dit Valentin Husson, c’est cette capacité à se \"désautomatiser\" qui fait la beauté et l’honneur de l’humanité.\n",
      "- **L’IA comme miroir de nos limites** : L’IA révèle aussi nos propres faiblesses, comme notre tendance à **projeter des intentions** sur les machines (anthropomorphisme) ou à surestimer leurs capacités. Laurence Devillers rappelle que les machines ne \"comprennent\" pas : elles simulent.\n",
      "- **Un avenir à inventer** : Le débat ne se conclut pas sur un rejet ou une adhésion totale à l’IA, mais sur la nécessité d’un **dialogue critique**. Comme le suggère Husson, l’humanité a toujours su s’adapter aux révolutions technologiques – à condition de ne pas céder à la peur ou à l’illusion.\n",
      "\n",
      "**Questions posées** :\n",
      "- L’IA peut-elle nous aider à mieux comprendre ce qui nous rend humains, ou au contraire, brouiller cette frontière ?\n",
      "- Comment préserver les dimensions irremplaçables de l’humanité (créativité, émotion, inconscient) dans un monde de plus en plus dominé par les machines ?\n",
      "- Faut-il voir l’IA comme une menace, une opportunité, ou les deux ?\n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion : un débat aux multiples facettes**\n",
      "Ce texte offre une **radiographie des tensions** autour de l’IA, entre fascination technologique, craintes existentielles et espoirs de progrès. Il invite à une réflexion **multidisciplinaire**, mêlant philosophie, histoire, éthique et technique, pour aborder l’IA non comme un simple outil, mais comme un **miroir de nos sociétés**.\n",
      "\n",
      "Les questions qu’il soulève sont autant de pistes pour l’avenir :\n",
      "- **Comment réguler l’IA sans étouffer l’innovation ?**\n",
      "- **Comment éduquer les citoyens à un usage critique des technologies ?**\n",
      "- **Comment préserver la singularité humaine dans un monde de plus en plus automatisé ?**\n",
      "\n",
      "Ces enjeux dépassent le cadre technique pour toucher à l’essence même de ce que signifie **être humain à l’ère numérique**."
     ]
    }
   ],
   "source": [
    "from PathRAG import QueryParam\n",
    "import asyncio\n",
    "\n",
    "def stream_pathRAG_response(stream_resp):\n",
    "    async def stream_response():        \n",
    "        # Process the async generator\n",
    "        async for chunk in stream_resp:\n",
    "            print(chunk or \"\", end=\"\")\n",
    "\n",
    "\n",
    "    # Run in Streamlit's existing event loop\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(stream_response())\n",
    "\n",
    "# question=\"résume ce texte dans sa langue source\"\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "\n",
    "resp=pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"rag\"].query(\n",
    "    query= question, \n",
    "    param=QueryParam(mode=\"hybrid\", stream=True,)\n",
    ")\n",
    "\n",
    "stream_pathRAG_response(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f595f8",
   "metadata": {},
   "source": [
    "============================\n",
    "### RAG vectoriel\n",
    "1. Embedding du document -> renseigner le nom de votre fichier dans `filename` et le nom de votre DB dans `doc_name_hybrid`\n",
    "2. Setup du retriever / reranker / llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76565f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from audio-text.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de chuncks: 51\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "# chargement et fragmentation du doc\n",
    "## Nom du doc à traiter\n",
    "filename=\"audio-text.txt\"\n",
    "\n",
    "## Nom pour la base vectorielle\n",
    "doc_name_hybrid=\"L-IA-notre-deuxieme-conscience_sample\" # nom de doc significatif\n",
    "\n",
    "\n",
    "\n",
    "#========= choix du modèle d'embedding\n",
    "\"\"\"\n",
    "    Le modèle choisi impacte la qualité du retriever, mais aussi le temps de traitement\n",
    "    Si le déploiement est prévu sur une VM limitée, un modèle plus petit est nécessaire\n",
    "    Explorer les comparatifs: https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "\"\"\"\n",
    "# Utiliser OllamaEmbeddings avec le modèle local \"embeddinggemma\"\n",
    "embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "\n",
    "\n",
    "# loader = UnstructuredFileLoader(filename)\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_doc = loader.load()\n",
    "print(f\"Loaded {len(txt_doc)} documents from {filename}\")\n",
    "\n",
    "\n",
    "#======== choix des paramètres de fragmentation\n",
    "\"\"\"\n",
    "    la taille du chunck_size est très important dans l'accès à une info précise\n",
    "    une plus petite taille permet de cibler de courts passages contenant l'info nécessaire à des réponses précises:\n",
    "        * lieu du projet\n",
    "        * dates du projet\n",
    "        * budget ...    \n",
    "    l'envoi de passages plus courts au llm évite une dispertion de son attention\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(txt_doc)\n",
    "\n",
    "# Filter out complex metadata (e.g., lists, dicts)\n",
    "docs = [Document(doc.page_content) for doc in docs]\n",
    "\n",
    "\n",
    "# Conversion des docs en embeddings \n",
    "chroma_db = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=f'./storage/vector_scores/{doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "    collection_name=doc_name_hybrid.replace(\" \",\"_\")\n",
    ")\n",
    "\n",
    "retriever=chroma_db.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ...existing code...\n",
    "all_docs = chroma_db.get()\n",
    "print(\"Nb de chuncks:\", len(all_docs['documents']))  # This will print the total number of docs stored\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f9772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nathan Devert]: France Culture. [Nathan Devert]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Comment expliquer les progrès phénoménaux que semble avoir accompli l'intelligence artificielle au cours de ces dernières années ? Depuis l'apparition de ChatGPT en novembre 2022, rapidement suivi par d'autres agents conversationnels, cette révolution technologique aux multiples aspects, paraît désormais capable d'exécuter de nombreuses tâches intellectuelles sur lesquelles l'esprit humain pensait jusqu'alors exercer un monopole. Écrire des articles, synthétiser des documents, traiter des données dans n'importe quel domaine, diagnostiquer une maladie, rédiger une dissertation, ou pourquoi pas, un scénario de film. Non contente de révolutionner le monde du travail, ses prouesses stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et \n",
      "=============\n",
      "stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et encore moins d'une âme, peut s'implémenter dans n'importe quelle machine, voir, comme s'inquiètent ou exultent certains, que l'IA sera un jour en mesure de remplacer notre conscience. À moins qu'il ne faille poser cette question à l'envers. Comment se fait-il que la machine puisse imiter les œuvres de de notre esprit ? Comment est-il possible en d'autres termes que notre vie mentale se révèle comme mécanisable, au même titre que nos facultés manuelles ? L'intelligence artificielle ne doit-elle pas sa capacité d'imitation au fait qu'elle a été théorisée, inventée, développée d'après une certaine représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre \n",
      "=============\n",
      "représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre propre pensée. L'intelligence artificielle, notre deuxième conscience. À l'occasion de la fête de la Science, j'aurai le plaisir d'en débattre sans préjuger aux côtés du mathématicien et philosophe Daniel Andler, de la professeur en intelligence artificielle et chercheuse Laurence Devillers et du philosophe Valentin Husson. [Narrateur radio]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Bonjour Laurence Devillers. [Laurence Devillers]: Bonjour. [Nathan Devert]: Vous êtes professeur en intelligence artificielle à Sorbonne Université, chercheuse au CNRS, auteur de \"L'IA, ange ou démon\" paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur \n",
      "=============\n",
      "paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur émérite à Sorbonne Université, membre de l'Académie des Sciences morales et politiques et auteur de \"Intelligence artificielle, intelligence humaine, la double énigme\" aux éditions Gallimard et bonjour Valentin Husson. [Valentin Husson]: Vous êtes philosophe et auteur de \"Folle résentimentale, petite philosophie des trolls\" chez Philosophie Magazine éditeur qui vient de paraître. Alors pour commencer cette cette discussion sur l'intelligence artificielle, je voulais vous soumettre une histoire qui a eu lieu récemment en mars 2025. Le Nouvel Obs a décidé d'organiser un concours littéraire entre deux auteurs, entre guillemets. Alors le premier était Hervé Le Tellier, pris Goncourt 2020, auteur de du célèbre roman l'anomalie et le deuxième était l'intelligence artificielle et plus précisément \n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "for c in all_docs[\"documents\"][:4]:\n",
    "    print(c, \"\\n=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84137027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.schema.document import Document\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "class RAG_hybrid():\n",
    "    def __init__(self, model):\n",
    "        self.model=model\n",
    "        self.retrieved_docs=[]\n",
    "        self.semantic_retriever_topK=10\n",
    "        self.sparse_retriever_topK=10\n",
    "        self.history=[]\n",
    "        self.llm_client = AsyncOpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "        )\n",
    "        self.reranker_llm=\"mistralai/mistral-small-3.1-24b-instruct\"\n",
    "        self.doc_name_hybrid=doc_name_hybrid\n",
    "        self.reranker_score_thresh=5\n",
    "        self.reranked_doc=[]\n",
    "\n",
    "    def semanticRetriever(self):\n",
    "        # 1. Semantic Retriever (Chroma + OllamaEmbeddings)\n",
    "        embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "        if self.doc_name_hybrid == 'None':\n",
    "            return \"Error: fournir le nom du document\"\n",
    "        \n",
    "        chroma_db = Chroma(\n",
    "            persist_directory=f'./storage/vector_scores/{self.doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "            collection_name=self.doc_name_hybrid.replace(\" \",\"_\"),\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        semantic_retriever=chroma_db.as_retriever(search_type=\"mmr\", k=self.semantic_retriever_topK)\n",
    "\n",
    "        self.chroma_db=chroma_db\n",
    "        self.semantic_retriever=semantic_retriever\n",
    "\n",
    "        return \"Success: ChromaDB setup avec succes\"\n",
    "    \n",
    "    def sparseRetriever(self):\n",
    "        # 2. Sparse Retriever (TF-IDF)\n",
    "\n",
    "        # Récupérer TOUS les documents depuis Chroma\n",
    "        all_data = self.chroma_db.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "        # Convertir en liste de `Document` objects pour LangChain\n",
    "        docs = [\n",
    "            Document(page_content=text, metadata=meta or {})  # <-- Si meta est None, on met {}\n",
    "            for text, meta in zip(all_data[\"documents\"], all_data[\"metadatas\"])\n",
    "        ]\n",
    "\n",
    "        # Créer le retriever TF-IDF\n",
    "        sparse_retriever = TFIDFRetriever.from_documents(\n",
    "            documents=docs,\n",
    "            k=self.sparse_retriever_topK,\n",
    "            tfidf_params={\"min_df\": 1, \"ngram_range\": (1, 2)}\n",
    "        )\n",
    "\n",
    "        self.sparse_retriever= sparse_retriever\n",
    "    \n",
    "    def ensembleRetriever(self):\n",
    "        # 3. Ensemble Retriever (Semantic + Sparse)\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.semantic_retriever, self.sparse_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        self.ensemble_retriever=ensemble_retriever\n",
    "\n",
    "    async def reranker(self, results, query):\n",
    "\n",
    "\n",
    "        async def llm_eval(doc, query):\n",
    "            system_prompt=\"\"\"\n",
    "                You're an expert assistant in reranking documents against a question.\n",
    "                Your role is to compare the question with a document and give a score from 0 to 10, where:\n",
    "                0=document out of context, unable to answer the question\n",
    "                10=highly relevant document, able to answer the question\n",
    "                                \n",
    "                The expected final output is the score in json format\n",
    "                Example:\n",
    "                ```json{\"score\": 5}```\n",
    "                \n",
    "                Always end your answer with this format                \n",
    "            \"\"\"            \n",
    "            response = await self.llm_client.chat.completions.create(\n",
    "                model=self.reranker_llm,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"La question est: {query}\\n Le document à évaluer est le suivant\\n: {doc}\" }\n",
    "                ],\n",
    "                temperature=0,\n",
    "            )\n",
    "            # Post-process to extract only the JSON part if extra text is present\n",
    "            content = response.choices[0].message.content\n",
    "            # Try to extract the JSON block if the model adds extra text\n",
    "            match = re.search(r\"\\{.*?\\}\", content, re.DOTALL)\n",
    "            if match:\n",
    "                content = match.group(0)\n",
    "\n",
    "            # extract score\n",
    "            score=None\n",
    "            try:\n",
    "                score=content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "                \n",
    "                score= json.loads(score)\n",
    "                score=score[\"score\"]\n",
    "            except Exception as e:\n",
    "                print(e)                \n",
    "            \n",
    "            return {\"content\": doc, \"score\": score}\n",
    "\n",
    "\n",
    "        tasks=[llm_eval(doc.page_content, query) for doc in results]\n",
    "        scored_docs= await asyncio.gather(*tasks)\n",
    "        i=1\n",
    "\n",
    "        for doc in scored_docs:\n",
    "          \n",
    "            print(f'chunk {i} score: {doc[\"score\"]}')\n",
    "            i+=1\n",
    "\n",
    "        filtred_docs=[d for d in scored_docs if int(d[\"score\"])>=self.reranker_score_thresh]\n",
    "        # print(f\"scored docs; \\n{scored_docs}\")\n",
    "        self.reranked_doc=filtred_docs\n",
    "\n",
    "        return filtred_docs\n",
    "\n",
    "    async def ask_llm(self, query):\n",
    "        # 5. Final processing step with an LLM (e.g., OpenAI via OpenRouter)\n",
    "\n",
    "        # init retrievers\n",
    "        status=self.semanticRetriever()\n",
    "        if \"Error\" in status:\n",
    "            return status\n",
    "        \n",
    "        self.sparseRetriever()\n",
    "        self.ensembleRetriever()\n",
    "\n",
    "        # retrieve relevant docs\n",
    "        results = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        print(f\"Nb of retrieved docs: {len(results)}\")\n",
    "\n",
    "        # rerank\n",
    "        scored_results=await self.reranker(results, query)\n",
    "        \n",
    "        # Concatenate retrieved documents for context\n",
    "        context = \"\\n\".join([f\"Fragment: \\n{doc['content']}\\n\" for doc in scored_results])\n",
    "\n",
    "        print(f\"Context lenght: {len(context.split(' '))} words\")\n",
    "        llm_prompt = f\"\"\"\n",
    "            Answer the question based **only** on the provided context.  \n",
    "\n",
    "            - If the context contains enough information to provide a complete or partial answer, use it to formulate a detailed and factual response.  \n",
    "            - If the context lacks relevant information, respond with: \"I don't know.\"  \n",
    "\n",
    "            ### **Context:**  \n",
    "            {context}  \n",
    "\n",
    "            ### **Question:**  \n",
    "            {query}  \n",
    "\n",
    "            ### **Answer:**  \n",
    "            Provide a clear, factual, and well-structured response based on the available context. Avoid speculation or adding external knowledge.  \n",
    "        \"\"\"\n",
    "\n",
    "        llm_completion = await self.llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in document Q/A and document synthesis\"},\n",
    "                {\"role\": \"user\", \"content\": llm_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        final_answer = \"\"\n",
    "        print(\"Réponse:\\n=========\")\n",
    "        async for chunk in llm_completion:\n",
    "            if hasattr(chunk.choices[0].delta, \"content\") and chunk.choices[0].delta.content:\n",
    "                final_answer += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        \n",
    "        self.history+=[\n",
    "            {\"role\": \"user\", 'content': query},\n",
    "            {\"role\": \"assistant\", \"content\": final_answer}\n",
    "        ]\n",
    "        \n",
    "        return final_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a224490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of retrieved docs: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 score: 4\n",
      "chunk 2 score: 9\n",
      "chunk 3 score: 4\n",
      "chunk 4 score: 9\n",
      "chunk 5 score: 2\n",
      "chunk 6 score: 3\n",
      "chunk 7 score: 2\n",
      "chunk 8 score: 7\n",
      "Context lenght: 504 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse:\n",
      "=========\n",
      "### **Answer:**\n",
      "\n",
      "#### **Principaux thèmes du texte:**\n",
      "\n",
      "1. **Créativité et Intelligence Artificielle (IA):**\n",
      "   - Le texte explore la capacité de l'IA à créer des métaphores et des analogies, en soulignant que cela relève davantage de l'optimisation mathématique que de la véritable créativité.\n",
      "   - Il est mentionné que l'humain apporte une touche unique de créativité, de vision métaphorique et d'inconscient, qui trouble et enrichit les productions de l'IA.\n",
      "\n",
      "2. **Philosophie de l'esprit et Intelligence:**\n",
      "   - Le texte aborde la question de savoir si l'intelligence peut être implémentée dans des machines, et si l'IA pourrait un jour remplacer la conscience humaine.\n",
      "   - Il pose également la question inverse : comment la machine peut-elle imiter les œuvres de l'esprit humain, et ce que cela révèle sur la mécanisabilité de la vie mentale.\n",
      "\n",
      "3. **Mécanique et Créativité Humaine:**\n",
      "   - Il est suggéré que la créativité humaine pourrait être vue comme une addition de méthodes, d'habitudes et de réflexes, révélant ainsi une certaine mécanique dans le processus créatif.\n",
      "   - Le texte aborde également l'angoisse fondamentale de l'être humain face à la machine, une thématique abordée par Freud comme l'une des trois blessures narcissiques de l'homme.\n",
      "\n",
      "#### **Questions qui peuvent être posées:**\n",
      "\n",
      "1. **Créativité et IA:**\n",
      "   - En quoi la créativité de l'IA diffère-t-elle de celle des humains ?\n",
      "   - Comment l'humain influence-t-il et enrichit-il les productions de l'IA ?\n",
      "\n",
      "2. **Philosophie de l'esprit:**\n",
      "   - L'intelligence peut-elle être entièrement reproduite par des machines ?\n",
      "   - Que révèle la capacité de l'IA à imiter l'esprit humain sur la nature de la conscience ?\n",
      "\n",
      "3. **Mécanique et Créativité Humaine:**\n",
      "   - Dans quelle mesure la créativité humaine peut-elle être considérée comme mécanique ?\n",
      "   - Comment l'angoisse de l'être humain face à la machine influence-t-elle la perception de la créativité et de l'intelligence ?\n",
      "\n",
      "Ces thèmes et questions illustrent les préoccupations centrales du texte, qui explore les frontières entre l'intelligence humaine et artificielle, ainsi que les implications philosophiques et psychologiques de ces interactions."
     ]
    }
   ],
   "source": [
    "rag_hybrid=RAG_hybrid(model=\"mistralai/mistral-small-3.2-24b-instruct\")\n",
    "\n",
    "# 4. Ask a question\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "results = await rag_hybrid.ask_llm(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f2574",
   "metadata": {},
   "source": [
    "==============================\n",
    "\n",
    "### Evaluation des réponses\n",
    "#### <u>Le cadre général:</u>\n",
    "**1. Construire les réponses de référence:**\n",
    "* Utiliser la question cadre \"Quels sont les principaux thèmes de ce texte ?\" \n",
    "* Charger le transcript du podcast et produire la réponse avec les modèles suivant:\n",
    "    * Dans le chat openrouter ou par API:\n",
    "        * openai/gpt-5.2\n",
    "        * google/gemini-3-pro-preview\n",
    "        * anthropic/claude-sonnet-4.5\n",
    "    * Dans NotebookLM (solution spécialisée dans l'analyse des documents)\n",
    "    \n",
    "**Important**: relire et comparer les réponses avec votre compréhension du podcast, corriger et valider\n",
    "\n",
    "**2. Utiliser les réponses de référence pour évaluer les RAGs:**\n",
    "\n",
    "Pour la question cadre ci dessus, le graphrag est plus apte que le vectoriel\n",
    "\n",
    "Il existe 2 paramètres techniques principaux qui influencent nettement la qualité des réponses:\n",
    "* Le modèle d'embedding: \n",
    "    * Utilisé lors de la création du graphe dans la structuration des noeuds et leur connexion (par rapprochement cosine sim)\n",
    "    * Utilisé lors de la lecture du graphe pour répondre à une question, qui mets en relation avec la question avec les noeuds les plus pertinent (cosine sim) pour sélectionner un/des point(s) de démarrage pour l'exploration du graphe\n",
    "* Le LLM:\n",
    "    * Utilisé lors de la création du graphe pour la reconnaissance des entités, leur définition a rôles, leurs catégories, les relations entre les entités, la normalisation, déduplication et unification des entités et des catégories ...\n",
    "    * Utilisé lors de la génération de la réponse, en recevant en entrée les pertinentes parties du graphe comme contexte\n",
    "\n",
    "#### Un tableau de combinaisons pour l'évaluation\n",
    "\n",
    "Exemple de grille systématique pour tester les combinaisons de modèles d'embedding et de LLM :\n",
    "\n",
    "## **Paramètres de Test**\n",
    "\n",
    "### **Modèles d'Embedding (Colonnes)**\n",
    "1. **Embedding bon marché**:\n",
    "* gemma embeddign 300m, ...\n",
    "2. **Embeddings premium**:\n",
    "* gemini embedding (via openrouter)\n",
    "* openai text-embedding-3-large\n",
    "\n",
    "\n",
    "### **Modèles LLM (Lignes)**\n",
    "**Pour la création de graphe:**\n",
    "#### Modèles `performants` et bon marché, éligible pour création de graphe:\n",
    "* deepseek/deepseek-chat-v3-0324: $0.20/M input tokens | $0.88/M output tokens\n",
    "* deepseek/deepseek-v3.1-terminus: $0.21/M input tokens | $0.79/M output tokens\n",
    "* prime-intellect/intellect-3: $0.20/M input tokens | $1.10/M output tokens\n",
    "* openai/gpt-5-mini: $0,25/M input tokens | $2/M output tokens\n",
    "* mistralai/mistral-large-2512: $0.50/M input tokens | $1.50/M output tokens\n",
    "* mistralai/mistral-medium-3.1; $0.40/M input tokens | $2/M output tokens\n",
    "\n",
    "#### Modèles `frontière`, meilleure qualité attendue, mais prix élevé pour modèles US:\n",
    "<div class=\"alert\" style=\"color: red\">Attention au prix de l'output variable</div>\n",
    "\n",
    "* google/gemini-2.5-pro: Starting at $1.25/M input tokens | Starting at $10/M output tokens\n",
    "* openai/gpt-5.1: $1.25/M input tokens | $10/M output tokens\n",
    "* deepseek/deepseek-v3.2: $0.28/M input tokens | $0.42/M output tokens\n",
    "* moonshotai/kimi-k2-thinking: $0.45/M input tokens | $2.35/M output tokens\n",
    "* z-ai/glm-4.6; $0.39/M input tokens | $1.90/M output tokens\n",
    "\n",
    "**Pour la génération de réponse:**\n",
    "En plus des modèles `tiers 2` ci dessus, envisager les modèles suivants, assez performants pour des questions de moindre complexité, et particulièrement peu chers;\n",
    "\n",
    "* mistralai/mistral-small-3.2-24b-instruct: $0.06/M input tokens | $0.18/M output tokens\n",
    "* meta-llama/llama-4-maverick: $0.15/M input tokens | $0.60/M output tokens\n",
    "* z-ai/glm-4.5-air: $0.104/M input tokens | $0.68/M output tokens\n",
    "* google/gemma-3-27b-it: $0.04/M input tokens | $0.15/M output tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Tableau de combinaisons (Grid Search)**\n",
    "\n",
    "| Combinaison | Embedding | LLM (Création) | LLM (Génération) | Coût Relatif | Gestion de la complexité |\n",
    "|-------------|-----------|----------------|------------------|--------------|------------|\n",
    "| **C1 - Baseline Économique** | gemma emb | google/gemma-3-27b-it | google/gemma-3-27b-it | Très faible | Simple |\n",
    "| **C2 - Hybride Léger** | gemma emb | deepseek-v3.1-terminus | mistral small | Faible | moyenne |\n",
    "| **C3 - Optimal Qualité** | gemma emb | deepseek/deepseek-v3.2 | deepseek/deepseek-v3.2 | Moyen | Elevée |\n",
    "| **C4 - Maximum Performance** | gemini ou openai large | openai/gpt-5.1 | openai/gpt-5.1 | Très élevé | Elevée|\n",
    "\n",
    "\n",
    "## **Métriques d'Évaluation par combinaison**\n",
    "\n",
    "### **Pour chaque combinaison, mesurer :**\n",
    "1. **Qualité RAG**\n",
    "   - Précision et couverture de la réponse VS référence\n",
    "   - RAGAS (Faithfulness, Answer Relevancy)\n",
    "   - Hallucinations\n",
    "\n",
    "2. **Performance Technique**\n",
    "   - Temps création graphe / génération réponses\n",
    "   - Coût pour création de graphe\n",
    "   - Coût par requête\n",
    "\n",
    "3. **Qualité Graphe**\n",
    "   - Nb de catégories formées\n",
    "   - Nb de noeuds formés\n",
    "   - Nombre de connexions\n",
    "\n",
    "\n",
    "\n",
    "## **Matrice de Décision**\n",
    "\n",
    "| Critère | Poids | C1 | C2 | C3 | C4 |\n",
    "|---------|-------|----|----|----|----|\n",
    "| **Qualité Réponse** | 70% | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| **Coût création graphe** | 10% | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐ |\n",
    "| **Coût génération réponse** | 15% | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐ |\n",
    "| **Temps** | 5% | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ |\n",
    "| **Score Total** | 100% | **58** | **68** | **72** | **77** |\n",
    "\n",
    "## **Recommandations**\n",
    "\n",
    "1. **Démarrer avec C3** (Optimal Qualité)\n",
    "* Si objectif qualité atteint (proche de C4 - Maximum Performance), tester C3 et mesurer la dégradation\n",
    "* Si objectif qualité non atteint passer à C4\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
