{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b419c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.créer un nouvel env conda à partir du terminal\n",
    "# conda create --name asr_rag python=3.10\n",
    "\n",
    "# 2. activer cet environnement\n",
    "# conda activate asr_rag\n",
    "\n",
    "# 3. installer les dépendences\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# 4. installer ollama: https://www.ollama.com/download\n",
    "\n",
    "\n",
    "# 5. installer le modème d'OllamaEmbeddings\n",
    "# ollama pull embeddinggemma\n",
    "\n",
    "# 6. créer une clé api sur openrouter pour utiliser des llm gratuitement\n",
    "\n",
    "# 7. mettre votre clé dans le fichier .env (racine du repertoire courant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0299e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from pathrag_retriever import create_graphdb, load_existing_graphdb, load_knowledgeGraph_vis\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "OPENROUTER_API_KEY=os.getenv(\"OPENROUTER_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbb31f",
   "metadata": {},
   "source": [
    "#### Création ou chargement d'un graphe existant:\n",
    "> Lorsque vous executer la cellule ci dessous, vous avez 2 options proposées:\n",
    "> 1. Créer un nouveau graphe\n",
    "> 2. Charger un graphe existant\n",
    "> \n",
    "> Un prompt de sélection s'affichera en haut du notebook<br>\n",
    "> Saisir l'action désirée dans le , suivre instructions\n",
    "\n",
    "> Si vous voulez charger un graphe déjà crée et que vous ne savez plus son nom, retrouver le dans le fichier `graphrag_hashes.json` (dans la racine du dossier), attribut `Nom du doc`\n",
    "\n",
    "> Si vous voulez modifier le LLM utilisé pour la création du graphe ou sa lecture, allez dans `pathrag_retriever.py`, sous la ligne 36( Choix du LLM OpenRouter), et prenez un modèle valide sur openrouter (attention à prendre un modèle qui supporte les `structured_outputs`, sur le site openrouter, à filtrer sur le paneau à gauche dans la liste des `Supported parameters`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29614848",
   "metadata": {},
   "source": [
    "#### 1. Créer un graphe, ou charger en un déjà crée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b4a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PathRAG:Logger initialized for working directory: /home/chougar/Téléchargements/RAG/RAG/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2\n",
      "INFO:PathRAG:Load KV llm_response_cache with 0 data\n",
      "INFO:PathRAG:Load KV full_docs with 1 data\n",
      "INFO:PathRAG:Load KV text_chunks with 3 data\n",
      "INFO:PathRAG:Loaded graph from /home/chougar/Téléchargements/RAG/RAG/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2/graph_chunk_entity_relation.graphml with 42 nodes, 36 edges\n",
      "INFO:nano-vectordb:Load (42, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Téléchargements/RAG/RAG/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2/vdb_entities.json'} 42 data\n",
      "INFO:nano-vectordb:Load (36, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Téléchargements/RAG/RAG/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2/vdb_relationships.json'} 36 data\n",
      "INFO:nano-vectordb:Load (3, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Téléchargements/RAG/RAG/storage/graph_stores/da824d7bce695f72eb26e2aceeedd4a69a07419e461e226546f40c47678a79a2/vdb_chunks.json'} 3 data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nom de votre graphe est IA seconde conscience\n",
      "\n",
      "        ----------------\n",
      "        #### Graph RAG retriever\n",
      "        Chargement de la base Graph RAG\n",
      "    \n",
      "**✅ Graph RAG chargé**\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# remetre à plat le text\n",
    "filename=\"audio-text.txt\"\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_docs = loader.load()\n",
    "text=\"\"\n",
    "for doc in txt_docs:\n",
    "    text+=doc.page_content\n",
    "\n",
    "\n",
    "r=input(\"Saisir 'C' pour créer un nouveau graphe, 'L' pour charger un graphe existant\")\n",
    "\n",
    "# créer un nouveau graphe\n",
    "messages=None\n",
    "if r=='C':\n",
    "    doc_name_graph=input('Saisir un nom unique pour votre graphe')\n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "    messages= create_graphdb(\n",
    "        text=text, \n",
    "        doc_name=doc_name_graph, # il faut donner un nom unique permettant d'identifier et charger le graph les prochaines fois\n",
    "    )\n",
    "# charger un graphe existant\n",
    "elif r=='L':\n",
    "    doc_name_graph=input('Saisir le nom du graphe à charger')\n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "\n",
    "    messages=load_existing_graphdb(doc_name_graph)\n",
    "else:\n",
    "    print('Option invalide')\n",
    "\n",
    "\n",
    "\n",
    "if messages:\n",
    "    pipeline_args={}\n",
    "    for feedback in messages:\n",
    "        if isinstance(feedback, str):\n",
    "            print(feedback)\n",
    "        elif isinstance(feedback, dict):\n",
    "            pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"]=feedback[\"pipeline_args\"]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1543579",
   "metadata": {},
   "source": [
    "#### 2. Poser vos questions au graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6de14e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:PathRAG:kw_prompt result:\n",
      "INFO:PathRAG:{\n",
      "  \"high_level_keywords\": [\"Principaux thèmes\", \"Questions à poser\", \"Analyse de texte\"],\n",
      "  \"low_level_keywords\": [\"Thèmes clés\", \"Interrogations\", \"Compréhension\", \"Contenu\"]\n",
      "}\n",
      "INFO:PathRAG:Local query uses 24 entites, 15 relations, 3 text units\n",
      "INFO:PathRAG:Global query uses 34 entites, 32 relations, 3 text units\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response all ready\n",
      "Les principaux thèmes abordés dans ce texte tournent autour de l'intelligence artificielle (IA), de la créativité humaine, et des implications sociétales et psychologiques de l'IA. Voici une analyse des thèmes centraux et des questions qu'ils soulèvent.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Créativité humaine vs IA**  \n",
      "**Thèmes :**  \n",
      "- Le texte distingue la créativité humaine (métaphores, originalité, motivation artistique) de l'optimisation algorithmique de l'IA (production de texte basée sur des contraintes et des données existantes).  \n",
      "- L'exemple du concours littéraire entre **Hervé Le Tellier** (Goncourt 2020) et **ChatGPT** illustre cette opposition.  \n",
      "\n",
      "**Questions possibles :**  \n",
      "- L'IA peut-elle vraiment être \"créative\", ou ne fait-elle que reproduire des schémas existants ?  \n",
      "- Dans quelle mesure la créativité humaine est-elle mécanique elle-même (habitudes, réflexes) ?  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. Impacts professionnels de l'IA**  \n",
      "**Thèmes :**  \n",
      "- La crainte du remplacement des métiers créatifs, comme l'écriture et le scénarisme (ex. grève des scénaristes à Hollywood).  \n",
      "- Le risque de perte de compétences chez les étudiants ou les ingénieurs dépendants de l'IA (dissertations, génération de code).  \n",
      "\n",
      "**Questions possibles :**  \n",
      "- Quels secteurs sont les plus vulnérables à l'automatisation par l'IA ?  \n",
      "- Comment préserver l'apprentissage humain face aux outils d'IA ?  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. Enjeux éthiques et psychologiques**  \n",
      "**Thèmes :**  \n",
      "- L'IA comme \"blessure narcissique\" pour l'humanité (référence à Freud, Copernic, Darwin).  \n",
      "- La dépendance croissante aux machines et la perte de maîtrise de son propre intellect (point soulevé par **Valentin Husson**).  \n",
      "- Les limites de l'IA face à l'inconscient et à l’humour humains.  \n",
      "\n",
      "**Questions possibles :**  \n",
      "- L'IA menace-t-elle l'identité humaine en imitant nos processus mentaux ?  \n",
      "- Peut-on envisager une IA dotée d'une forme de conscience ou de subjectivité ?  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. Fondamentaux technologiques de l'IA**  \n",
      "**Thèmes :**  \n",
      "- L'IA repose sur des **données numérisées** existantes, ce qui limite son \"originalité\".  \n",
      "- Sa performance dans des tâches contraintes (ex. une nouvelle policière) contraste avec son incapacité à innover \"du néant\" comme un chercheur en maths.  \n",
      "\n",
      "**Questions possibles :**  \n",
      "- Les futures IA, nourries de leurs propres productions, vont-elles perdre en qualité créative ?  \n",
      "- Comment différencier l'optimisation algorithmique de la véritable innovation ?  \n",
      "\n",
      "---\n",
      "\n",
      "### **Synthèse**  \n",
      "Le texte interroge les frontières entre l'intelligence humaine et artificielle, tout en soulignant les risques (dépendance, érosion des compétences) et les paradoxes (une IA \"créative\" mais dépendante des données humaines). Ces débats appellent à une réflexion sur la place de l'IA dans la société, l'art et le travail.  \n",
      "\n",
      "**Question ultime :** L'IA nous force-t-elle à redéfinir ce qui rend l’humain unique ?"
     ]
    }
   ],
   "source": [
    "from PathRAG import QueryParam\n",
    "import asyncio\n",
    "\n",
    "def stream_pathRAG_response(stream_resp):\n",
    "    async def stream_response():        \n",
    "        # Process the async generator\n",
    "        async for chunk in stream_resp:\n",
    "            print(chunk or \"\", end=\"\")\n",
    "\n",
    "\n",
    "    # Run in Streamlit's existing event loop\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(stream_response())\n",
    "\n",
    "# question=\"résume ce texte dans sa langue source\"\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "\n",
    "resp=pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"rag\"].query(query= question, param=QueryParam(mode=\"hybrid\", stream=True))\n",
    "\n",
    "stream_pathRAG_response(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f595f8",
   "metadata": {},
   "source": [
    "============================\n",
    "### RAG vectoriel\n",
    "1. Embedding du document -> renseigner le nom de votre fichier dans `filename` et le nom de votre DB dans `doc_name_hybrid`\n",
    "2. Setup du retriever / reranker / llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76565f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from audio-text.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de chuncks: 51\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "# chargement et fragmentation du doc\n",
    "## Nom du doc à traiter\n",
    "filename=\"audio-text.txt\"\n",
    "\n",
    "## Nom pour la base vectorielle\n",
    "doc_name_hybrid=\"L-IA-notre-deuxieme-conscience_sample\" # nom de doc significatif\n",
    "\n",
    "\n",
    "\n",
    "#========= choix du modèle d'embedding\n",
    "\"\"\"\n",
    "    Le modèle choisi impacte la qualité du retriever, mais aussi le temps de traitement\n",
    "    Si le déploiement est prévu sur une VM limitée, un modèle plus petit est nécessaire\n",
    "    Explorer les comparatifs: https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "\"\"\"\n",
    "# Utiliser OllamaEmbeddings avec le modèle local \"embeddinggemma\"\n",
    "embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "\n",
    "\n",
    "# loader = UnstructuredFileLoader(filename)\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "txt_doc = loader.load()\n",
    "print(f\"Loaded {len(txt_doc)} documents from {filename}\")\n",
    "\n",
    "\n",
    "#======== choix des paramètres de fragmentation\n",
    "\"\"\"\n",
    "    la taille du chunck_size est très important dans l'accès à une info précise\n",
    "    une plus petite taille permet de cibler de courts passages contenant l'info nécessaire à des réponses précises:\n",
    "        * lieu du projet\n",
    "        * dates du projet\n",
    "        * budget ...    \n",
    "    l'envoi de passages plus courts au llm évite une dispertion de son attention\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(txt_doc)\n",
    "\n",
    "# Filter out complex metadata (e.g., lists, dicts)\n",
    "docs = [Document(doc.page_content) for doc in docs]\n",
    "\n",
    "\n",
    "# Conversion des docs en embeddings \n",
    "chroma_db = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=f'./storage/vector_scores/{doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "    collection_name=doc_name_hybrid.replace(\" \",\"_\")\n",
    ")\n",
    "\n",
    "retriever=chroma_db.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ...existing code...\n",
    "all_docs = chroma_db.get()\n",
    "print(\"Nb de chuncks:\", len(all_docs['documents']))  # This will print the total number of docs stored\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f9772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nathan Devert]: France Culture. [Nathan Devert]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Comment expliquer les progrès phénoménaux que semble avoir accompli l'intelligence artificielle au cours de ces dernières années ? Depuis l'apparition de ChatGPT en novembre 2022, rapidement suivi par d'autres agents conversationnels, cette révolution technologique aux multiples aspects, paraît désormais capable d'exécuter de nombreuses tâches intellectuelles sur lesquelles l'esprit humain pensait jusqu'alors exercer un monopole. Écrire des articles, synthétiser des documents, traiter des données dans n'importe quel domaine, diagnostiquer une maladie, rédiger une dissertation, ou pourquoi pas, un scénario de film. Non contente de révolutionner le monde du travail, ses prouesses stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et \n",
      "=============\n",
      "stupéfiantes de la technique soulèvent une interrogation majeure dans le domaine de la philosophie de l'esprit. Faut-il en déduire que l'intelligence, n'étant pas le propre d'un cerveau biologique et encore moins d'une âme, peut s'implémenter dans n'importe quelle machine, voir, comme s'inquiètent ou exultent certains, que l'IA sera un jour en mesure de remplacer notre conscience. À moins qu'il ne faille poser cette question à l'envers. Comment se fait-il que la machine puisse imiter les œuvres de de notre esprit ? Comment est-il possible en d'autres termes que notre vie mentale se révèle comme mécanisable, au même titre que nos facultés manuelles ? L'intelligence artificielle ne doit-elle pas sa capacité d'imitation au fait qu'elle a été théorisée, inventée, développée d'après une certaine représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre \n",
      "=============\n",
      "représentation, peut-être incomplète ou biaisée, des ressorts de notre conscience, si bien que les miracles de l'IA nous inviteraient à explorer à nouveau frais l'irréductible singularité de notre propre pensée. L'intelligence artificielle, notre deuxième conscience. À l'occasion de la fête de la Science, j'aurai le plaisir d'en débattre sans préjuger aux côtés du mathématicien et philosophe Daniel Andler, de la professeur en intelligence artificielle et chercheuse Laurence Devillers et du philosophe Valentin Husson. [Narrateur radio]: France Culture, sans préjuger. Nathan Devert. [Nathan Devert]: Bonjour Laurence Devillers. [Laurence Devillers]: Bonjour. [Nathan Devert]: Vous êtes professeur en intelligence artificielle à Sorbonne Université, chercheuse au CNRS, auteur de \"L'IA, ange ou démon\" paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur \n",
      "=============\n",
      "paru en 2025 aux éditions du Serge et présidente de la Fondation Blaise Pascal. Bonjour Daniel Andler. [Daniel Andler]: Bonjour. [Nathan Devert]: Vous êtes mathématicien, philosophe, professeur émérite à Sorbonne Université, membre de l'Académie des Sciences morales et politiques et auteur de \"Intelligence artificielle, intelligence humaine, la double énigme\" aux éditions Gallimard et bonjour Valentin Husson. [Valentin Husson]: Vous êtes philosophe et auteur de \"Folle résentimentale, petite philosophie des trolls\" chez Philosophie Magazine éditeur qui vient de paraître. Alors pour commencer cette cette discussion sur l'intelligence artificielle, je voulais vous soumettre une histoire qui a eu lieu récemment en mars 2025. Le Nouvel Obs a décidé d'organiser un concours littéraire entre deux auteurs, entre guillemets. Alors le premier était Hervé Le Tellier, pris Goncourt 2020, auteur de du célèbre roman l'anomalie et le deuxième était l'intelligence artificielle et plus précisément \n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "for c in all_docs[\"documents\"][:4]:\n",
    "    print(c, \"\\n=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84137027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.schema.document import Document\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "class RAG_hybrid():\n",
    "    def __init__(self, model):\n",
    "        self.model=model\n",
    "        self.retrieved_docs=[]\n",
    "        self.semantic_retriever_topK=10\n",
    "        self.sparse_retriever_topK=10\n",
    "        self.history=[]\n",
    "        self.llm_client = AsyncOpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "        )\n",
    "        self.reranker_llm=\"mistralai/mistral-small-3.1-24b-instruct\"\n",
    "        self.doc_name_hybrid=doc_name_hybrid\n",
    "        self.reranker_score_thresh=5\n",
    "        self.reranked_doc=[]\n",
    "\n",
    "    def semanticRetriever(self):\n",
    "        # 1. Semantic Retriever (Chroma + OllamaEmbeddings)\n",
    "        embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "        if self.doc_name_hybrid == 'None':\n",
    "            return \"Error: fournir le nom du document\"\n",
    "        \n",
    "        chroma_db = Chroma(\n",
    "            persist_directory=f'./storage/vector_scores/{self.doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "            collection_name=self.doc_name_hybrid.replace(\" \",\"_\"),\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        semantic_retriever=chroma_db.as_retriever(search_type=\"mmr\", k=self.semantic_retriever_topK)\n",
    "\n",
    "        self.chroma_db=chroma_db\n",
    "        self.semantic_retriever=semantic_retriever\n",
    "\n",
    "        return \"Success: ChromaDB setup avec succes\"\n",
    "    \n",
    "    def sparseRetriever(self):\n",
    "        # 2. Sparse Retriever (TF-IDF)\n",
    "\n",
    "        # Récupérer TOUS les documents depuis Chroma\n",
    "        all_data = self.chroma_db.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "        # Convertir en liste de `Document` objects pour LangChain\n",
    "        docs = [\n",
    "            Document(page_content=text, metadata=meta or {})  # <-- Si meta est None, on met {}\n",
    "            for text, meta in zip(all_data[\"documents\"], all_data[\"metadatas\"])\n",
    "        ]\n",
    "\n",
    "        # Créer le retriever TF-IDF\n",
    "        sparse_retriever = TFIDFRetriever.from_documents(\n",
    "            documents=docs,\n",
    "            k=self.sparse_retriever_topK,\n",
    "            tfidf_params={\"min_df\": 1, \"ngram_range\": (1, 2)}\n",
    "        )\n",
    "\n",
    "        self.sparse_retriever= sparse_retriever\n",
    "    \n",
    "    def ensembleRetriever(self):\n",
    "        # 3. Ensemble Retriever (Semantic + Sparse)\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.semantic_retriever, self.sparse_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        self.ensemble_retriever=ensemble_retriever\n",
    "\n",
    "    async def reranker(self, results, query):\n",
    "\n",
    "\n",
    "        async def llm_eval(doc, query):\n",
    "            system_prompt=\"\"\"\n",
    "                You're an expert assistant in reranking documents against a question.\n",
    "                Your role is to compare the question with a document and give a score from 0 to 10, where:\n",
    "                0=document out of context, unable to answer the question\n",
    "                10=highly relevant document, able to answer the question\n",
    "                                \n",
    "                The expected final output is the score in json format\n",
    "                Example:\n",
    "                ```json{\"score\": 5}```\n",
    "                \n",
    "                Always end your answer with this format                \n",
    "            \"\"\"            \n",
    "            response = await self.llm_client.chat.completions.create(\n",
    "                model=self.reranker_llm,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"La question est: {query}\\n Le document à évaluer est le suivant\\n: {doc}\" }\n",
    "                ],\n",
    "                temperature=0,\n",
    "            )\n",
    "            # Post-process to extract only the JSON part if extra text is present\n",
    "            content = response.choices[0].message.content\n",
    "            # Try to extract the JSON block if the model adds extra text\n",
    "            match = re.search(r\"\\{.*?\\}\", content, re.DOTALL)\n",
    "            if match:\n",
    "                content = match.group(0)\n",
    "\n",
    "            # extract score\n",
    "            score=None\n",
    "            try:\n",
    "                score=content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "                \n",
    "                score= json.loads(score)\n",
    "                score=score[\"score\"]\n",
    "            except Exception as e:\n",
    "                print(e)                \n",
    "            \n",
    "            return {\"content\": doc, \"score\": score}\n",
    "\n",
    "\n",
    "        tasks=[llm_eval(doc.page_content, query) for doc in results]\n",
    "        scored_docs= await asyncio.gather(*tasks)\n",
    "        i=1\n",
    "\n",
    "        for doc in scored_docs:\n",
    "          \n",
    "            print(f'chunk {i} score: {doc[\"score\"]}')\n",
    "            i+=1\n",
    "\n",
    "        filtred_docs=[d for d in scored_docs if int(d[\"score\"])>=self.reranker_score_thresh]\n",
    "        # print(f\"scored docs; \\n{scored_docs}\")\n",
    "        self.reranked_doc=filtred_docs\n",
    "\n",
    "        return filtred_docs\n",
    "\n",
    "    async def ask_llm(self, query):\n",
    "        # 5. Final processing step with an LLM (e.g., OpenAI via OpenRouter)\n",
    "\n",
    "        # init retrievers\n",
    "        status=self.semanticRetriever()\n",
    "        if \"Error\" in status:\n",
    "            return status\n",
    "        \n",
    "        self.sparseRetriever()\n",
    "        self.ensembleRetriever()\n",
    "\n",
    "        # retrieve relevant docs\n",
    "        results = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        print(f\"Nb of retrieved docs: {len(results)}\")\n",
    "\n",
    "        # rerank\n",
    "        scored_results=await self.reranker(results, query)\n",
    "        \n",
    "        # Concatenate retrieved documents for context\n",
    "        context = \"\\n\".join([f\"Fragment: \\n{doc['content']}\\n\" for doc in scored_results])\n",
    "\n",
    "        print(f\"Context lenght: {len(context.split(' '))} words\")\n",
    "        llm_prompt = f\"\"\"\n",
    "            Answer the question based **only** on the provided context.  \n",
    "\n",
    "            - If the context contains enough information to provide a complete or partial answer, use it to formulate a detailed and factual response.  \n",
    "            - If the context lacks relevant information, respond with: \"I don't know.\"  \n",
    "\n",
    "            ### **Context:**  \n",
    "            {context}  \n",
    "\n",
    "            ### **Question:**  \n",
    "            {query}  \n",
    "\n",
    "            ### **Answer:**  \n",
    "            Provide a clear, factual, and well-structured response based on the available context. Avoid speculation or adding external knowledge.  \n",
    "        \"\"\"\n",
    "\n",
    "        llm_completion = await self.llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in document Q/A and document synthesis\"},\n",
    "                {\"role\": \"user\", \"content\": llm_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        final_answer = \"\"\n",
    "        print(\"Réponse:\\n=========\")\n",
    "        async for chunk in llm_completion:\n",
    "            if hasattr(chunk.choices[0].delta, \"content\") and chunk.choices[0].delta.content:\n",
    "                final_answer += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        \n",
    "        self.history+=[\n",
    "            {\"role\": \"user\", 'content': query},\n",
    "            {\"role\": \"assistant\", \"content\": final_answer}\n",
    "        ]\n",
    "        \n",
    "        return final_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a224490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of retrieved docs: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 score: 4\n",
      "chunk 2 score: 9\n",
      "chunk 3 score: 4\n",
      "chunk 4 score: 9\n",
      "chunk 5 score: 2\n",
      "chunk 6 score: 3\n",
      "chunk 7 score: 2\n",
      "chunk 8 score: 7\n",
      "Context lenght: 504 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse:\n",
      "=========\n",
      "### **Answer:**\n",
      "\n",
      "#### **Principaux thèmes du texte:**\n",
      "\n",
      "1. **Créativité et Intelligence Artificielle (IA):**\n",
      "   - Le texte explore la capacité de l'IA à créer des métaphores et des analogies, en soulignant que cela relève davantage de l'optimisation mathématique que de la véritable créativité.\n",
      "   - Il est mentionné que l'humain apporte une touche unique de créativité, de vision métaphorique et d'inconscient, qui trouble et enrichit les productions de l'IA.\n",
      "\n",
      "2. **Philosophie de l'esprit et Intelligence:**\n",
      "   - Le texte aborde la question de savoir si l'intelligence peut être implémentée dans des machines, et si l'IA pourrait un jour remplacer la conscience humaine.\n",
      "   - Il pose également la question inverse : comment la machine peut-elle imiter les œuvres de l'esprit humain, et ce que cela révèle sur la mécanisabilité de la vie mentale.\n",
      "\n",
      "3. **Mécanique et Créativité Humaine:**\n",
      "   - Il est suggéré que la créativité humaine pourrait être vue comme une addition de méthodes, d'habitudes et de réflexes, révélant ainsi une certaine mécanique dans le processus créatif.\n",
      "   - Le texte aborde également l'angoisse fondamentale de l'être humain face à la machine, une thématique abordée par Freud comme l'une des trois blessures narcissiques de l'homme.\n",
      "\n",
      "#### **Questions qui peuvent être posées:**\n",
      "\n",
      "1. **Créativité et IA:**\n",
      "   - En quoi la créativité de l'IA diffère-t-elle de celle des humains ?\n",
      "   - Comment l'humain influence-t-il et enrichit-il les productions de l'IA ?\n",
      "\n",
      "2. **Philosophie de l'esprit:**\n",
      "   - L'intelligence peut-elle être entièrement reproduite par des machines ?\n",
      "   - Que révèle la capacité de l'IA à imiter l'esprit humain sur la nature de la conscience ?\n",
      "\n",
      "3. **Mécanique et Créativité Humaine:**\n",
      "   - Dans quelle mesure la créativité humaine peut-elle être considérée comme mécanique ?\n",
      "   - Comment l'angoisse de l'être humain face à la machine influence-t-elle la perception de la créativité et de l'intelligence ?\n",
      "\n",
      "Ces thèmes et questions illustrent les préoccupations centrales du texte, qui explore les frontières entre l'intelligence humaine et artificielle, ainsi que les implications philosophiques et psychologiques de ces interactions."
     ]
    }
   ],
   "source": [
    "rag_hybrid=RAG_hybrid(model=\"mistralai/mistral-small-3.2-24b-instruct\")\n",
    "\n",
    "# 4. Ask a question\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "results = await rag_hybrid.ask_llm(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_rag_asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
